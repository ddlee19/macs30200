{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling - BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Reddit texts are too short for coherent/interpretable topics from LDA, so let's try BERT Topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "def converter(x):\n",
    "    return literal_eval(x)\n",
    "\n",
    "# full corpus\n",
    "full_df = pd.read_csv('../data/full_df_processed.csv', converters={'tokens_new':converter, 'normalized_tokens':converter})\n",
    "\n",
    "# csv file/df of posts and comments not scraped from r/asianamerican\n",
    "non_aa_df = pd.read_csv('../data/not_asianamerican_df.csv', converters={'tokens_new':converter, 'normalized_tokens':converter})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to tokenize by sentence, not individual words for CBOW or skipgrams to make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>username</th>\n",
       "      <th>time_created</th>\n",
       "      <th>flair</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>tokens_new</th>\n",
       "      <th>word_count</th>\n",
       "      <th>normalized_tokens</th>\n",
       "      <th>normalized_tokens_count</th>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>sr_is_asian</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>14m8mf4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-29 10:54:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Megathread] Supreme Court Ruling on Affirmati...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[Megathread, Supreme, Court, Ruling, on, Affir...</td>\n",
       "      <td>78</td>\n",
       "      <td>[megathread, supreme, court, ruling, affirmati...</td>\n",
       "      <td>62</td>\n",
       "      <td>asianamerican</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>jq5du0z</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-30 11:33:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[Thanks, to, everyone, who, engaged, in, insig...</td>\n",
       "      <td>20</td>\n",
       "      <td>[thank, engage, insightful, respectful, discou...</td>\n",
       "      <td>9</td>\n",
       "      <td>asianamerican</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>jq0dgzx</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>2023-06-29 11:16:15</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[I, would, prefer, using, a, process, that, ta...</td>\n",
       "      <td>103</td>\n",
       "      <td>[prefer, process, take, account, poverty, inst...</td>\n",
       "      <td>52</td>\n",
       "      <td>asianamerican</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>jq0cg7k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>2023-06-29 11:09:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[u/Tungsten_,, Thanks, for, creating, a, secti...</td>\n",
       "      <td>269</td>\n",
       "      <td>[u/tungsten_,, thank, create, section, discuss...</td>\n",
       "      <td>126</td>\n",
       "      <td>asianamerican</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>jq0f52k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>2023-06-29 11:26:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[As, with, anything, related, to, Asians, in, ...</td>\n",
       "      <td>59</td>\n",
       "      <td>[relate, asians, politic, m, see, lot, non, as...</td>\n",
       "      <td>25</td>\n",
       "      <td>asianamerican</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       id   parent_id        username         time_created  \\\n",
       "0           0  14m8mf4         NaN       Tungsten_  2023-06-29 10:54:44   \n",
       "1           1  jq5du0z  t3_14m8mf4       Tungsten_  2023-06-30 11:33:11   \n",
       "2           2  jq0dgzx  t3_14m8mf4  ProudBlackMatt  2023-06-29 11:16:15   \n",
       "3           3  jq0cg7k  t3_14m8mf4    TomatoCanned  2023-06-29 11:09:47   \n",
       "4           4  jq0f52k  t3_14m8mf4    bad-fengshui  2023-06-29 11:26:41   \n",
       "\n",
       "              flair                                               body  \\\n",
       "0               NaN  [Megathread] Supreme Court Ruling on Affirmati...   \n",
       "1               NaN  Thanks to everyone who engaged in insightful a...   \n",
       "2  Chinese-American  I would prefer using a process that takes into...   \n",
       "3               NaN  u/Tungsten_, Thanks for creating a section jus...   \n",
       "4               NaN  As with anything related to Asians in politics...   \n",
       "\n",
       "  subreddit                                         tokens_new  word_count  \\\n",
       "0  t5_2rfyw  [Megathread, Supreme, Court, Ruling, on, Affir...          78   \n",
       "1  t5_2rfyw  [Thanks, to, everyone, who, engaged, in, insig...          20   \n",
       "2  t5_2rfyw  [I, would, prefer, using, a, process, that, ta...         103   \n",
       "3  t5_2rfyw  [u/Tungsten_,, Thanks, for, creating, a, secti...         269   \n",
       "4  t5_2rfyw  [As, with, anything, related, to, Asians, in, ...          59   \n",
       "\n",
       "                                   normalized_tokens  normalized_tokens_count  \\\n",
       "0  [megathread, supreme, court, ruling, affirmati...                       62   \n",
       "1  [thank, engage, insightful, respectful, discou...                        9   \n",
       "2  [prefer, process, take, account, poverty, inst...                       52   \n",
       "3  [u/tungsten_,, thank, create, section, discuss...                      126   \n",
       "4  [relate, asians, politic, m, see, lot, non, as...                       25   \n",
       "\n",
       "  subreddit_name  sr_is_asian  \n",
       "0  asianamerican         True  \n",
       "1  asianamerican         True  \n",
       "2  asianamerican         True  \n",
       "3  asianamerican         True  \n",
       "4  asianamerican         True  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deep copy of full_df\n",
    "full_df_new = full_df.copy(deep=True)\n",
    "full_df_new.drop(columns=['Unnamed: 0','tokens_new', 'normalized_tokens', 'normalized_tokens_count'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize(word_list, model=nlp, MAX_LEN=1500000):\n",
    "    '''\n",
    "    note that word_list here is in fact a string. If it happens to be a list, we convert it to string format. \n",
    "    '''\n",
    "    tokenized = []\n",
    "    if type(word_list) == list and len(word_list) == 1:\n",
    "        word_list = word_list[0]\n",
    "\n",
    "    if type(word_list) == list:\n",
    "        word_list = ' '.join([str(elem) for elem in word_list]) \n",
    "    # since we're only tokenizing, I remove RAM intensive operations and increase max text size\n",
    "\n",
    "    model.max_length = MAX_LEN\n",
    "    doc = model(word_list, disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "\n",
    "    for token in doc:\n",
    "        if not token.is_punct and len(token.text.strip()) > 0:\n",
    "            tokenized.append(token.text)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_tokenize(word_list, model=nlp):\n",
    "    doc = model(word_list)\n",
    "    sentences = [sent.text.strip() for sent in doc.sents]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeTokens(word_list, extra_stop=[], model=nlp, lemma=True, MAX_LEN=1500000):\n",
    "    #We can use a generator here as we just need to iterate over it\n",
    "    normalized = []\n",
    "    if type(word_list) == list and len(word_list) == 1:\n",
    "        word_list = word_list[0]\n",
    "\n",
    "    if type(word_list) == list:\n",
    "        word_list = ' '.join([str(elem) for elem in word_list]) \n",
    "\n",
    "    # since we're only normalizing, I remove RAM intensive operations and increase max text size\n",
    "\n",
    "    model.max_length = MAX_LEN\n",
    "    doc = model(word_list.lower(), disable=[\"parser\", \"ner\"])\n",
    "\n",
    "    if len(extra_stop) > 0:\n",
    "        for stopword in extra_stop:\n",
    "            lexeme = nlp.vocab[stopword]\n",
    "            lexeme.is_stop = True\n",
    "\n",
    "    # we check if we want lemmas or not earlier to avoid checking every time we loop\n",
    "    if lemma:\n",
    "        for w in doc:\n",
    "            # if it's not a stop word or punctuation mark, add it to our article\n",
    "            if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num and len(w.text.strip()) > 0:\n",
    "            # we add the lematized version of the word\n",
    "                normalized.append(str(w.lemma_))\n",
    "    else:\n",
    "        for w in doc:\n",
    "            # if it's not a stop word or punctuation mark, add it to our article\n",
    "            if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num and len(w.text.strip()) > 0:\n",
    "            # we add the lematized version of the word\n",
    "                normalized.append(str(w.text.strip()))\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\spacy\\pipeline\\lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "full_df_new['tokens'] = full_df['body'].apply(lambda x: [word_tokenize(s) for s in sent_tokenize(x)]) # takes 10 mins and a half to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_repr_html_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    732\u001b[0m             \u001b[1;31m# Argument 1 to \"ensure_index\" has incompatible type \"Collection[Any]\";\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    733\u001b[0m             \u001b[1;31m# expected \"Union[Union[Union[ExtensionArray, ndarray],\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 734\u001b[1;33m             \u001b[1;31m# Index, Series], Sequence[Any]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    735\u001b[0m             \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m             \u001b[1;31m# Argument 1 to \"ensure_index\" has incompatible type \"Collection[Any]\";\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_html\u001b[1;34m(self, buf, encoding, classes, notebook, border)\u001b[0m\n\u001b[0;32m    982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m     def to_html(\n\u001b[1;32m--> 984\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    985\u001b[0m         \u001b[0mbuf\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFilePathOrBuffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    986\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\html.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, formatter, classes, border, table_id, render_links)\u001b[0m\n\u001b[0;32m     66\u001b[0m         self.col_space = {\n\u001b[0;32m     67\u001b[0m             \u001b[0mcolumn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34mf\"{value}px\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m         }\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "      Unnamed: 0       id   parent_id              username  \\\n",
       "0              0  14m8mf4         NaN             Tungsten_   \n",
       "1              1  jq5du0z  t3_14m8mf4             Tungsten_   \n",
       "2              2  jq0dgzx  t3_14m8mf4        ProudBlackMatt   \n",
       "3              3  jq0cg7k  t3_14m8mf4          TomatoCanned   \n",
       "4              4  jq0f52k  t3_14m8mf4          bad-fengshui   \n",
       "...          ...      ...         ...                   ...   \n",
       "4515        4515   osjkh6         NaN             yellowmix   \n",
       "4516        4516  iw0q5sn   t3_yr5o90             yellowmix   \n",
       "4517        4517   uyzxgz         NaN      Kamala_Metamorph   \n",
       "4518        4518  l20ftbs  t3_1cfw1ru  Extension_River_9901   \n",
       "4519        4519   1khnmw         NaN              Swordbow   \n",
       "\n",
       "             time_created             flair  \\\n",
       "0     2023-06-29 10:54:44               NaN   \n",
       "1     2023-06-30 11:33:11               NaN   \n",
       "2     2023-06-29 11:16:15  Chinese-American   \n",
       "3     2023-06-29 11:09:47               NaN   \n",
       "4     2023-06-29 11:26:41               NaN   \n",
       "...                   ...               ...   \n",
       "4515  2021-07-27 09:32:14               NaN   \n",
       "4516  2022-11-12 01:09:36               NaN   \n",
       "4517  2022-05-27 14:52:16                 ​   \n",
       "4518  2024-04-30 22:56:39          New user   \n",
       "4519  2013-08-16 14:46:17                6∆   \n",
       "\n",
       "                                                   body subreddit  word_count  \\\n",
       "0     [Megathread] Supreme Court Ruling on Affirmati...  t5_2rfyw          78   \n",
       "1     Thanks to everyone who engaged in insightful a...  t5_2rfyw          20   \n",
       "2     I would prefer using a process that takes into...  t5_2rfyw         103   \n",
       "3     u/Tungsten_, Thanks for creating a section jus...  t5_2rfyw         269   \n",
       "4     As with anything related to Asians in politics...  t5_2rfyw          59   \n",
       "...                                                 ...       ...         ...   \n",
       "4515  This Is the End of Affirmative Action. What ar...  t5_2qhgd          15   \n",
       "4516  What do you mean? That nepotism comes from whi...  t5_2qhgd          32   \n",
       "4517  How to have a conversation with an open-minded...  t5_38jid         237   \n",
       "4518    Democrats that want to expand education .Fun...  t5_3amv4         349   \n",
       "4519  I can't trust someone who argues from pure sel...  t5_2w2s8         180   \n",
       "\n",
       "     subreddit_name  sr_is_asian  \\\n",
       "0     asianamerican         True   \n",
       "1     asianamerican         True   \n",
       "2     asianamerican         True   \n",
       "3     asianamerican         True   \n",
       "4     asianamerican         True   \n",
       "...             ...          ...   \n",
       "4515         racism        False   \n",
       "4516         racism        False   \n",
       "4517        MensLib        False   \n",
       "4518    aznidentity         True   \n",
       "4519   changemyview        False   \n",
       "\n",
       "                                                 tokens  \n",
       "0     [[Megathread, Supreme, Court, Ruling, on, Affi...  \n",
       "1     [[Thanks, to, everyone, who, engaged, in, insi...  \n",
       "2     [[I, would, prefer, using, a, process, that, t...  \n",
       "3     [[u, Tungsten, Thanks, for, creating, a, secti...  \n",
       "4     [[As, with, anything, related, to, Asians, in,...  \n",
       "...                                                 ...  \n",
       "4515  [[This, Is, the, End, of, Affirmative, Action]...  \n",
       "4516  [[What, do, you, mean], [That, nepotism, comes...  \n",
       "4517  [[How, to, have, a, conversation, with, an, op...  \n",
       "4518  [[Democrats, that, want, to, expand, education...  \n",
       "4519  [[I, ca, n't, trust, someone, who, argues, fro...  \n",
       "\n",
       "[4520 rows x 12 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_new['normalized_tokens'] = full_df_new['tokens'].apply(lambda x: [normalizeTokens(s, lemma=False) for s in x]) # takes a minute and half to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_new.to_csv('../data/full_df_processed_sent.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>username</th>\n",
       "      <th>time_created</th>\n",
       "      <th>flair</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>word_count</th>\n",
       "      <th>subreddit_name</th>\n",
       "      <th>sr_is_asian</th>\n",
       "      <th>tokens</th>\n",
       "      <th>normalized_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14m8mf4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-29 10:54:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Megathread] Supreme Court Ruling on Affirmati...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>78</td>\n",
       "      <td>asianamerican</td>\n",
       "      <td>True</td>\n",
       "      <td>[[Megathread, Supreme, Court, Ruling, on, Affi...</td>\n",
       "      <td>[[megathread, supreme, court, ruling, affirmat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jq5du0z</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-30 11:33:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>20</td>\n",
       "      <td>asianamerican</td>\n",
       "      <td>True</td>\n",
       "      <td>[[Thanks, to, everyone, who, engaged, in, insi...</td>\n",
       "      <td>[[thanks, engaged, insightful, respectful, dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jq0dgzx</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>2023-06-29 11:16:15</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>103</td>\n",
       "      <td>asianamerican</td>\n",
       "      <td>True</td>\n",
       "      <td>[[I, would, prefer, using, a, process, that, t...</td>\n",
       "      <td>[[prefer, process, takes, account, poverty, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jq0cg7k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>2023-06-29 11:09:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>269</td>\n",
       "      <td>asianamerican</td>\n",
       "      <td>True</td>\n",
       "      <td>[[u, Tungsten, Thanks, for, creating, a, secti...</td>\n",
       "      <td>[[u, tungsten, thanks, creating, section, disc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jq0f52k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>2023-06-29 11:26:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>59</td>\n",
       "      <td>asianamerican</td>\n",
       "      <td>True</td>\n",
       "      <td>[[As, with, anything, related, to, Asians, in,...</td>\n",
       "      <td>[[related, asians, politics, m, seeing, lot, n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4515</th>\n",
       "      <td>osjkh6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yellowmix</td>\n",
       "      <td>2021-07-27 09:32:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This Is the End of Affirmative Action. What ar...</td>\n",
       "      <td>t5_2qhgd</td>\n",
       "      <td>15</td>\n",
       "      <td>racism</td>\n",
       "      <td>False</td>\n",
       "      <td>[[This, Is, the, End, of, Affirmative, Action]...</td>\n",
       "      <td>[[end, affirmative, action], [going]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4516</th>\n",
       "      <td>iw0q5sn</td>\n",
       "      <td>t3_yr5o90</td>\n",
       "      <td>yellowmix</td>\n",
       "      <td>2022-11-12 01:09:36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What do you mean? That nepotism comes from whi...</td>\n",
       "      <td>t5_2qhgd</td>\n",
       "      <td>32</td>\n",
       "      <td>racism</td>\n",
       "      <td>False</td>\n",
       "      <td>[[What, do, you, mean], [That, nepotism, comes...</td>\n",
       "      <td>[[mean], [nepotism, comes, white, supremacy], ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4517</th>\n",
       "      <td>uyzxgz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kamala_Metamorph</td>\n",
       "      <td>2022-05-27 14:52:16</td>\n",
       "      <td>​</td>\n",
       "      <td>How to have a conversation with an open-minded...</td>\n",
       "      <td>t5_38jid</td>\n",
       "      <td>237</td>\n",
       "      <td>MensLib</td>\n",
       "      <td>False</td>\n",
       "      <td>[[How, to, have, a, conversation, with, an, op...</td>\n",
       "      <td>[[conversation, open, minded, disadvantaged, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4518</th>\n",
       "      <td>l20ftbs</td>\n",
       "      <td>t3_1cfw1ru</td>\n",
       "      <td>Extension_River_9901</td>\n",
       "      <td>2024-04-30 22:56:39</td>\n",
       "      <td>New user</td>\n",
       "      <td>Democrats that want to expand education .Fun...</td>\n",
       "      <td>t5_3amv4</td>\n",
       "      <td>349</td>\n",
       "      <td>aznidentity</td>\n",
       "      <td>True</td>\n",
       "      <td>[[Democrats, that, want, to, expand, education...</td>\n",
       "      <td>[[democrats, want, expand, education, .funding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4519</th>\n",
       "      <td>1khnmw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Swordbow</td>\n",
       "      <td>2013-08-16 14:46:17</td>\n",
       "      <td>6∆</td>\n",
       "      <td>I can't trust someone who argues from pure sel...</td>\n",
       "      <td>t5_2w2s8</td>\n",
       "      <td>180</td>\n",
       "      <td>changemyview</td>\n",
       "      <td>False</td>\n",
       "      <td>[[I, ca, n't, trust, someone, who, argues, fro...</td>\n",
       "      <td>[[trust, argues, pure, self, interest, princip...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4520 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id   parent_id              username         time_created  \\\n",
       "0     14m8mf4         NaN             Tungsten_  2023-06-29 10:54:44   \n",
       "1     jq5du0z  t3_14m8mf4             Tungsten_  2023-06-30 11:33:11   \n",
       "2     jq0dgzx  t3_14m8mf4        ProudBlackMatt  2023-06-29 11:16:15   \n",
       "3     jq0cg7k  t3_14m8mf4          TomatoCanned  2023-06-29 11:09:47   \n",
       "4     jq0f52k  t3_14m8mf4          bad-fengshui  2023-06-29 11:26:41   \n",
       "...       ...         ...                   ...                  ...   \n",
       "4515   osjkh6         NaN             yellowmix  2021-07-27 09:32:14   \n",
       "4516  iw0q5sn   t3_yr5o90             yellowmix  2022-11-12 01:09:36   \n",
       "4517   uyzxgz         NaN      Kamala_Metamorph  2022-05-27 14:52:16   \n",
       "4518  l20ftbs  t3_1cfw1ru  Extension_River_9901  2024-04-30 22:56:39   \n",
       "4519   1khnmw         NaN              Swordbow  2013-08-16 14:46:17   \n",
       "\n",
       "                 flair                                               body  \\\n",
       "0                  NaN  [Megathread] Supreme Court Ruling on Affirmati...   \n",
       "1                  NaN  Thanks to everyone who engaged in insightful a...   \n",
       "2     Chinese-American  I would prefer using a process that takes into...   \n",
       "3                  NaN  u/Tungsten_, Thanks for creating a section jus...   \n",
       "4                  NaN  As with anything related to Asians in politics...   \n",
       "...                ...                                                ...   \n",
       "4515               NaN  This Is the End of Affirmative Action. What ar...   \n",
       "4516               NaN  What do you mean? That nepotism comes from whi...   \n",
       "4517                 ​  How to have a conversation with an open-minded...   \n",
       "4518          New user    Democrats that want to expand education .Fun...   \n",
       "4519                6∆  I can't trust someone who argues from pure sel...   \n",
       "\n",
       "     subreddit  word_count subreddit_name  sr_is_asian  \\\n",
       "0     t5_2rfyw          78  asianamerican         True   \n",
       "1     t5_2rfyw          20  asianamerican         True   \n",
       "2     t5_2rfyw         103  asianamerican         True   \n",
       "3     t5_2rfyw         269  asianamerican         True   \n",
       "4     t5_2rfyw          59  asianamerican         True   \n",
       "...        ...         ...            ...          ...   \n",
       "4515  t5_2qhgd          15         racism        False   \n",
       "4516  t5_2qhgd          32         racism        False   \n",
       "4517  t5_38jid         237        MensLib        False   \n",
       "4518  t5_3amv4         349    aznidentity         True   \n",
       "4519  t5_2w2s8         180   changemyview        False   \n",
       "\n",
       "                                                 tokens  \\\n",
       "0     [[Megathread, Supreme, Court, Ruling, on, Affi...   \n",
       "1     [[Thanks, to, everyone, who, engaged, in, insi...   \n",
       "2     [[I, would, prefer, using, a, process, that, t...   \n",
       "3     [[u, Tungsten, Thanks, for, creating, a, secti...   \n",
       "4     [[As, with, anything, related, to, Asians, in,...   \n",
       "...                                                 ...   \n",
       "4515  [[This, Is, the, End, of, Affirmative, Action]...   \n",
       "4516  [[What, do, you, mean], [That, nepotism, comes...   \n",
       "4517  [[How, to, have, a, conversation, with, an, op...   \n",
       "4518  [[Democrats, that, want, to, expand, education...   \n",
       "4519  [[I, ca, n't, trust, someone, who, argues, fro...   \n",
       "\n",
       "                                      normalized_tokens  \n",
       "0     [[megathread, supreme, court, ruling, affirmat...  \n",
       "1     [[thanks, engaged, insightful, respectful, dis...  \n",
       "2     [[prefer, process, takes, account, poverty, in...  \n",
       "3     [[u, tungsten, thanks, creating, section, disc...  \n",
       "4     [[related, asians, politics, m, seeing, lot, n...  \n",
       "...                                                 ...  \n",
       "4515              [[end, affirmative, action], [going]]  \n",
       "4516  [[mean], [nepotism, comes, white, supremacy], ...  \n",
       "4517  [[conversation, open, minded, disadvantaged, g...  \n",
       "4518  [[democrats, want, expand, education, .funding...  \n",
       "4519  [[trust, argues, pure, self, interest, princip...  \n",
       "\n",
       "[4520 rows x 12 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'suppress_warnings' from 'numpy.testing' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-e70e92d32c6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\gensim\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \"\"\"\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummarization\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[1;31m# noqa:F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\gensim\\corpora\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# bring corpus classes directly into package namespace, to save some typing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mindexedcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mIndexedCorpus\u001b[0m  \u001b[1;31m# noqa:F401 must appear before the other classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmmcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMmCorpus\u001b[0m  \u001b[1;31m# noqa:F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\gensim\\corpora\\indexedcorpus.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mlogger\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\gensim\\interfaces.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\gensim\\matutils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mentropy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlapack\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_lapack_funcs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\scipy\\stats\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    439\u001b[0m \"\"\"\n\u001b[0;32m    440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 441\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmorestats\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\scipy\\stats\\stats.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mspecial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmstats_basic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m from ._stats_mstats_common import (_find_repeats, linregress, theilslopes,\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\scipy\\stats\\distributions.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#       instead of `git blame -Lxxx,+x`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_distn_infrastructure\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrv_discrete\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrv_continuous\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrv_frozen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_continuous_distns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# for root finding for continuous distribution ppf, and max likelihood\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# estimation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moptimize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m# for functions of continuous distributions (e.g. moments, entropy, cdf)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\scipy\\optimize\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 401\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_minimize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    402\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_root\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_root_scalar\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_minimize.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_trustregion_krylov\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_minimize_trust_krylov\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_trustregion_exact\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_minimize_trustregion_exact\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_trustregion_constr\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_minimize_trustregion_constr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# constrained minimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_trustregion_constr\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mminimize_trustregion_constr\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_minimize_trustregion_constr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0m__all__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'_minimize_trustregion_constr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_trustregion_constr\\minimize_trustregion_constr.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLinearOperator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_differentiable_functions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVectorFunction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m from .._constraints import (\n\u001b[0m\u001b[0;32m      6\u001b[0m     NonlinearConstraint, LinearConstraint, PreparedConstraint, strict_bounds)\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hessian_update_strategy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBFGS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_constraints.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOptimizeWarning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mwarnings\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msuppress_warnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'suppress_warnings' from 'numpy.testing' (unknown location)"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
