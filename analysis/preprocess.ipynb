{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "import spacy.symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4861, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>username</th>\n",
       "      <th>time_created</th>\n",
       "      <th>flair</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14m8mf4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-29 10:54:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Megathread] Supreme Court Ruling on Affirmati...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jq5du0z</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-30 11:33:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jq0dgzx</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>2023-06-29 11:16:15</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jq0cg7k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>2023-06-29 11:09:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jq0f52k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>2023-06-29 11:26:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4856</th>\n",
       "      <td>osjkh6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yellowmix</td>\n",
       "      <td>2021-07-27 09:32:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This Is the End of Affirmative Action. What ar...</td>\n",
       "      <td>t5_2qhgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4857</th>\n",
       "      <td>iw0q5sn</td>\n",
       "      <td>t3_yr5o90</td>\n",
       "      <td>yellowmix</td>\n",
       "      <td>2022-11-12 01:09:36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What do you mean? That nepotism comes from whi...</td>\n",
       "      <td>t5_2qhgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4858</th>\n",
       "      <td>uyzxgz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kamala_Metamorph</td>\n",
       "      <td>2022-05-27 14:52:16</td>\n",
       "      <td>​</td>\n",
       "      <td>How to have a conversation with an open-minded...</td>\n",
       "      <td>t5_38jid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4859</th>\n",
       "      <td>l20ftbs</td>\n",
       "      <td>t3_1cfw1ru</td>\n",
       "      <td>Extension_River_9901</td>\n",
       "      <td>2024-04-30 22:56:39</td>\n",
       "      <td>New user</td>\n",
       "      <td>Democrats that want to expand education .Fun...</td>\n",
       "      <td>t5_3amv4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4860</th>\n",
       "      <td>1khnmw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Swordbow</td>\n",
       "      <td>2013-08-16 14:46:17</td>\n",
       "      <td>6∆</td>\n",
       "      <td>I can't trust someone who argues from pure sel...</td>\n",
       "      <td>t5_2w2s8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4861 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id   parent_id              username         time_created  \\\n",
       "0     14m8mf4         NaN             Tungsten_  2023-06-29 10:54:44   \n",
       "1     jq5du0z  t3_14m8mf4             Tungsten_  2023-06-30 11:33:11   \n",
       "2     jq0dgzx  t3_14m8mf4        ProudBlackMatt  2023-06-29 11:16:15   \n",
       "3     jq0cg7k  t3_14m8mf4          TomatoCanned  2023-06-29 11:09:47   \n",
       "4     jq0f52k  t3_14m8mf4          bad-fengshui  2023-06-29 11:26:41   \n",
       "...       ...         ...                   ...                  ...   \n",
       "4856   osjkh6         NaN             yellowmix  2021-07-27 09:32:14   \n",
       "4857  iw0q5sn   t3_yr5o90             yellowmix  2022-11-12 01:09:36   \n",
       "4858   uyzxgz         NaN      Kamala_Metamorph  2022-05-27 14:52:16   \n",
       "4859  l20ftbs  t3_1cfw1ru  Extension_River_9901  2024-04-30 22:56:39   \n",
       "4860   1khnmw         NaN              Swordbow  2013-08-16 14:46:17   \n",
       "\n",
       "                 flair                                               body  \\\n",
       "0                  NaN  [Megathread] Supreme Court Ruling on Affirmati...   \n",
       "1                  NaN  Thanks to everyone who engaged in insightful a...   \n",
       "2     Chinese-American  I would prefer using a process that takes into...   \n",
       "3                  NaN  u/Tungsten_, Thanks for creating a section jus...   \n",
       "4                  NaN  As with anything related to Asians in politics...   \n",
       "...                ...                                                ...   \n",
       "4856               NaN  This Is the End of Affirmative Action. What ar...   \n",
       "4857               NaN  What do you mean? That nepotism comes from whi...   \n",
       "4858                 ​  How to have a conversation with an open-minded...   \n",
       "4859          New user    Democrats that want to expand education .Fun...   \n",
       "4860                6∆  I can't trust someone who argues from pure sel...   \n",
       "\n",
       "     subreddit  \n",
       "0     t5_2rfyw  \n",
       "1     t5_2rfyw  \n",
       "2     t5_2rfyw  \n",
       "3     t5_2rfyw  \n",
       "4     t5_2rfyw  \n",
       "...        ...  \n",
       "4856  t5_2qhgd  \n",
       "4857  t5_2qhgd  \n",
       "4858  t5_38jid  \n",
       "4859  t5_3amv4  \n",
       "4860  t5_2w2s8  \n",
       "\n",
       "[4861 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df = pd.read_csv('../data/full_df.csv')\n",
    "#\"C:\\Users\\danie\\Study\\MACSS\\macs30200\\scraper\\top_100_post_comments_user_time_text.txt\"\n",
    "print(comments_df.shape)\n",
    "comments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop rows with deleted text body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of deleted rows: 307\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>username</th>\n",
       "      <th>time_created</th>\n",
       "      <th>flair</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14m8mf4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-29 10:54:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Megathread] Supreme Court Ruling on Affirmati...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jq5du0z</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-30 11:33:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jq0dgzx</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>2023-06-29 11:16:15</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jq0cg7k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>2023-06-29 11:09:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jq0f52k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>2023-06-29 11:26:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4549</th>\n",
       "      <td>osjkh6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yellowmix</td>\n",
       "      <td>2021-07-27 09:32:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This Is the End of Affirmative Action. What ar...</td>\n",
       "      <td>t5_2qhgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4550</th>\n",
       "      <td>iw0q5sn</td>\n",
       "      <td>t3_yr5o90</td>\n",
       "      <td>yellowmix</td>\n",
       "      <td>2022-11-12 01:09:36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What do you mean? That nepotism comes from whi...</td>\n",
       "      <td>t5_2qhgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4551</th>\n",
       "      <td>uyzxgz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kamala_Metamorph</td>\n",
       "      <td>2022-05-27 14:52:16</td>\n",
       "      <td>​</td>\n",
       "      <td>How to have a conversation with an open-minded...</td>\n",
       "      <td>t5_38jid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4552</th>\n",
       "      <td>l20ftbs</td>\n",
       "      <td>t3_1cfw1ru</td>\n",
       "      <td>Extension_River_9901</td>\n",
       "      <td>2024-04-30 22:56:39</td>\n",
       "      <td>New user</td>\n",
       "      <td>Democrats that want to expand education .Fun...</td>\n",
       "      <td>t5_3amv4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4553</th>\n",
       "      <td>1khnmw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Swordbow</td>\n",
       "      <td>2013-08-16 14:46:17</td>\n",
       "      <td>6∆</td>\n",
       "      <td>I can't trust someone who argues from pure sel...</td>\n",
       "      <td>t5_2w2s8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4554 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id   parent_id              username         time_created  \\\n",
       "0     14m8mf4         NaN             Tungsten_  2023-06-29 10:54:44   \n",
       "1     jq5du0z  t3_14m8mf4             Tungsten_  2023-06-30 11:33:11   \n",
       "2     jq0dgzx  t3_14m8mf4        ProudBlackMatt  2023-06-29 11:16:15   \n",
       "3     jq0cg7k  t3_14m8mf4          TomatoCanned  2023-06-29 11:09:47   \n",
       "4     jq0f52k  t3_14m8mf4          bad-fengshui  2023-06-29 11:26:41   \n",
       "...       ...         ...                   ...                  ...   \n",
       "4549   osjkh6         NaN             yellowmix  2021-07-27 09:32:14   \n",
       "4550  iw0q5sn   t3_yr5o90             yellowmix  2022-11-12 01:09:36   \n",
       "4551   uyzxgz         NaN      Kamala_Metamorph  2022-05-27 14:52:16   \n",
       "4552  l20ftbs  t3_1cfw1ru  Extension_River_9901  2024-04-30 22:56:39   \n",
       "4553   1khnmw         NaN              Swordbow  2013-08-16 14:46:17   \n",
       "\n",
       "                 flair                                               body  \\\n",
       "0                  NaN  [Megathread] Supreme Court Ruling on Affirmati...   \n",
       "1                  NaN  Thanks to everyone who engaged in insightful a...   \n",
       "2     Chinese-American  I would prefer using a process that takes into...   \n",
       "3                  NaN  u/Tungsten_, Thanks for creating a section jus...   \n",
       "4                  NaN  As with anything related to Asians in politics...   \n",
       "...                ...                                                ...   \n",
       "4549               NaN  This Is the End of Affirmative Action. What ar...   \n",
       "4550               NaN  What do you mean? That nepotism comes from whi...   \n",
       "4551                 ​  How to have a conversation with an open-minded...   \n",
       "4552          New user    Democrats that want to expand education .Fun...   \n",
       "4553                6∆  I can't trust someone who argues from pure sel...   \n",
       "\n",
       "     subreddit  \n",
       "0     t5_2rfyw  \n",
       "1     t5_2rfyw  \n",
       "2     t5_2rfyw  \n",
       "3     t5_2rfyw  \n",
       "4     t5_2rfyw  \n",
       "...        ...  \n",
       "4549  t5_2qhgd  \n",
       "4550  t5_2qhgd  \n",
       "4551  t5_38jid  \n",
       "4552  t5_3amv4  \n",
       "4553  t5_2w2s8  \n",
       "\n",
       "[4554 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deleted_rows = comments_df[comments_df['body'].isin(['[deleted]','[removed]'])]\n",
    "print(f'Num of deleted rows: {deleted_rows.shape[0]}')\n",
    "\n",
    "# drop rows\n",
    "comments_df.drop(deleted_rows.index, inplace=True)\n",
    "comments_df.reset_index(drop=True, inplace=True)\n",
    "comments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: 307 rows were of deleted comments and so dropped from dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop rows that have been filtered by the AutoModerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of moderated rows: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>username</th>\n",
       "      <th>time_created</th>\n",
       "      <th>flair</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14m8mf4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-29 10:54:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Megathread] Supreme Court Ruling on Affirmati...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jq5du0z</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-30 11:33:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jq0dgzx</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>2023-06-29 11:16:15</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jq0cg7k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>2023-06-29 11:09:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jq0f52k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>2023-06-29 11:26:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4515</th>\n",
       "      <td>osjkh6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yellowmix</td>\n",
       "      <td>2021-07-27 09:32:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This Is the End of Affirmative Action. What ar...</td>\n",
       "      <td>t5_2qhgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4516</th>\n",
       "      <td>iw0q5sn</td>\n",
       "      <td>t3_yr5o90</td>\n",
       "      <td>yellowmix</td>\n",
       "      <td>2022-11-12 01:09:36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What do you mean? That nepotism comes from whi...</td>\n",
       "      <td>t5_2qhgd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4517</th>\n",
       "      <td>uyzxgz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kamala_Metamorph</td>\n",
       "      <td>2022-05-27 14:52:16</td>\n",
       "      <td>​</td>\n",
       "      <td>How to have a conversation with an open-minded...</td>\n",
       "      <td>t5_38jid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4518</th>\n",
       "      <td>l20ftbs</td>\n",
       "      <td>t3_1cfw1ru</td>\n",
       "      <td>Extension_River_9901</td>\n",
       "      <td>2024-04-30 22:56:39</td>\n",
       "      <td>New user</td>\n",
       "      <td>Democrats that want to expand education .Fun...</td>\n",
       "      <td>t5_3amv4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4519</th>\n",
       "      <td>1khnmw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Swordbow</td>\n",
       "      <td>2013-08-16 14:46:17</td>\n",
       "      <td>6∆</td>\n",
       "      <td>I can't trust someone who argues from pure sel...</td>\n",
       "      <td>t5_2w2s8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4520 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id   parent_id              username         time_created  \\\n",
       "0     14m8mf4         NaN             Tungsten_  2023-06-29 10:54:44   \n",
       "1     jq5du0z  t3_14m8mf4             Tungsten_  2023-06-30 11:33:11   \n",
       "2     jq0dgzx  t3_14m8mf4        ProudBlackMatt  2023-06-29 11:16:15   \n",
       "3     jq0cg7k  t3_14m8mf4          TomatoCanned  2023-06-29 11:09:47   \n",
       "4     jq0f52k  t3_14m8mf4          bad-fengshui  2023-06-29 11:26:41   \n",
       "...       ...         ...                   ...                  ...   \n",
       "4515   osjkh6         NaN             yellowmix  2021-07-27 09:32:14   \n",
       "4516  iw0q5sn   t3_yr5o90             yellowmix  2022-11-12 01:09:36   \n",
       "4517   uyzxgz         NaN      Kamala_Metamorph  2022-05-27 14:52:16   \n",
       "4518  l20ftbs  t3_1cfw1ru  Extension_River_9901  2024-04-30 22:56:39   \n",
       "4519   1khnmw         NaN              Swordbow  2013-08-16 14:46:17   \n",
       "\n",
       "                 flair                                               body  \\\n",
       "0                  NaN  [Megathread] Supreme Court Ruling on Affirmati...   \n",
       "1                  NaN  Thanks to everyone who engaged in insightful a...   \n",
       "2     Chinese-American  I would prefer using a process that takes into...   \n",
       "3                  NaN  u/Tungsten_, Thanks for creating a section jus...   \n",
       "4                  NaN  As with anything related to Asians in politics...   \n",
       "...                ...                                                ...   \n",
       "4515               NaN  This Is the End of Affirmative Action. What ar...   \n",
       "4516               NaN  What do you mean? That nepotism comes from whi...   \n",
       "4517                 ​  How to have a conversation with an open-minded...   \n",
       "4518          New user    Democrats that want to expand education .Fun...   \n",
       "4519                6∆  I can't trust someone who argues from pure sel...   \n",
       "\n",
       "     subreddit  \n",
       "0     t5_2rfyw  \n",
       "1     t5_2rfyw  \n",
       "2     t5_2rfyw  \n",
       "3     t5_2rfyw  \n",
       "4     t5_2rfyw  \n",
       "...        ...  \n",
       "4515  t5_2qhgd  \n",
       "4516  t5_2qhgd  \n",
       "4517  t5_38jid  \n",
       "4518  t5_3amv4  \n",
       "4519  t5_2w2s8  \n",
       "\n",
       "[4520 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moderated_rows = comments_df[comments_df['username']=='AutoModerator']\n",
    "print(f'Num of moderated rows: {moderated_rows.shape[0]}')\n",
    "\n",
    "comments_df.drop(moderated_rows.index, inplace=True)\n",
    "comments_df.reset_index(drop=True, inplace=True)\n",
    "comments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_str(str):\n",
    "    tokenized = []\n",
    "    doc = nlp(str)\n",
    "    for token in doc:\n",
    "        if not token.is_punct and len(token.text.strip()) > 0:\n",
    "            tokenized.append(token.text)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column of tokens\n",
    "comments_df['tokens'] = comments_df['body'].apply(lambda x: tokenize_str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>username</th>\n",
       "      <th>time_created</th>\n",
       "      <th>flair</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14m8mf4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-29 10:54:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Megathread] Supreme Court Ruling on Affirmati...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[Megathread, Supreme, Court, Ruling, on, Affir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jq5du0z</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-30 11:33:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[Thanks, to, everyone, who, engaged, in, insig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jq0dgzx</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>2023-06-29 11:16:15</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[I, would, prefer, using, a, process, that, ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jq0cg7k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>2023-06-29 11:09:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[u, Tungsten, Thanks, for, creating, a, sectio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jq0f52k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>2023-06-29 11:26:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[As, with, anything, related, to, Asians, in, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jq0mhrx</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Pancake_muncher</td>\n",
       "      <td>2023-06-29 12:12:39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yet colleges will allow alumni and doners in e...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[Yet, colleges, will, allow, alumni, and, done...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>jq0mlbs</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>suberry</td>\n",
       "      <td>2023-06-29 12:13:16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I just hated Affirmative Action as a distracti...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[I, just, hated, Affirmative, Action, as, a, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jq0cvvn</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Puzzled-Painter3301</td>\n",
       "      <td>2023-06-29 11:12:31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My own feeling is that I was never in love wit...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[My, own, feeling, is, that, I, was, never, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jq0jtzk</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>e9967780</td>\n",
       "      <td>2023-06-29 11:55:52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anti Asian racism whether against East Asians ...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[Anti, Asian, racism, whether, against, East, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jq0kfzq</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-06-29 11:59:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Can we overturn legacy and athlete admissions ...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[Can, we, overturn, legacy, and, athlete, admi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>jq0iyws</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>OkartoIceCream</td>\n",
       "      <td>2023-06-29 11:50:34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I want to remind people that in California, on...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[I, want, to, remind, people, that, in, Califo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>jq20w8p</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>ShalomHasaeyo</td>\n",
       "      <td>2023-06-29 17:36:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I think this is great news for our nation. Whi...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[I, think, this, is, great, news, for, our, na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>jq0b8ts</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>SteadfastEnd</td>\n",
       "      <td>2023-06-29 11:02:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My main concern is that, since the Court did n...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[My, main, concern, is, that, since, the, Cour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>jq2fp75</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>MsNewKicks</td>\n",
       "      <td>2023-06-29 19:26:21</td>\n",
       "      <td>First Of Her Name, Queen ABG, 나쁜 기집애, Blocker ...</td>\n",
       "      <td>There is obviously a lot to it and no real eas...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[There, is, obviously, a, lot, to, it, and, no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>jq0dxui</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>pal2002</td>\n",
       "      <td>2023-06-29 11:19:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I’m sure selective schools like Harvard will f...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[I, ’m, sure, selective, schools, like, Harvar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id   parent_id             username         time_created  \\\n",
       "0   14m8mf4         NaN            Tungsten_  2023-06-29 10:54:44   \n",
       "1   jq5du0z  t3_14m8mf4            Tungsten_  2023-06-30 11:33:11   \n",
       "2   jq0dgzx  t3_14m8mf4       ProudBlackMatt  2023-06-29 11:16:15   \n",
       "3   jq0cg7k  t3_14m8mf4         TomatoCanned  2023-06-29 11:09:47   \n",
       "4   jq0f52k  t3_14m8mf4         bad-fengshui  2023-06-29 11:26:41   \n",
       "5   jq0mhrx  t3_14m8mf4      Pancake_muncher  2023-06-29 12:12:39   \n",
       "6   jq0mlbs  t3_14m8mf4              suberry  2023-06-29 12:13:16   \n",
       "7   jq0cvvn  t3_14m8mf4  Puzzled-Painter3301  2023-06-29 11:12:31   \n",
       "8   jq0jtzk  t3_14m8mf4             e9967780  2023-06-29 11:55:52   \n",
       "9   jq0kfzq  t3_14m8mf4                  NaN  2023-06-29 11:59:41   \n",
       "10  jq0iyws  t3_14m8mf4       OkartoIceCream  2023-06-29 11:50:34   \n",
       "11  jq20w8p  t3_14m8mf4        ShalomHasaeyo  2023-06-29 17:36:41   \n",
       "12  jq0b8ts  t3_14m8mf4         SteadfastEnd  2023-06-29 11:02:09   \n",
       "13  jq2fp75  t3_14m8mf4           MsNewKicks  2023-06-29 19:26:21   \n",
       "14  jq0dxui  t3_14m8mf4              pal2002  2023-06-29 11:19:14   \n",
       "\n",
       "                                                flair  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                    Chinese-American   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6                                                 NaN   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11                                                NaN   \n",
       "12                                                NaN   \n",
       "13  First Of Her Name, Queen ABG, 나쁜 기집애, Blocker ...   \n",
       "14                                                NaN   \n",
       "\n",
       "                                                 body subreddit  \\\n",
       "0   [Megathread] Supreme Court Ruling on Affirmati...  t5_2rfyw   \n",
       "1   Thanks to everyone who engaged in insightful a...  t5_2rfyw   \n",
       "2   I would prefer using a process that takes into...  t5_2rfyw   \n",
       "3   u/Tungsten_, Thanks for creating a section jus...  t5_2rfyw   \n",
       "4   As with anything related to Asians in politics...  t5_2rfyw   \n",
       "5   Yet colleges will allow alumni and doners in e...  t5_2rfyw   \n",
       "6   I just hated Affirmative Action as a distracti...  t5_2rfyw   \n",
       "7   My own feeling is that I was never in love wit...  t5_2rfyw   \n",
       "8   Anti Asian racism whether against East Asians ...  t5_2rfyw   \n",
       "9   Can we overturn legacy and athlete admissions ...  t5_2rfyw   \n",
       "10  I want to remind people that in California, on...  t5_2rfyw   \n",
       "11  I think this is great news for our nation. Whi...  t5_2rfyw   \n",
       "12  My main concern is that, since the Court did n...  t5_2rfyw   \n",
       "13  There is obviously a lot to it and no real eas...  t5_2rfyw   \n",
       "14  I’m sure selective schools like Harvard will f...  t5_2rfyw   \n",
       "\n",
       "                                               tokens  \n",
       "0   [Megathread, Supreme, Court, Ruling, on, Affir...  \n",
       "1   [Thanks, to, everyone, who, engaged, in, insig...  \n",
       "2   [I, would, prefer, using, a, process, that, ta...  \n",
       "3   [u, Tungsten, Thanks, for, creating, a, sectio...  \n",
       "4   [As, with, anything, related, to, Asians, in, ...  \n",
       "5   [Yet, colleges, will, allow, alumni, and, done...  \n",
       "6   [I, just, hated, Affirmative, Action, as, a, d...  \n",
       "7   [My, own, feeling, is, that, I, was, never, in...  \n",
       "8   [Anti, Asian, racism, whether, against, East, ...  \n",
       "9   [Can, we, overturn, legacy, and, athlete, admi...  \n",
       "10  [I, want, to, remind, people, that, in, Califo...  \n",
       "11  [I, think, this, is, great, news, for, our, na...  \n",
       "12  [My, main, concern, is, that, since, the, Cour...  \n",
       "13  [There, is, obviously, a, lot, to, it, and, no...  \n",
       "14  [I, ’m, sure, selective, schools, like, Harvar...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem with Reddit comments: We don't want to get split on (get rid of) forward slashes nor get rid of punctuations on usernames like \"u/Tungsten_\"\n",
    "\n",
    "So, instead of using the native Spacy package, we use RedditScore which is built on Spacy but modified for Reddit/Twitter comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u/Tungsten_, Thanks for creating a section just to discuss this. When I read the news I immediately went searching for a forum where folks might have civil discourse on this topic.\n",
      "\n",
      "Just had a few comments/questions:\n",
      "\n",
      "1. Has anyone come across seemingly legitimate data sets on asians & college admission with respect to Affirmative Action (AA for short going forward)\n",
      "2. As an Asian (not born in the US but pretty much assimilated here for 35+ years), I am conflicted. Research results like this one show: [https://www.pewresearch.org/race-ethnicity/2023/06/08/asian-americans-hold-mixed-views-around-affirmative-action/](https://www.pewresearch.org/race-ethnicity/2023/06/08/asian-americans-hold-mixed-views-around-affirmative-action/) that something like 53% Asians think AA is a good thing, and yet when you scroll down and look at the question of \"Should colleges consider race/ethnicity in college admissions,\" the percentage of Asians that say yes are at 21%, no at 76%.\n",
      "\n",
      "I am part of the 76%.... and I'm conflicted. I know especially for the underserved, AA makes a significant impact in giving folks better chances at life which in turn translates to diversity in every facet of work, society, life in general, which I view is a good thing.\n",
      "\n",
      "But specifically regarding college admissions.. say for my own kids? (not college aged yet)  I would like to see more data on whether year 2000 and beyond AA in college admissions was harmful to Asians in general. In my own experience (anecdotal, totally not data science driven), I feel like AA in college admissions has hurt friends and family, in a reverse sort of sense.\n",
      "\n",
      "But for the sake of the underserved, I didn't want AA to go away. So I am deeply conflicted.\n",
      "\n",
      "Your thoughts?\n"
     ]
    }
   ],
   "source": [
    "sample_comment = comments_df['body'][2]\n",
    "print(sample_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special case in Spacy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'CrazyTokenizer' doesn't work bc the code is reliant on an old version of Spacy. So, we will create a special case in Spacy's tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'Hi', ',', 'u', '/', 'Tungsten', '_', ',', 'Thanks', 'for', 'creating', 'a', 'section']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "65",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-0950b2f4b101>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mspecial_case\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'TEXT'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'REGEX'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34mr'u\\/([[:word:]]|-){3,23}'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_special_case\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'redditor'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspecial_case\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' Hi, u/Tungsten_, Thanks for creating a section'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\spacy\\tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.add_special_case\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\spacy\\tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._validate_special_case\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 65"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "doc = nlp(' Hi, u/Tungsten_, Thanks for creating a section')\n",
    "print([w.text for w in doc])\n",
    "\n",
    "special_case = [{'TEXT': {'REGEX': r'u\\/([[:word:]]|-){3,23}'}}]\n",
    "nlp.tokenizer.add_special_case('redditor', special_case)\n",
    "doc = nlp(' Hi, u/Tungsten_, Thanks for creating a section')\n",
    "\n",
    "print(print([w.text for w in doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "nlp.tokenizer = Tokenizer(nlp.vocab, token_match=re.compile(r'u/{1}\\w{3,23}').search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_comment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-0e4e9aa983af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_comment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sample_comment' is not defined"
     ]
    }
   ],
   "source": [
    "doc = nlp(sample_comment)\n",
    "\n",
    "print([w.text for w in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u', 'Tungsten', 'Thanks', 'for', 'creating', 'a', 'section', 'just', 'to', 'discuss', 'this', 'When', 'I', 'read', 'the', 'news', 'I', 'immediately', 'went', 'searching', 'for', 'a', 'forum', 'where', 'folks', 'might', 'have', 'civil', 'discourse', 'on', 'this', 'topic', 'Just', 'had', 'a', 'few', 'comments', 'questions', '1', 'Has', 'anyone', 'come', 'across', 'seemingly', 'legitimate', 'data', 'sets', 'on', 'asians', 'college', 'admission', 'with', 'respect', 'to', 'Affirmative', 'Action', 'AA', 'for', 'short', 'going', 'forward', '2', 'As', 'an', 'Asian', 'not', 'born', 'in', 'the', 'US', 'but', 'pretty', 'much', 'assimilated', 'here', 'for', '35', '+', 'years', 'I', 'am', 'conflicted', 'Research', 'results', 'like', 'this', 'one', 'show', 'https://www.pewresearch.org/race-ethnicity/2023/06/08/asian-americans-hold-mixed-views-around-affirmative-action/](https://www.pewresearch.org/race-ethnicity/2023/06/08/asian-americans-hold-mixed-views-around-affirmative-action/', 'that', 'something', 'like', '53', 'Asians', 'think', 'AA', 'is', 'a', 'good', 'thing', 'and', 'yet', 'when', 'you', 'scroll', 'down', 'and', 'look', 'at', 'the', 'question', 'of', 'Should', 'colleges', 'consider', 'race', 'ethnicity', 'in', 'college', 'admissions', 'the', 'percentage', 'of', 'Asians', 'that', 'say', 'yes', 'are', 'at', '21', 'no', 'at', '76', 'I', 'am', 'part', 'of', 'the', '76', 'and', 'I', \"'m\", 'conflicted', 'I', 'know', 'especially', 'for', 'the', 'underserved', 'AA', 'makes', 'a', 'significant', 'impact', 'in', 'giving', 'folks', 'better', 'chances', 'at', 'life', 'which', 'in', 'turn', 'translates', 'to', 'diversity', 'in', 'every', 'facet', 'of', 'work', 'society', 'life', 'in', 'general', 'which', 'I', 'view', 'is', 'a', 'good', 'thing', 'But', 'specifically', 'regarding', 'college', 'admissions', 'say', 'for', 'my', 'own', 'kids', 'not', 'college', 'aged', 'yet', 'I', 'would', 'like', 'to', 'see', 'more', 'data', 'on', 'whether', 'year', '2000', 'and', 'beyond', 'AA', 'in', 'college', 'admissions', 'was', 'harmful', 'to', 'Asians', 'in', 'general', 'In', 'my', 'own', 'experience', 'anecdotal', 'totally', 'not', 'data', 'science', 'driven', 'I', 'feel', 'like', 'AA', 'in', 'college', 'admissions', 'has', 'hurt', 'friends', 'and', 'family', 'in', 'a', 'reverse', 'sort', 'of', 'sense', 'But', 'for', 'the', 'sake', 'of', 'the', 'underserved', 'I', 'did', \"n't\", 'want', 'AA', 'to', 'go', 'away', 'So', 'I', 'am', 'deeply', 'conflicted', 'Your', 'thoughts']\n"
     ]
    }
   ],
   "source": [
    "print(comments_df['tokens'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from above that the tokenization got worse when we added the token_match criteria. We need to modify the English class-attribute before loading model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_comment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-9b7f7fe41d96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#nlp.tokenizer.token_match = None (run line if results are same)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdoc1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_comment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sample_comment' is not defined"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#nlp.tokenizer.token_match = None (run line if results are same)\n",
    "doc1 = nlp(sample_comment)\n",
    "print([w.text for w in doc1])\n",
    "\n",
    "# add token_match to tokenizer\n",
    "nlp.tokenizer.token_match = re.compile(r'u/{1}\\w{3,23}').match\n",
    "doc2 = nlp(sample_comment)\n",
    "print([w.text for w in doc2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason, the reddit username includes the comma.\n",
    "*IDEA: We can strip the string of any characters that are not valid in a username.*\n",
    "\n",
    "Our tokenization of reddit usernames is not perfect, it may include trailing punctuation. But let's move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 11), match='u/Tungsten_'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 11), match='u/Tungsten_'>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#redditor_regex = re.compile(r'u/{1}\\w{3,23}')\n",
    "#print(redditor_regex.search(sample_comment))\n",
    "#redditor_regex.match(sample_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "      <td>[Thanks, to, everyone, who, engaged, in, insig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "      <td>[I, would, prefer, using, a, process, that, ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "      <td>[u, Tungsten, Thanks, for, creating, a, sectio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "      <td>[As, with, anything, related, to, Asians, in, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pancake_muncher</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yet colleges will allow alumni and doners in e...</td>\n",
       "      <td>[Yet, colleges, will, allow, alumni, and, done...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          username        flair_text  \\\n",
       "0        Tungsten_               NaN   \n",
       "1   ProudBlackMatt  Chinese-American   \n",
       "2     TomatoCanned               NaN   \n",
       "3     bad-fengshui               NaN   \n",
       "4  Pancake_muncher               NaN   \n",
       "\n",
       "                                                body  \\\n",
       "0  Thanks to everyone who engaged in insightful a...   \n",
       "1  I would prefer using a process that takes into...   \n",
       "2  u/Tungsten_, Thanks for creating a section jus...   \n",
       "3  As with anything related to Asians in politics...   \n",
       "4  Yet colleges will allow alumni and doners in e...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [Thanks, to, everyone, who, engaged, in, insig...  \n",
       "1  [I, would, prefer, using, a, process, that, ta...  \n",
       "2  [u, Tungsten, Thanks, for, creating, a, sectio...  \n",
       "3  [As, with, anything, related, to, Asians, in, ...  \n",
       "4  [Yet, colleges, will, allow, alumni, and, done...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.tokenizer.token_match = re.compile(r'u/{1}\\w{3,23}').match # match reddit usernames\n",
    "\n",
    "# drop old tokens column from data frame\n",
    "comments_df.drop(['tokens'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column to data frame\n",
    "comments_df['tokens_new'] = comments_df['body'].apply(lambda x: tokenize_str(x)) # takes a minute and half to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>username</th>\n",
       "      <th>time_created</th>\n",
       "      <th>flair</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>tokens_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14m8mf4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-29 10:54:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Megathread] Supreme Court Ruling on Affirmati...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[Megathread, Supreme, Court, Ruling, on, Affir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jq5du0z</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-30 11:33:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[Thanks, to, everyone, who, engaged, in, insig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jq0dgzx</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>2023-06-29 11:16:15</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[I, would, prefer, using, a, process, that, ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jq0cg7k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>2023-06-29 11:09:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[u/Tungsten_,, Thanks, for, creating, a, secti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jq0f52k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>2023-06-29 11:26:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[As, with, anything, related, to, Asians, in, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jq0mhrx</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Pancake_muncher</td>\n",
       "      <td>2023-06-29 12:12:39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yet colleges will allow alumni and doners in e...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[Yet, colleges, will, allow, alumni, and, done...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>jq0mlbs</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>suberry</td>\n",
       "      <td>2023-06-29 12:13:16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I just hated Affirmative Action as a distracti...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[I, just, hated, Affirmative, Action, as, a, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jq0cvvn</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Puzzled-Painter3301</td>\n",
       "      <td>2023-06-29 11:12:31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My own feeling is that I was never in love wit...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[My, own, feeling, is, that, I, was, never, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jq0jtzk</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>e9967780</td>\n",
       "      <td>2023-06-29 11:55:52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anti Asian racism whether against East Asians ...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[Anti, Asian, racism, whether, against, East, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jq0kfzq</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-06-29 11:59:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Can we overturn legacy and athlete admissions ...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[Can, we, overturn, legacy, and, athlete, admi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>jq0iyws</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>OkartoIceCream</td>\n",
       "      <td>2023-06-29 11:50:34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I want to remind people that in California, on...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[I, want, to, remind, people, that, in, Califo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>jq20w8p</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>ShalomHasaeyo</td>\n",
       "      <td>2023-06-29 17:36:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I think this is great news for our nation. Whi...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[I, think, this, is, great, news, for, our, na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>jq0b8ts</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>SteadfastEnd</td>\n",
       "      <td>2023-06-29 11:02:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My main concern is that, since the Court did n...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[My, main, concern, is, that, since, the, Cour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>jq2fp75</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>MsNewKicks</td>\n",
       "      <td>2023-06-29 19:26:21</td>\n",
       "      <td>First Of Her Name, Queen ABG, 나쁜 기집애, Blocker ...</td>\n",
       "      <td>There is obviously a lot to it and no real eas...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[There, is, obviously, a, lot, to, it, and, no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>jq0dxui</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>pal2002</td>\n",
       "      <td>2023-06-29 11:19:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I’m sure selective schools like Harvard will f...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[I, ’m, sure, selective, schools, like, Harvar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id   parent_id             username         time_created  \\\n",
       "0   14m8mf4         NaN            Tungsten_  2023-06-29 10:54:44   \n",
       "1   jq5du0z  t3_14m8mf4            Tungsten_  2023-06-30 11:33:11   \n",
       "2   jq0dgzx  t3_14m8mf4       ProudBlackMatt  2023-06-29 11:16:15   \n",
       "3   jq0cg7k  t3_14m8mf4         TomatoCanned  2023-06-29 11:09:47   \n",
       "4   jq0f52k  t3_14m8mf4         bad-fengshui  2023-06-29 11:26:41   \n",
       "5   jq0mhrx  t3_14m8mf4      Pancake_muncher  2023-06-29 12:12:39   \n",
       "6   jq0mlbs  t3_14m8mf4              suberry  2023-06-29 12:13:16   \n",
       "7   jq0cvvn  t3_14m8mf4  Puzzled-Painter3301  2023-06-29 11:12:31   \n",
       "8   jq0jtzk  t3_14m8mf4             e9967780  2023-06-29 11:55:52   \n",
       "9   jq0kfzq  t3_14m8mf4                  NaN  2023-06-29 11:59:41   \n",
       "10  jq0iyws  t3_14m8mf4       OkartoIceCream  2023-06-29 11:50:34   \n",
       "11  jq20w8p  t3_14m8mf4        ShalomHasaeyo  2023-06-29 17:36:41   \n",
       "12  jq0b8ts  t3_14m8mf4         SteadfastEnd  2023-06-29 11:02:09   \n",
       "13  jq2fp75  t3_14m8mf4           MsNewKicks  2023-06-29 19:26:21   \n",
       "14  jq0dxui  t3_14m8mf4              pal2002  2023-06-29 11:19:14   \n",
       "\n",
       "                                                flair  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                    Chinese-American   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6                                                 NaN   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11                                                NaN   \n",
       "12                                                NaN   \n",
       "13  First Of Her Name, Queen ABG, 나쁜 기집애, Blocker ...   \n",
       "14                                                NaN   \n",
       "\n",
       "                                                 body subreddit  \\\n",
       "0   [Megathread] Supreme Court Ruling on Affirmati...  t5_2rfyw   \n",
       "1   Thanks to everyone who engaged in insightful a...  t5_2rfyw   \n",
       "2   I would prefer using a process that takes into...  t5_2rfyw   \n",
       "3   u/Tungsten_, Thanks for creating a section jus...  t5_2rfyw   \n",
       "4   As with anything related to Asians in politics...  t5_2rfyw   \n",
       "5   Yet colleges will allow alumni and doners in e...  t5_2rfyw   \n",
       "6   I just hated Affirmative Action as a distracti...  t5_2rfyw   \n",
       "7   My own feeling is that I was never in love wit...  t5_2rfyw   \n",
       "8   Anti Asian racism whether against East Asians ...  t5_2rfyw   \n",
       "9   Can we overturn legacy and athlete admissions ...  t5_2rfyw   \n",
       "10  I want to remind people that in California, on...  t5_2rfyw   \n",
       "11  I think this is great news for our nation. Whi...  t5_2rfyw   \n",
       "12  My main concern is that, since the Court did n...  t5_2rfyw   \n",
       "13  There is obviously a lot to it and no real eas...  t5_2rfyw   \n",
       "14  I’m sure selective schools like Harvard will f...  t5_2rfyw   \n",
       "\n",
       "                                           tokens_new  \n",
       "0   [Megathread, Supreme, Court, Ruling, on, Affir...  \n",
       "1   [Thanks, to, everyone, who, engaged, in, insig...  \n",
       "2   [I, would, prefer, using, a, process, that, ta...  \n",
       "3   [u/Tungsten_,, Thanks, for, creating, a, secti...  \n",
       "4   [As, with, anything, related, to, Asians, in, ...  \n",
       "5   [Yet, colleges, will, allow, alumni, and, done...  \n",
       "6   [I, just, hated, Affirmative, Action, as, a, d...  \n",
       "7   [My, own, feeling, is, that, I, was, never, in...  \n",
       "8   [Anti, Asian, racism, whether, against, East, ...  \n",
       "9   [Can, we, overturn, legacy, and, athlete, admi...  \n",
       "10  [I, want, to, remind, people, that, in, Califo...  \n",
       "11  [I, think, this, is, great, news, for, our, na...  \n",
       "12  [My, main, concern, is, that, since, the, Cour...  \n",
       "13  [There, is, obviously, a, lot, to, it, and, no...  \n",
       "14  [I, ’m, sure, selective, schools, like, Harvar...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and normalize text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make all words lowercase (trivial)\n",
    "2. Drop non-word tokens (may already be done in tokenize_str() function)\n",
    "3. Remove stop-words (in a sophisticated manner)\n",
    "4. Stem words to remove suffixes, prefixes, infixes OR Lemmatize tokens (intelligently)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Remove stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 10504),\n",
       " ('to', 7373),\n",
       " ('of', 5903),\n",
       " ('and', 5694),\n",
       " ('a', 5293),\n",
       " ('that', 4814),\n",
       " ('in', 4436),\n",
       " ('i', 4103),\n",
       " ('is', 3984),\n",
       " ('it', 3495),\n",
       " ('are', 2626),\n",
       " ('you', 2537),\n",
       " ('for', 2504),\n",
       " ('asian', 2033),\n",
       " ('not', 2024),\n",
       " (\"'s\", 2019),\n",
       " ('this', 1859),\n",
       " ('as', 1843),\n",
       " (\"n't\", 1806),\n",
       " ('but', 1744),\n",
       " ('they', 1688),\n",
       " ('be', 1683),\n",
       " ('do', 1644),\n",
       " ('on', 1624),\n",
       " ('have', 1560),\n",
       " ('action', 1439),\n",
       " ('with', 1432),\n",
       " ('affirmative', 1415),\n",
       " ('we', 1334),\n",
       " ('asians', 1321),\n",
       " ('or', 1244),\n",
       " ('people', 1224),\n",
       " ('at', 1217),\n",
       " ('if', 1192),\n",
       " ('about', 1101),\n",
       " ('from', 1096),\n",
       " ('white', 1039),\n",
       " ('who', 1017),\n",
       " ('their', 981),\n",
       " ('more', 980),\n",
       " ('was', 959),\n",
       " ('there', 935),\n",
       " ('so', 900),\n",
       " ('an', 873),\n",
       " ('like', 846),\n",
       " ('just', 846),\n",
       " ('by', 844),\n",
       " ('what', 838),\n",
       " ('other', 837),\n",
       " ('because', 830)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stop words using word counts\n",
    "counts_dict = {}\n",
    "for word in comments_df['tokens_new'].sum():\n",
    "    word = word.lower()\n",
    "    if word in counts_dict:\n",
    "        counts_dict[word]+=1\n",
    "    else:\n",
    "        counts_dict[word] = 1\n",
    "\n",
    "word_counts = sorted(counts_dict.items(), key = lambda x: x[1], reverse=True)\n",
    "word_counts[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 11882),\n",
       " ('to', 8178),\n",
       " ('of', 6683),\n",
       " ('and', 6456),\n",
       " ('a', 5916),\n",
       " ('that', 5361),\n",
       " ('in', 4994),\n",
       " ('is', 4546),\n",
       " ('i', 4546),\n",
       " ('it', 4017),\n",
       " ('are', 2945),\n",
       " ('you', 2838),\n",
       " ('for', 2794),\n",
       " ('not', 2295),\n",
       " (\"'s\", 2276),\n",
       " ('asian', 2240),\n",
       " ('as', 2076),\n",
       " (\"n't\", 2060),\n",
       " ('this', 2042),\n",
       " ('but', 1954),\n",
       " ('they', 1902),\n",
       " ('be', 1866),\n",
       " ('do', 1863),\n",
       " ('on', 1860),\n",
       " ('have', 1776),\n",
       " ('action', 1647),\n",
       " ('affirmative', 1630),\n",
       " ('with', 1589),\n",
       " ('we', 1467),\n",
       " ('asians', 1454),\n",
       " ('people', 1420),\n",
       " ('or', 1383),\n",
       " ('at', 1350),\n",
       " ('if', 1316),\n",
       " ('from', 1274)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts[:35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mark/remove words as stop words that are more frequent than the first noun ('i')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'to', 'of', 'and', 'a', 'that', 'in']\n"
     ]
    }
   ],
   "source": [
    "stop_words_freq = []\n",
    "for word, count in word_counts:\n",
    "    if word == 'i':\n",
    "        break\n",
    "    else:\n",
    "        stop_words_freq.append(word)\n",
    "\n",
    "print(stop_words_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging from the list, we can add 'is' and 'it' to list of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'to', 'of', 'and', 'a', 'that', 'in', 'is', 'it', \"'m\", '>', '<']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_freq += ['is', 'it', \"'m\", '>', '<']\n",
    "stop_words_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to normalize tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tokens(word_list, extra_stop_words=[]):\n",
    "    normalized = []\n",
    "    if type(word_list) == list and len(word_list) == 1:\n",
    "        word_list = word_list[0]\n",
    "\n",
    "    if type(word_list) == list:\n",
    "        word_list = ' '.join([str(elem) for elem in word_list])\n",
    "\n",
    "    doc = nlp(word_list.lower()) # lowercase words in word_list\n",
    "\n",
    "    # add lexeme property of stopword to words considered as stopwords\n",
    "    if len(extra_stop_words) > 0:\n",
    "        for stopword in extra_stop_words:\n",
    "            lexeme = nlp.vocab[stopword]\n",
    "            lexeme.is_stop = True\n",
    "\n",
    "    for w in doc:\n",
    "        # if not stop word or punctuation, add it to normalized\n",
    "        if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num and len(w.text.strip()) > 0:\n",
    "            # add lemmatized version of word\n",
    "            normalized.append(str(w.lemma_))\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>username</th>\n",
       "      <th>time_created</th>\n",
       "      <th>flair</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>tokens_new</th>\n",
       "      <th>word_count</th>\n",
       "      <th>normalized_tokens</th>\n",
       "      <th>normalized_tokens_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14m8mf4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-29 10:54:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Megathread] Supreme Court Ruling on Affirmati...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[Megathread, Supreme, Court, Ruling, on, Affir...</td>\n",
       "      <td>78</td>\n",
       "      <td>[megathread, supreme, court, ruling, affirmati...</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jq5du0z</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-30 11:33:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[Thanks, to, everyone, who, engaged, in, insig...</td>\n",
       "      <td>20</td>\n",
       "      <td>[thank, engage, insightful, respectful, discou...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jq0dgzx</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>2023-06-29 11:16:15</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[I, would, prefer, using, a, process, that, ta...</td>\n",
       "      <td>103</td>\n",
       "      <td>[prefer, process, take, account, poverty, inst...</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jq0cg7k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>2023-06-29 11:09:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[u/Tungsten_,, Thanks, for, creating, a, secti...</td>\n",
       "      <td>269</td>\n",
       "      <td>[u/tungsten_,, thank, create, section, discuss...</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jq0f52k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>2023-06-29 11:26:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[As, with, anything, related, to, Asians, in, ...</td>\n",
       "      <td>59</td>\n",
       "      <td>[relate, asians, politic, m, see, lot, non, as...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jq0mhrx</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Pancake_muncher</td>\n",
       "      <td>2023-06-29 12:12:39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yet colleges will allow alumni and doners in e...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[Yet, colleges, will, allow, alumni, and, done...</td>\n",
       "      <td>40</td>\n",
       "      <td>[college, allow, alumnus, doner, easily, consi...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>jq0mlbs</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>suberry</td>\n",
       "      <td>2023-06-29 12:13:16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I just hated Affirmative Action as a distracti...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[I, just, hated, Affirmative, Action, as, a, d...</td>\n",
       "      <td>171</td>\n",
       "      <td>[hate, affirmative, action, distraction, banda...</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jq0cvvn</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Puzzled-Painter3301</td>\n",
       "      <td>2023-06-29 11:12:31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My own feeling is that I was never in love wit...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[My, own, feeling, is, that, I, was, never, in...</td>\n",
       "      <td>231</td>\n",
       "      <td>[feeling, love, affirmative, action, possible,...</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jq0jtzk</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>e9967780</td>\n",
       "      <td>2023-06-29 11:55:52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anti Asian racism whether against East Asians ...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[Anti, Asian, racism, whether, against, East, ...</td>\n",
       "      <td>46</td>\n",
       "      <td>[anti, asian, racism, east, asians, south, asi...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jq0kfzq</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-06-29 11:59:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Can we overturn legacy and athlete admissions ...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[Can, we, overturn, legacy, and, athlete, admi...</td>\n",
       "      <td>29</td>\n",
       "      <td>[overturn, legacy, athlete, admission, point, ...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>jq0iyws</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>OkartoIceCream</td>\n",
       "      <td>2023-06-29 11:50:34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I want to remind people that in California, on...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[I, want, to, remind, people, that, in, Califo...</td>\n",
       "      <td>200</td>\n",
       "      <td>[want, remind, people, california, progressive...</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>jq20w8p</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>ShalomHasaeyo</td>\n",
       "      <td>2023-06-29 17:36:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I think this is great news for our nation. Whi...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[I, think, this, is, great, news, for, our, na...</td>\n",
       "      <td>127</td>\n",
       "      <td>[think, great, news, nation, abolish, legacy, ...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>jq0b8ts</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>SteadfastEnd</td>\n",
       "      <td>2023-06-29 11:02:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My main concern is that, since the Court did n...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[My, main, concern, is, that, since, the, Cour...</td>\n",
       "      <td>141</td>\n",
       "      <td>[main, concern, court, strike, grutter, outrig...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>jq2fp75</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>MsNewKicks</td>\n",
       "      <td>2023-06-29 19:26:21</td>\n",
       "      <td>First Of Her Name, Queen ABG, 나쁜 기집애, Blocker ...</td>\n",
       "      <td>There is obviously a lot to it and no real eas...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[There, is, obviously, a, lot, to, it, and, no...</td>\n",
       "      <td>147</td>\n",
       "      <td>[obviously, lot, real, easy, answer, satisfy, ...</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>jq0dxui</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>pal2002</td>\n",
       "      <td>2023-06-29 11:19:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I’m sure selective schools like Harvard will f...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[I, ’m, sure, selective, schools, like, Harvar...</td>\n",
       "      <td>48</td>\n",
       "      <td>[m, sure, selective, school, like, harvard, fi...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>jq0w387</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Ok-Value5827</td>\n",
       "      <td>2023-06-29 13:12:38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TBH, I don't know the depth of how affirmative...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[TBH, I, do, n't, know, the, depth, of, how, a...</td>\n",
       "      <td>84</td>\n",
       "      <td>[tbh, know, depth, affirmative, action, colleg...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>jq22me5</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>j3ychen</td>\n",
       "      <td>2023-06-29 17:49:07</td>\n",
       "      <td>Taiwanese</td>\n",
       "      <td>People can continue to debate the legal implic...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[People, can, continue, to, debate, the, legal...</td>\n",
       "      <td>90</td>\n",
       "      <td>[people, continue, debate, legal, implication,...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>jq28kbb</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>HappyPineapple11</td>\n",
       "      <td>2023-06-29 18:32:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The only issue with affirmative action was tha...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[The, only, issue, with, affirmative, action, ...</td>\n",
       "      <td>199</td>\n",
       "      <td>[issue, affirmative, action, explicit, quota, ...</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>jq0heyu</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>alandizzle</td>\n",
       "      <td>2023-06-29 11:40:55</td>\n",
       "      <td>I'm Asian. Hi.</td>\n",
       "      <td>You know what I hate?\\n\\nThe main subreddits w...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[You, know, what, I, hate, The, main, subreddi...</td>\n",
       "      <td>453</td>\n",
       "      <td>[know, hate, main, subreddit, certain, white, ...</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>jq28fcf</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>gamesrgreat</td>\n",
       "      <td>2023-06-29 18:31:37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I’m not a fan of affirmative action even tho i...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[I, ’m, not, a, fan, of, affirmative, action, ...</td>\n",
       "      <td>87</td>\n",
       "      <td>[m, fan, affirmative, action, tho, help, minor...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id   parent_id             username         time_created  \\\n",
       "0   14m8mf4         NaN            Tungsten_  2023-06-29 10:54:44   \n",
       "1   jq5du0z  t3_14m8mf4            Tungsten_  2023-06-30 11:33:11   \n",
       "2   jq0dgzx  t3_14m8mf4       ProudBlackMatt  2023-06-29 11:16:15   \n",
       "3   jq0cg7k  t3_14m8mf4         TomatoCanned  2023-06-29 11:09:47   \n",
       "4   jq0f52k  t3_14m8mf4         bad-fengshui  2023-06-29 11:26:41   \n",
       "5   jq0mhrx  t3_14m8mf4      Pancake_muncher  2023-06-29 12:12:39   \n",
       "6   jq0mlbs  t3_14m8mf4              suberry  2023-06-29 12:13:16   \n",
       "7   jq0cvvn  t3_14m8mf4  Puzzled-Painter3301  2023-06-29 11:12:31   \n",
       "8   jq0jtzk  t3_14m8mf4             e9967780  2023-06-29 11:55:52   \n",
       "9   jq0kfzq  t3_14m8mf4                  NaN  2023-06-29 11:59:41   \n",
       "10  jq0iyws  t3_14m8mf4       OkartoIceCream  2023-06-29 11:50:34   \n",
       "11  jq20w8p  t3_14m8mf4        ShalomHasaeyo  2023-06-29 17:36:41   \n",
       "12  jq0b8ts  t3_14m8mf4         SteadfastEnd  2023-06-29 11:02:09   \n",
       "13  jq2fp75  t3_14m8mf4           MsNewKicks  2023-06-29 19:26:21   \n",
       "14  jq0dxui  t3_14m8mf4              pal2002  2023-06-29 11:19:14   \n",
       "15  jq0w387  t3_14m8mf4         Ok-Value5827  2023-06-29 13:12:38   \n",
       "16  jq22me5  t3_14m8mf4              j3ychen  2023-06-29 17:49:07   \n",
       "17  jq28kbb  t3_14m8mf4     HappyPineapple11  2023-06-29 18:32:41   \n",
       "18  jq0heyu  t3_14m8mf4           alandizzle  2023-06-29 11:40:55   \n",
       "19  jq28fcf  t3_14m8mf4          gamesrgreat  2023-06-29 18:31:37   \n",
       "\n",
       "                                                flair  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                    Chinese-American   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6                                                 NaN   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11                                                NaN   \n",
       "12                                                NaN   \n",
       "13  First Of Her Name, Queen ABG, 나쁜 기집애, Blocker ...   \n",
       "14                                                NaN   \n",
       "15                                                NaN   \n",
       "16                                          Taiwanese   \n",
       "17                                                NaN   \n",
       "18                                     I'm Asian. Hi.   \n",
       "19                                                NaN   \n",
       "\n",
       "                                                 body subreddit  \\\n",
       "0   [Megathread] Supreme Court Ruling on Affirmati...  t5_2rfyw   \n",
       "1   Thanks to everyone who engaged in insightful a...  t5_2rfyw   \n",
       "2   I would prefer using a process that takes into...  t5_2rfyw   \n",
       "3   u/Tungsten_, Thanks for creating a section jus...  t5_2rfyw   \n",
       "4   As with anything related to Asians in politics...  t5_2rfyw   \n",
       "5   Yet colleges will allow alumni and doners in e...  t5_2rfyw   \n",
       "6   I just hated Affirmative Action as a distracti...  t5_2rfyw   \n",
       "7   My own feeling is that I was never in love wit...  t5_2rfyw   \n",
       "8   Anti Asian racism whether against East Asians ...  t5_2rfyw   \n",
       "9   Can we overturn legacy and athlete admissions ...  t5_2rfyw   \n",
       "10  I want to remind people that in California, on...  t5_2rfyw   \n",
       "11  I think this is great news for our nation. Whi...  t5_2rfyw   \n",
       "12  My main concern is that, since the Court did n...  t5_2rfyw   \n",
       "13  There is obviously a lot to it and no real eas...  t5_2rfyw   \n",
       "14  I’m sure selective schools like Harvard will f...  t5_2rfyw   \n",
       "15  TBH, I don't know the depth of how affirmative...  t5_2rfyw   \n",
       "16  People can continue to debate the legal implic...  t5_2rfyw   \n",
       "17  The only issue with affirmative action was tha...  t5_2rfyw   \n",
       "18  You know what I hate?\\n\\nThe main subreddits w...  t5_2rfyw   \n",
       "19  I’m not a fan of affirmative action even tho i...  t5_2rfyw   \n",
       "\n",
       "                                           tokens_new  word_count  \\\n",
       "0   [Megathread, Supreme, Court, Ruling, on, Affir...          78   \n",
       "1   [Thanks, to, everyone, who, engaged, in, insig...          20   \n",
       "2   [I, would, prefer, using, a, process, that, ta...         103   \n",
       "3   [u/Tungsten_,, Thanks, for, creating, a, secti...         269   \n",
       "4   [As, with, anything, related, to, Asians, in, ...          59   \n",
       "5   [Yet, colleges, will, allow, alumni, and, done...          40   \n",
       "6   [I, just, hated, Affirmative, Action, as, a, d...         171   \n",
       "7   [My, own, feeling, is, that, I, was, never, in...         231   \n",
       "8   [Anti, Asian, racism, whether, against, East, ...          46   \n",
       "9   [Can, we, overturn, legacy, and, athlete, admi...          29   \n",
       "10  [I, want, to, remind, people, that, in, Califo...         200   \n",
       "11  [I, think, this, is, great, news, for, our, na...         127   \n",
       "12  [My, main, concern, is, that, since, the, Cour...         141   \n",
       "13  [There, is, obviously, a, lot, to, it, and, no...         147   \n",
       "14  [I, ’m, sure, selective, schools, like, Harvar...          48   \n",
       "15  [TBH, I, do, n't, know, the, depth, of, how, a...          84   \n",
       "16  [People, can, continue, to, debate, the, legal...          90   \n",
       "17  [The, only, issue, with, affirmative, action, ...         199   \n",
       "18  [You, know, what, I, hate, The, main, subreddi...         453   \n",
       "19  [I, ’m, not, a, fan, of, affirmative, action, ...          87   \n",
       "\n",
       "                                    normalized_tokens  normalized_tokens_count  \n",
       "0   [megathread, supreme, court, ruling, affirmati...                       62  \n",
       "1   [thank, engage, insightful, respectful, discou...                        9  \n",
       "2   [prefer, process, take, account, poverty, inst...                       52  \n",
       "3   [u/tungsten_,, thank, create, section, discuss...                      126  \n",
       "4   [relate, asians, politic, m, see, lot, non, as...                       25  \n",
       "5   [college, allow, alumnus, doner, easily, consi...                       19  \n",
       "6   [hate, affirmative, action, distraction, banda...                       78  \n",
       "7   [feeling, love, affirmative, action, possible,...                      102  \n",
       "8   [anti, asian, racism, east, asians, south, asi...                       21  \n",
       "9   [overturn, legacy, athlete, admission, point, ...                       15  \n",
       "10  [want, remind, people, california, progressive...                      104  \n",
       "11  [think, great, news, nation, abolish, legacy, ...                       58  \n",
       "12  [main, concern, court, strike, grutter, outrig...                       72  \n",
       "13  [obviously, lot, real, easy, answer, satisfy, ...                       46  \n",
       "14  [m, sure, selective, school, like, harvard, fi...                       20  \n",
       "15  [tbh, know, depth, affirmative, action, colleg...                       39  \n",
       "16  [people, continue, debate, legal, implication,...                       42  \n",
       "17  [issue, affirmative, action, explicit, quota, ...                       84  \n",
       "18  [know, hate, main, subreddit, certain, white, ...                      223  \n",
       "19  [m, fan, affirmative, action, tho, help, minor...                       40  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add normalized tokens and count columns\n",
    "comments_df['word_count'] = comments_df['tokens_new'].apply(lambda x: len(x))\n",
    "comments_df['normalized_tokens'] = comments_df['tokens_new'].apply(lambda x: normalize_tokens(x))\n",
    "comments_df['normalized_tokens_count'] = comments_df['normalized_tokens'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>username</th>\n",
       "      <th>time_created</th>\n",
       "      <th>flair</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>tokens_new</th>\n",
       "      <th>word_count</th>\n",
       "      <th>normalized_tokens</th>\n",
       "      <th>normalized_tokens_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14m8mf4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-29 10:54:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Megathread] Supreme Court Ruling on Affirmati...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[Megathread, Supreme, Court, Ruling, on, Affir...</td>\n",
       "      <td>78</td>\n",
       "      <td>[megathread, supreme, court, ruling, affirmati...</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jq5du0z</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-30 11:33:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[Thanks, to, everyone, who, engaged, in, insig...</td>\n",
       "      <td>20</td>\n",
       "      <td>[thank, engage, insightful, respectful, discou...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jq0dgzx</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>2023-06-29 11:16:15</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[I, would, prefer, using, a, process, that, ta...</td>\n",
       "      <td>103</td>\n",
       "      <td>[prefer, process, take, account, poverty, inst...</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jq0cg7k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>2023-06-29 11:09:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[u/Tungsten_,, Thanks, for, creating, a, secti...</td>\n",
       "      <td>269</td>\n",
       "      <td>[u/tungsten_,, thank, create, section, discuss...</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jq0f52k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>2023-06-29 11:26:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "      <td>[As, with, anything, related, to, Asians, in, ...</td>\n",
       "      <td>59</td>\n",
       "      <td>[relate, asians, politic, m, see, lot, non, as...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4515</th>\n",
       "      <td>osjkh6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yellowmix</td>\n",
       "      <td>2021-07-27 09:32:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>This Is the End of Affirmative Action. What ar...</td>\n",
       "      <td>t5_2qhgd</td>\n",
       "      <td>[This, Is, the, End, of, Affirmative, Action, ...</td>\n",
       "      <td>15</td>\n",
       "      <td>[end, affirmative, action, go]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4516</th>\n",
       "      <td>iw0q5sn</td>\n",
       "      <td>t3_yr5o90</td>\n",
       "      <td>yellowmix</td>\n",
       "      <td>2022-11-12 01:09:36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What do you mean? That nepotism comes from whi...</td>\n",
       "      <td>t5_2qhgd</td>\n",
       "      <td>[What, do, you, mean, That, nepotism, comes, f...</td>\n",
       "      <td>32</td>\n",
       "      <td>[mean, nepotism, come, white, supremacy, nepot...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4517</th>\n",
       "      <td>uyzxgz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kamala_Metamorph</td>\n",
       "      <td>2022-05-27 14:52:16</td>\n",
       "      <td>​</td>\n",
       "      <td>How to have a conversation with an open-minded...</td>\n",
       "      <td>t5_38jid</td>\n",
       "      <td>[How, to, have, a, conversation, with, an, ope...</td>\n",
       "      <td>237</td>\n",
       "      <td>[conversation, open, minded, disadvantaged, gu...</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4518</th>\n",
       "      <td>l20ftbs</td>\n",
       "      <td>t3_1cfw1ru</td>\n",
       "      <td>Extension_River_9901</td>\n",
       "      <td>2024-04-30 22:56:39</td>\n",
       "      <td>New user</td>\n",
       "      <td>Democrats that want to expand education .Fun...</td>\n",
       "      <td>t5_3amv4</td>\n",
       "      <td>[Democrats, that, want, to, expand, education,...</td>\n",
       "      <td>349</td>\n",
       "      <td>[democrats, want, expand, education, .funding,...</td>\n",
       "      <td>197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4519</th>\n",
       "      <td>1khnmw</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Swordbow</td>\n",
       "      <td>2013-08-16 14:46:17</td>\n",
       "      <td>6∆</td>\n",
       "      <td>I can't trust someone who argues from pure sel...</td>\n",
       "      <td>t5_2w2s8</td>\n",
       "      <td>[I, ca, n't, trust, someone, who, argues, from...</td>\n",
       "      <td>180</td>\n",
       "      <td>[trust, argue, pure, self, interest, principle...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4520 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id   parent_id              username         time_created  \\\n",
       "0     14m8mf4         NaN             Tungsten_  2023-06-29 10:54:44   \n",
       "1     jq5du0z  t3_14m8mf4             Tungsten_  2023-06-30 11:33:11   \n",
       "2     jq0dgzx  t3_14m8mf4        ProudBlackMatt  2023-06-29 11:16:15   \n",
       "3     jq0cg7k  t3_14m8mf4          TomatoCanned  2023-06-29 11:09:47   \n",
       "4     jq0f52k  t3_14m8mf4          bad-fengshui  2023-06-29 11:26:41   \n",
       "...       ...         ...                   ...                  ...   \n",
       "4515   osjkh6         NaN             yellowmix  2021-07-27 09:32:14   \n",
       "4516  iw0q5sn   t3_yr5o90             yellowmix  2022-11-12 01:09:36   \n",
       "4517   uyzxgz         NaN      Kamala_Metamorph  2022-05-27 14:52:16   \n",
       "4518  l20ftbs  t3_1cfw1ru  Extension_River_9901  2024-04-30 22:56:39   \n",
       "4519   1khnmw         NaN              Swordbow  2013-08-16 14:46:17   \n",
       "\n",
       "                 flair                                               body  \\\n",
       "0                  NaN  [Megathread] Supreme Court Ruling on Affirmati...   \n",
       "1                  NaN  Thanks to everyone who engaged in insightful a...   \n",
       "2     Chinese-American  I would prefer using a process that takes into...   \n",
       "3                  NaN  u/Tungsten_, Thanks for creating a section jus...   \n",
       "4                  NaN  As with anything related to Asians in politics...   \n",
       "...                ...                                                ...   \n",
       "4515               NaN  This Is the End of Affirmative Action. What ar...   \n",
       "4516               NaN  What do you mean? That nepotism comes from whi...   \n",
       "4517                 ​  How to have a conversation with an open-minded...   \n",
       "4518          New user    Democrats that want to expand education .Fun...   \n",
       "4519                6∆  I can't trust someone who argues from pure sel...   \n",
       "\n",
       "     subreddit                                         tokens_new  word_count  \\\n",
       "0     t5_2rfyw  [Megathread, Supreme, Court, Ruling, on, Affir...          78   \n",
       "1     t5_2rfyw  [Thanks, to, everyone, who, engaged, in, insig...          20   \n",
       "2     t5_2rfyw  [I, would, prefer, using, a, process, that, ta...         103   \n",
       "3     t5_2rfyw  [u/Tungsten_,, Thanks, for, creating, a, secti...         269   \n",
       "4     t5_2rfyw  [As, with, anything, related, to, Asians, in, ...          59   \n",
       "...        ...                                                ...         ...   \n",
       "4515  t5_2qhgd  [This, Is, the, End, of, Affirmative, Action, ...          15   \n",
       "4516  t5_2qhgd  [What, do, you, mean, That, nepotism, comes, f...          32   \n",
       "4517  t5_38jid  [How, to, have, a, conversation, with, an, ope...         237   \n",
       "4518  t5_3amv4  [Democrats, that, want, to, expand, education,...         349   \n",
       "4519  t5_2w2s8  [I, ca, n't, trust, someone, who, argues, from...         180   \n",
       "\n",
       "                                      normalized_tokens  \\\n",
       "0     [megathread, supreme, court, ruling, affirmati...   \n",
       "1     [thank, engage, insightful, respectful, discou...   \n",
       "2     [prefer, process, take, account, poverty, inst...   \n",
       "3     [u/tungsten_,, thank, create, section, discuss...   \n",
       "4     [relate, asians, politic, m, see, lot, non, as...   \n",
       "...                                                 ...   \n",
       "4515                     [end, affirmative, action, go]   \n",
       "4516  [mean, nepotism, come, white, supremacy, nepot...   \n",
       "4517  [conversation, open, minded, disadvantaged, gu...   \n",
       "4518  [democrats, want, expand, education, .funding,...   \n",
       "4519  [trust, argue, pure, self, interest, principle...   \n",
       "\n",
       "      normalized_tokens_count  \n",
       "0                          62  \n",
       "1                           9  \n",
       "2                          52  \n",
       "3                         126  \n",
       "4                          25  \n",
       "...                       ...  \n",
       "4515                        4  \n",
       "4516                       15  \n",
       "4517                      117  \n",
       "4518                      197  \n",
       "4519                       72  \n",
       "\n",
       "[4520 rows x 11 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# check if normalized_tokens column is list or string - Result: list\n",
    "print(type(comments_df['normalized_tokens'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save above df as csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df.to_csv('../data/full_df_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comments_processed.csv\n"
     ]
    }
   ],
   "source": [
    "input_file = 'comments.csv'\n",
    "output_file = input_file[0:-4] + '_processed' + input_file[-4:]\n",
    "\n",
    "print(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_fq_dist = nltk.ConditionalFreqDist(((len(w), w) for w in comments_df['normalized_tokens'].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130955\n"
     ]
    }
   ],
   "source": [
    "print(comments_fq_dist.N()) # number of total words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('asian', 2442),\n",
       " ('white', 1321),\n",
       " ('think', 956),\n",
       " ('black', 787),\n",
       " ('group', 573)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_fq_dist[5].most_common(5) #most common 5-letter words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d9defa72c2715dab9f7f172572cd30a1ab1a2083462d32ef96aadb7c6e0c73b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
