{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "import spacy.symbols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3316, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>username</th>\n",
       "      <th>time_created</th>\n",
       "      <th>flair</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14m8mf4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-29 10:54:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Megathread] Supreme Court Ruling on Affirmati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jq5du0z</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-30 11:33:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jq0dgzx</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>2023-06-29 11:16:15</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jq0cg7k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>2023-06-29 11:09:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jq0f52k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>2023-06-29 11:26:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jq0mhrx</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Pancake_muncher</td>\n",
       "      <td>2023-06-29 12:12:39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yet colleges will allow alumni and doners in e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>jq0mlbs</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>suberry</td>\n",
       "      <td>2023-06-29 12:13:16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I just hated Affirmative Action as a distracti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jq0cvvn</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Puzzled-Painter3301</td>\n",
       "      <td>2023-06-29 11:12:31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My own feeling is that I was never in love wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jq0jtzk</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>e9967780</td>\n",
       "      <td>2023-06-29 11:55:52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anti Asian racism whether against East Asians ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jq0kfzq</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-06-29 11:59:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Can we overturn legacy and athlete admissions ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   parent_id             username         time_created  \\\n",
       "0  14m8mf4         NaN            Tungsten_  2023-06-29 10:54:44   \n",
       "1  jq5du0z  t3_14m8mf4            Tungsten_  2023-06-30 11:33:11   \n",
       "2  jq0dgzx  t3_14m8mf4       ProudBlackMatt  2023-06-29 11:16:15   \n",
       "3  jq0cg7k  t3_14m8mf4         TomatoCanned  2023-06-29 11:09:47   \n",
       "4  jq0f52k  t3_14m8mf4         bad-fengshui  2023-06-29 11:26:41   \n",
       "5  jq0mhrx  t3_14m8mf4      Pancake_muncher  2023-06-29 12:12:39   \n",
       "6  jq0mlbs  t3_14m8mf4              suberry  2023-06-29 12:13:16   \n",
       "7  jq0cvvn  t3_14m8mf4  Puzzled-Painter3301  2023-06-29 11:12:31   \n",
       "8  jq0jtzk  t3_14m8mf4             e9967780  2023-06-29 11:55:52   \n",
       "9  jq0kfzq  t3_14m8mf4                  NaN  2023-06-29 11:59:41   \n",
       "\n",
       "              flair                                               body  \n",
       "0               NaN  [Megathread] Supreme Court Ruling on Affirmati...  \n",
       "1               NaN  Thanks to everyone who engaged in insightful a...  \n",
       "2  Chinese-American  I would prefer using a process that takes into...  \n",
       "3               NaN  u/Tungsten_, Thanks for creating a section jus...  \n",
       "4               NaN  As with anything related to Asians in politics...  \n",
       "5               NaN  Yet colleges will allow alumni and doners in e...  \n",
       "6               NaN  I just hated Affirmative Action as a distracti...  \n",
       "7               NaN  My own feeling is that I was never in love wit...  \n",
       "8               NaN  Anti Asian racism whether against East Asians ...  \n",
       "9               NaN  Can we overturn legacy and athlete admissions ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df = pd.read_csv('C:/Users/danie/Study/MACSS/macs30200/scraper/top_100_post_comments_no_chinese.txt', header=None, names=['id', 'parent_id','username', 'time_created', 'flair', 'body'])\n",
    "#\"C:\\Users\\danie\\Study\\MACSS\\macs30200\\scraper\\top_100_post_comments_user_time_text.txt\"\n",
    "print(comments_df.shape)\n",
    "comments_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop rows with deleted text body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3054, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>username</th>\n",
       "      <th>time_created</th>\n",
       "      <th>flair</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14m8mf4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-29 10:54:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Megathread] Supreme Court Ruling on Affirmati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jq5du0z</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-30 11:33:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jq0dgzx</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>2023-06-29 11:16:15</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jq0cg7k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>2023-06-29 11:09:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jq0f52k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>2023-06-29 11:26:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jq0mhrx</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Pancake_muncher</td>\n",
       "      <td>2023-06-29 12:12:39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yet colleges will allow alumni and doners in e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>jq0mlbs</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>suberry</td>\n",
       "      <td>2023-06-29 12:13:16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I just hated Affirmative Action as a distracti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jq0cvvn</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Puzzled-Painter3301</td>\n",
       "      <td>2023-06-29 11:12:31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My own feeling is that I was never in love wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jq0jtzk</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>e9967780</td>\n",
       "      <td>2023-06-29 11:55:52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anti Asian racism whether against East Asians ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jq0kfzq</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-06-29 11:59:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Can we overturn legacy and athlete admissions ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>jq0iyws</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>OkartoIceCream</td>\n",
       "      <td>2023-06-29 11:50:34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I want to remind people that in California, on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>jq20w8p</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>ShalomHasaeyo</td>\n",
       "      <td>2023-06-29 17:36:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I think this is great news for our nation. Whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>jq0b8ts</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>SteadfastEnd</td>\n",
       "      <td>2023-06-29 11:02:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My main concern is that, since the Court did n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>jq2fp75</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>MsNewKicks</td>\n",
       "      <td>2023-06-29 19:26:21</td>\n",
       "      <td>First Of Her Name, Queen ABG, 나쁜 기집애, Blocker ...</td>\n",
       "      <td>There is obviously a lot to it and no real eas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>jq0dxui</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>pal2002</td>\n",
       "      <td>2023-06-29 11:19:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I’m sure selective schools like Harvard will f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id   parent_id             username         time_created  \\\n",
       "0   14m8mf4         NaN            Tungsten_  2023-06-29 10:54:44   \n",
       "1   jq5du0z  t3_14m8mf4            Tungsten_  2023-06-30 11:33:11   \n",
       "2   jq0dgzx  t3_14m8mf4       ProudBlackMatt  2023-06-29 11:16:15   \n",
       "3   jq0cg7k  t3_14m8mf4         TomatoCanned  2023-06-29 11:09:47   \n",
       "4   jq0f52k  t3_14m8mf4         bad-fengshui  2023-06-29 11:26:41   \n",
       "5   jq0mhrx  t3_14m8mf4      Pancake_muncher  2023-06-29 12:12:39   \n",
       "6   jq0mlbs  t3_14m8mf4              suberry  2023-06-29 12:13:16   \n",
       "7   jq0cvvn  t3_14m8mf4  Puzzled-Painter3301  2023-06-29 11:12:31   \n",
       "8   jq0jtzk  t3_14m8mf4             e9967780  2023-06-29 11:55:52   \n",
       "9   jq0kfzq  t3_14m8mf4                  NaN  2023-06-29 11:59:41   \n",
       "10  jq0iyws  t3_14m8mf4       OkartoIceCream  2023-06-29 11:50:34   \n",
       "11  jq20w8p  t3_14m8mf4        ShalomHasaeyo  2023-06-29 17:36:41   \n",
       "12  jq0b8ts  t3_14m8mf4         SteadfastEnd  2023-06-29 11:02:09   \n",
       "13  jq2fp75  t3_14m8mf4           MsNewKicks  2023-06-29 19:26:21   \n",
       "15  jq0dxui  t3_14m8mf4              pal2002  2023-06-29 11:19:14   \n",
       "\n",
       "                                                flair  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                    Chinese-American   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6                                                 NaN   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11                                                NaN   \n",
       "12                                                NaN   \n",
       "13  First Of Her Name, Queen ABG, 나쁜 기집애, Blocker ...   \n",
       "15                                                NaN   \n",
       "\n",
       "                                                 body  \n",
       "0   [Megathread] Supreme Court Ruling on Affirmati...  \n",
       "1   Thanks to everyone who engaged in insightful a...  \n",
       "2   I would prefer using a process that takes into...  \n",
       "3   u/Tungsten_, Thanks for creating a section jus...  \n",
       "4   As with anything related to Asians in politics...  \n",
       "5   Yet colleges will allow alumni and doners in e...  \n",
       "6   I just hated Affirmative Action as a distracti...  \n",
       "7   My own feeling is that I was never in love wit...  \n",
       "8   Anti Asian racism whether against East Asians ...  \n",
       "9   Can we overturn legacy and athlete admissions ...  \n",
       "10  I want to remind people that in California, on...  \n",
       "11  I think this is great news for our nation. Whi...  \n",
       "12  My main concern is that, since the Court did n...  \n",
       "13  There is obviously a lot to it and no real eas...  \n",
       "15  I’m sure selective schools like Harvard will f...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deleted_rows = comments_df[comments_df['body'].isin(['[deleted]','[removed]'])]\n",
    "\n",
    "# drop rows\n",
    "comments_df.drop(deleted_rows.index, inplace=True)\n",
    "print(comments_df.shape)\n",
    "comments_df.reset_index(drop=True)\n",
    "comments_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: 307 rows were of deleted comments and so dropped from dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop rows that have been filtered by the AutoModerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3029, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>username</th>\n",
       "      <th>time_created</th>\n",
       "      <th>flair</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14m8mf4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-29 10:54:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Megathread] Supreme Court Ruling on Affirmati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jq5du0z</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-30 11:33:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jq0dgzx</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>2023-06-29 11:16:15</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jq0cg7k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>2023-06-29 11:09:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jq0f52k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>2023-06-29 11:26:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jq0mhrx</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Pancake_muncher</td>\n",
       "      <td>2023-06-29 12:12:39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yet colleges will allow alumni and doners in e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>jq0mlbs</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>suberry</td>\n",
       "      <td>2023-06-29 12:13:16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I just hated Affirmative Action as a distracti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jq0cvvn</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Puzzled-Painter3301</td>\n",
       "      <td>2023-06-29 11:12:31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My own feeling is that I was never in love wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jq0jtzk</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>e9967780</td>\n",
       "      <td>2023-06-29 11:55:52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anti Asian racism whether against East Asians ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jq0kfzq</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-06-29 11:59:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Can we overturn legacy and athlete admissions ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>jq0iyws</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>OkartoIceCream</td>\n",
       "      <td>2023-06-29 11:50:34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I want to remind people that in California, on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>jq20w8p</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>ShalomHasaeyo</td>\n",
       "      <td>2023-06-29 17:36:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I think this is great news for our nation. Whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>jq0b8ts</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>SteadfastEnd</td>\n",
       "      <td>2023-06-29 11:02:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My main concern is that, since the Court did n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>jq2fp75</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>MsNewKicks</td>\n",
       "      <td>2023-06-29 19:26:21</td>\n",
       "      <td>First Of Her Name, Queen ABG, 나쁜 기집애, Blocker ...</td>\n",
       "      <td>There is obviously a lot to it and no real eas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>jq0dxui</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>pal2002</td>\n",
       "      <td>2023-06-29 11:19:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I’m sure selective schools like Harvard will f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id   parent_id             username         time_created  \\\n",
       "0   14m8mf4         NaN            Tungsten_  2023-06-29 10:54:44   \n",
       "1   jq5du0z  t3_14m8mf4            Tungsten_  2023-06-30 11:33:11   \n",
       "2   jq0dgzx  t3_14m8mf4       ProudBlackMatt  2023-06-29 11:16:15   \n",
       "3   jq0cg7k  t3_14m8mf4         TomatoCanned  2023-06-29 11:09:47   \n",
       "4   jq0f52k  t3_14m8mf4         bad-fengshui  2023-06-29 11:26:41   \n",
       "5   jq0mhrx  t3_14m8mf4      Pancake_muncher  2023-06-29 12:12:39   \n",
       "6   jq0mlbs  t3_14m8mf4              suberry  2023-06-29 12:13:16   \n",
       "7   jq0cvvn  t3_14m8mf4  Puzzled-Painter3301  2023-06-29 11:12:31   \n",
       "8   jq0jtzk  t3_14m8mf4             e9967780  2023-06-29 11:55:52   \n",
       "9   jq0kfzq  t3_14m8mf4                  NaN  2023-06-29 11:59:41   \n",
       "10  jq0iyws  t3_14m8mf4       OkartoIceCream  2023-06-29 11:50:34   \n",
       "11  jq20w8p  t3_14m8mf4        ShalomHasaeyo  2023-06-29 17:36:41   \n",
       "12  jq0b8ts  t3_14m8mf4         SteadfastEnd  2023-06-29 11:02:09   \n",
       "13  jq2fp75  t3_14m8mf4           MsNewKicks  2023-06-29 19:26:21   \n",
       "15  jq0dxui  t3_14m8mf4              pal2002  2023-06-29 11:19:14   \n",
       "\n",
       "                                                flair  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                    Chinese-American   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6                                                 NaN   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11                                                NaN   \n",
       "12                                                NaN   \n",
       "13  First Of Her Name, Queen ABG, 나쁜 기집애, Blocker ...   \n",
       "15                                                NaN   \n",
       "\n",
       "                                                 body  \n",
       "0   [Megathread] Supreme Court Ruling on Affirmati...  \n",
       "1   Thanks to everyone who engaged in insightful a...  \n",
       "2   I would prefer using a process that takes into...  \n",
       "3   u/Tungsten_, Thanks for creating a section jus...  \n",
       "4   As with anything related to Asians in politics...  \n",
       "5   Yet colleges will allow alumni and doners in e...  \n",
       "6   I just hated Affirmative Action as a distracti...  \n",
       "7   My own feeling is that I was never in love wit...  \n",
       "8   Anti Asian racism whether against East Asians ...  \n",
       "9   Can we overturn legacy and athlete admissions ...  \n",
       "10  I want to remind people that in California, on...  \n",
       "11  I think this is great news for our nation. Whi...  \n",
       "12  My main concern is that, since the Court did n...  \n",
       "13  There is obviously a lot to it and no real eas...  \n",
       "15  I’m sure selective schools like Harvard will f...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moderated_rows = comments_df[comments_df['username']=='AutoModerator']\n",
    "moderated_rows\n",
    "\n",
    "comments_df.drop(moderated_rows.index, inplace=True)\n",
    "print(comments_df.shape)\n",
    "comments_df.reset_index(drop=True)\n",
    "comments_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_str(str):\n",
    "    tokenized = []\n",
    "    doc = nlp(str)\n",
    "    for token in doc:\n",
    "        if not token.is_punct and len(token.text.strip()) > 0:\n",
    "            tokenized.append(token.text)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-43919e4bf064>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# create new column of tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcomments_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokens'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomments_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'body'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtokenize_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3848\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-43919e4bf064>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# create new column of tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcomments_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokens'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomments_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'body'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtokenize_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-feeb0a808930>\u001b[0m in \u001b[0;36mtokenize_str\u001b[1;34m(str)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtokenize_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mtokenized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_punct\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\spacy\\language.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1047\u001b[0m                 \u001b[0merror_handler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_error_handler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1049\u001b[1;33m                 \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1050\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m                 \u001b[1;31m# This typically happens if a component is not initialized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.transition_parser.Parser.predict\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.transition_parser.Parser.greedy_parse\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\thinc\\model.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[0monly\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         \"\"\"\n\u001b[1;32m--> 334\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfinish_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptimizer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\spacy\\ml\\tb_framework.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0munseen_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"unseen_classes\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mhas_upper\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_upper\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     )\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\spacy\\ml\\parser_model.pyx\u001b[0m in \u001b[0;36mspacy.ml.parser_model.ParserStepModel.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\thinc\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[0;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[1;32m--> 310\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"Model\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\thinc\\layers\\chain.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mcallbacks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minc_layer_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minc_layer_grad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\thinc\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    308\u001b[0m         \"\"\"Call the model's `forward` function, returning the output and a\n\u001b[0;32m    309\u001b[0m         callback to compute the gradients via backpropagation.\"\"\"\n\u001b[1;32m--> 310\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mInT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mOutT\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;34m\"Model\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\thinc\\layers\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFloats2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_param\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"W\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFloats1d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_param\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"b\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgemm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrans2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mY\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create new column of tokens\n",
    "comments_df['tokens'] = comments_df['body'].apply(lambda x: tokenize_str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "      <td>[Thanks, to, everyone, who, engaged, in, insig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "      <td>[I, would, prefer, using, a, process, that, ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "      <td>[u, Tungsten, Thanks, for, creating, a, sectio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "      <td>[As, with, anything, related, to, Asians, in, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pancake_muncher</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yet colleges will allow alumni and doners in e...</td>\n",
       "      <td>[Yet, colleges, will, allow, alumni, and, done...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>suberry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I just hated Affirmative Action as a distracti...</td>\n",
       "      <td>[I, just, hated, Affirmative, Action, as, a, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Puzzled-Painter3301</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My own feeling is that I was never in love wit...</td>\n",
       "      <td>[My, own, feeling, is, that, I, was, never, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>e9967780</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anti Asian racism whether against East Asians ...</td>\n",
       "      <td>[Anti, Asian, racism, whether, against, East, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Can we overturn legacy and athlete admissions ...</td>\n",
       "      <td>[Can, we, overturn, legacy, and, athlete, admi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OkartoIceCream</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I want to remind people that in California, on...</td>\n",
       "      <td>[I, want, to, remind, people, that, in, Califo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ShalomHasaeyo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I think this is great news for our nation. Whi...</td>\n",
       "      <td>[I, think, this, is, great, news, for, our, na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SteadfastEnd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My main concern is that, since the Court did n...</td>\n",
       "      <td>[My, main, concern, is, that, since, the, Cour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MsNewKicks</td>\n",
       "      <td>First Of Her Name, Queen ABG, 나쁜 기집애, Blocker ...</td>\n",
       "      <td>There is obviously a lot to it and no real eas...</td>\n",
       "      <td>[There, is, obviously, a, lot, to, it, and, no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pal2002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I’m sure selective schools like Harvard will f...</td>\n",
       "      <td>[I, ’m, sure, selective, schools, like, Harvar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ok-Value5827</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TBH, I don't know the depth of how affirmative...</td>\n",
       "      <td>[TBH, I, do, n't, know, the, depth, of, how, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               username                                         flair_text  \\\n",
       "0             Tungsten_                                                NaN   \n",
       "1        ProudBlackMatt                                   Chinese-American   \n",
       "2          TomatoCanned                                                NaN   \n",
       "3          bad-fengshui                                                NaN   \n",
       "4       Pancake_muncher                                                NaN   \n",
       "5               suberry                                                NaN   \n",
       "6   Puzzled-Painter3301                                                NaN   \n",
       "7              e9967780                                                NaN   \n",
       "8                   NaN                                                NaN   \n",
       "9        OkartoIceCream                                                NaN   \n",
       "10        ShalomHasaeyo                                                NaN   \n",
       "11         SteadfastEnd                                                NaN   \n",
       "12           MsNewKicks  First Of Her Name, Queen ABG, 나쁜 기집애, Blocker ...   \n",
       "14              pal2002                                                NaN   \n",
       "15         Ok-Value5827                                                NaN   \n",
       "\n",
       "                                                 body  \\\n",
       "0   Thanks to everyone who engaged in insightful a...   \n",
       "1   I would prefer using a process that takes into...   \n",
       "2   u/Tungsten_, Thanks for creating a section jus...   \n",
       "3   As with anything related to Asians in politics...   \n",
       "4   Yet colleges will allow alumni and doners in e...   \n",
       "5   I just hated Affirmative Action as a distracti...   \n",
       "6   My own feeling is that I was never in love wit...   \n",
       "7   Anti Asian racism whether against East Asians ...   \n",
       "8   Can we overturn legacy and athlete admissions ...   \n",
       "9   I want to remind people that in California, on...   \n",
       "10  I think this is great news for our nation. Whi...   \n",
       "11  My main concern is that, since the Court did n...   \n",
       "12  There is obviously a lot to it and no real eas...   \n",
       "14  I’m sure selective schools like Harvard will f...   \n",
       "15  TBH, I don't know the depth of how affirmative...   \n",
       "\n",
       "                                               tokens  \n",
       "0   [Thanks, to, everyone, who, engaged, in, insig...  \n",
       "1   [I, would, prefer, using, a, process, that, ta...  \n",
       "2   [u, Tungsten, Thanks, for, creating, a, sectio...  \n",
       "3   [As, with, anything, related, to, Asians, in, ...  \n",
       "4   [Yet, colleges, will, allow, alumni, and, done...  \n",
       "5   [I, just, hated, Affirmative, Action, as, a, d...  \n",
       "6   [My, own, feeling, is, that, I, was, never, in...  \n",
       "7   [Anti, Asian, racism, whether, against, East, ...  \n",
       "8   [Can, we, overturn, legacy, and, athlete, admi...  \n",
       "9   [I, want, to, remind, people, that, in, Califo...  \n",
       "10  [I, think, this, is, great, news, for, our, na...  \n",
       "11  [My, main, concern, is, that, since, the, Cour...  \n",
       "12  [There, is, obviously, a, lot, to, it, and, no...  \n",
       "14  [I, ’m, sure, selective, schools, like, Harvar...  \n",
       "15  [TBH, I, do, n't, know, the, depth, of, how, a...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(comments_df['tokens'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem with Reddit comments: We don't want to get split on (get rid of) forward slashes nor get rid of punctuations on usernames like \"u/Tungsten_\"\n",
    "\n",
    "So, instead of using the native Spacy package, we use RedditScore which is built on Spacy but modified for Reddit/Twitter comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u/Tungsten_, Thanks for creating a section just to discuss this. When I read the news I immediately went searching for a forum where folks might have civil discourse on this topic.\n",
      "\n",
      "Just had a few comments/questions:\n",
      "\n",
      "1. Has anyone come across seemingly legitimate data sets on asians & college admission with respect to Affirmative Action (AA for short going forward)\n",
      "2. As an Asian (not born in the US but pretty much assimilated here for 35+ years), I am conflicted. Research results like this one show: [https://www.pewresearch.org/race-ethnicity/2023/06/08/asian-americans-hold-mixed-views-around-affirmative-action/](https://www.pewresearch.org/race-ethnicity/2023/06/08/asian-americans-hold-mixed-views-around-affirmative-action/) that something like 53% Asians think AA is a good thing, and yet when you scroll down and look at the question of \"Should colleges consider race/ethnicity in college admissions,\" the percentage of Asians that say yes are at 21%, no at 76%.\n",
      "\n",
      "I am part of the 76%.... and I'm conflicted. I know especially for the underserved, AA makes a significant impact in giving folks better chances at life which in turn translates to diversity in every facet of work, society, life in general, which I view is a good thing.\n",
      "\n",
      "But specifically regarding college admissions.. say for my own kids? (not college aged yet)  I would like to see more data on whether year 2000 and beyond AA in college admissions was harmful to Asians in general. In my own experience (anecdotal, totally not data science driven), I feel like AA in college admissions has hurt friends and family, in a reverse sort of sense.\n",
      "\n",
      "But for the sake of the underserved, I didn't want AA to go away. So I am deeply conflicted.\n",
      "\n",
      "Your thoughts?\n"
     ]
    }
   ],
   "source": [
    "sample_comment = comments_df['body'][2]\n",
    "print(sample_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RedditScore \n",
    "(not using anymore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/crazyfrogspb/RedditScore.git\n",
      "  Cloning https://github.com/crazyfrogspb/RedditScore.git to c:\\users\\danie\\appdata\\local\\temp\\pip-req-build-l3ypk4yu\n",
      "Requirement already satisfied: setuptools in c:\\users\\danie\\anaconda3\\lib\\site-packages (from redditscore==0.7.3) (45.2.0.post20200210)\n",
      "Requirement already satisfied: spacy>=2.0.11 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from redditscore==0.7.3) (3.7.4)\n",
      "Collecting tldextract>=2.1.0\n",
      "  Downloading tldextract-4.0.0-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: requests>=2.18.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from redditscore==0.7.3) (2.22.0)\n",
      "Requirement already satisfied: scikit-learn>=0.19.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from redditscore==0.7.3) (0.22.1)\n",
      "Requirement already satisfied: pandas>=0.22.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from redditscore==0.7.3) (1.0.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from redditscore==0.7.3) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from redditscore==0.7.3) (1.18.1)\n",
      "Requirement already satisfied: beautifulsoup4>=4.6.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from redditscore==0.7.3) (4.8.2)\n",
      "Requirement already satisfied: adjustText>=0.6.3 in c:\\users\\danie\\anaconda3\\lib\\site-packages\\adjusttext-0.8b2-py3.7.egg (from redditscore==0.7.3) (0.8b2)\n",
      "Collecting eventlet>=0.22.1\n",
      "  Downloading eventlet-0.35.2-py3-none-any.whl (359 kB)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (20.1)\n",
      "Requirement already satisfied: typing-extensions<4.5.0,>=3.7.4.1; python_version < \"3.8\" in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (4.4.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (1.0.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (2.4.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (2.11.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (3.0.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (4.42.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (3.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (2.5.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (1.1.2)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (8.2.3)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (0.9.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (2.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (1.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (0.3.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (6.4.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (2.0.10)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from tldextract>=2.1.0->redditscore==0.7.3) (3.0.12)\n",
      "Requirement already satisfied: idna in c:\\users\\danie\\anaconda3\\lib\\site-packages (from tldextract>=2.1.0->redditscore==0.7.3) (2.8)\n",
      "Collecting requests-file>=1.4\n",
      "  Downloading requests_file-2.0.0-py2.py3-none-any.whl (4.2 kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from requests>=2.18.0->redditscore==0.7.3) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from requests>=2.18.0->redditscore==0.7.3) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from requests>=2.18.0->redditscore==0.7.3) (2022.6.15)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from scikit-learn>=0.19.0->redditscore==0.7.3) (0.14.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from pandas>=0.22.0->redditscore==0.7.3) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from pandas>=0.22.0->redditscore==0.7.3) (2019.3)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.6.0->redditscore==0.7.3) (1.9.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\danie\\anaconda3\\lib\\site-packages (from adjustText>=0.6.3->redditscore==0.7.3) (3.1.3)\n",
      "Collecting dnspython>=1.15.0\n",
      "  Downloading dnspython-2.3.0-py3-none-any.whl (283 kB)\n",
      "Collecting greenlet>=1.0\n",
      "  Downloading greenlet-3.0.3-cp37-cp37m-win_amd64.whl (291 kB)\n",
      "Requirement already satisfied: six in c:\\users\\danie\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy>=2.0.11->redditscore==0.7.3) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy>=2.0.11->redditscore==0.7.3) (2.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from jinja2->spacy>=2.0.11->redditscore==0.7.3) (1.1.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.0.11->redditscore==0.7.3) (0.5.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version == \"3.7\" in c:\\users\\danie\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.0.11->redditscore==0.7.3) (1.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.0.11->redditscore==0.7.3) (2.14.6)\n",
      "Requirement already satisfied: colorama>=0.4.6; sys_platform == \"win32\" and python_version >= \"3.7\" in c:\\users\\danie\\anaconda3\\lib\\site-packages (from wasabi<1.2.0,>=0.9.1->spacy>=2.0.11->redditscore==0.7.3) (0.4.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.0.11->redditscore==0.7.3) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.0.11->redditscore==0.7.3) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy>=2.0.11->redditscore==0.7.3) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy>=2.0.11->redditscore==0.7.3) (0.16.0)\n",
      "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in c:\\users\\danie\\anaconda3\\lib\\site-packages (from catalogue<2.1.0,>=2.0.6->spacy>=2.0.11->redditscore==0.7.3) (2.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from matplotlib->adjustText>=0.6.3->redditscore==0.7.3) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from matplotlib->adjustText>=0.6.3->redditscore==0.7.3) (0.10.0)\n",
      "Building wheels for collected packages: redditscore\n",
      "  Building wheel for redditscore (setup.py): started\n",
      "  Building wheel for redditscore (setup.py): finished with status 'done'\n",
      "  Created wheel for redditscore: filename=redditscore-0.7.3-py3-none-any.whl size=7852150 sha256=affc20c003c412f8a666e5207c543b14bc9fd9152ef7420421ec55bb62eb8563\n",
      "  Stored in directory: C:\\Users\\danie\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-vd0158ij\\wheels\\18\\6a\\1b\\4be247e0dd084d11afc3c7abf830b0fe5a7b5c2f165876803e\n",
      "Successfully built redditscore\n",
      "Installing collected packages: requests-file, tldextract, dnspython, greenlet, eventlet, dill, redditscore\n",
      "  Attempting uninstall: greenlet\n",
      "    Found existing installation: greenlet 0.4.15\n",
      "    Uninstalling greenlet-0.4.15:\n",
      "      Successfully uninstalled greenlet-0.4.15\n",
      "Successfully installed dill-0.3.7 dnspython-2.3.0 eventlet-0.35.2 greenlet-3.0.3 redditscore-0.7.3 requests-file-2.0.0 tldextract-4.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/crazyfrogspb/RedditScore.git 'C:\\Users\\danie\\AppData\\Local\\Temp\\pip-req-build-l3ypk4yu'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/crazyfrogspb/RedditScore.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "add() takes exactly 2 positional arguments (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-c65f745d9e40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mredditscore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCrazyTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCrazyTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhashtags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreddit_usernames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtokenize_str_reddit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\redditscore\\tokenizer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, lowercase, keepcaps, normalize, ignore_quotes, ignore_reddit_quotes, ignore_stopwords, stem, remove_punct, remove_breaks, decontract, twitter_handles, urls, hashtags, numbers, subreddits, reddit_usernames, emails, extra_patterns, keep_untokenized, whitespaces_to_underscores, remove_nonunicode, pos_emojis, neg_emojis, neutral_emojis, print_url_warnings, latin_chars_fix, ngrams)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         self._merging_matcher.add(\n\u001b[1;32m--> 359\u001b[1;33m             'HASHTAG', None, [{'ORTH': '#'}, {'IS_ASCII': True}])\n\u001b[0m\u001b[0;32m    360\u001b[0m         self._merging_matcher.add(\n\u001b[0;32m    361\u001b[0m             \u001b[1;34m'SUBREDDIT'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\spacy\\matcher\\matcher.pyx\u001b[0m in \u001b[0;36mspacy.matcher.matcher.Matcher.add\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: add() takes exactly 2 positional arguments (3 given)"
     ]
    }
   ],
   "source": [
    "from redditscore.tokenizer import CrazyTokenizer\n",
    "\n",
    "tokenizer = CrazyTokenizer(hashtags=False, lowercase=True, reddit_usernames=True, normalize=False)\n",
    "\n",
    "def tokenize_str_reddit(str):\n",
    "    tokenized = []\n",
    "    doc = tokenizer.tokenizer(str)\n",
    "    for token in doc:\n",
    "        if not token.is_punct and len(token.text.strip()) > 0:\n",
    "            tokenized.append(token.text)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "add() takes exactly 2 positional arguments (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-dcb2731b9d1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcomments_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokens'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomments_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'body'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtokenize_str_reddit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3848\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-dcb2731b9d1f>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcomments_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokens'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomments_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'body'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtokenize_str_reddit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-374fc19dc617>\u001b[0m in \u001b[0;36mtokenize_str_reddit\u001b[1;34m(str)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtokenize_str_reddit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtokenized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCrazyTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_punct\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\redditscore\\tokenizer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, lowercase, keepcaps, normalize, ignore_quotes, ignore_reddit_quotes, ignore_stopwords, stem, remove_punct, remove_breaks, decontract, twitter_handles, urls, hashtags, numbers, subreddits, reddit_usernames, emails, extra_patterns, keep_untokenized, whitespaces_to_underscores, remove_nonunicode, pos_emojis, neg_emojis, neutral_emojis, print_url_warnings, latin_chars_fix, ngrams)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         self._merging_matcher.add(\n\u001b[1;32m--> 359\u001b[1;33m             'HASHTAG', None, [{'ORTH': '#'}, {'IS_ASCII': True}])\n\u001b[0m\u001b[0;32m    360\u001b[0m         self._merging_matcher.add(\n\u001b[0;32m    361\u001b[0m             \u001b[1;34m'SUBREDDIT'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\spacy\\matcher\\matcher.pyx\u001b[0m in \u001b[0;36mspacy.matcher.matcher.Matcher.add\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: add() takes exactly 2 positional arguments (3 given)"
     ]
    }
   ],
   "source": [
    "comments_df['tokens'] = comments_df['body'].apply(lambda x: tokenize_str_reddit(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special case in Spacy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'CrazyTokenizer' doesn't work bc the code is reliant on an old version of Spacy. So, we will create a special case in Spacy's tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'Hi', ',', 'u', '/', 'Tungsten', '_', ',', 'Thanks', 'for', 'creating', 'a', 'section']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "65",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0950b2f4b101>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mspecial_case\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'TEXT'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'REGEX'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34mr'u\\/([[:word:]]|-){3,23}'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_special_case\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'redditor'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspecial_case\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' Hi, u/Tungsten_, Thanks for creating a section'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\spacy\\tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.add_special_case\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\spacy\\tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._validate_special_case\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 65"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "doc = nlp(' Hi, u/Tungsten_, Thanks for creating a section')\n",
    "print([w.text for w in doc])\n",
    "\n",
    "special_case = [{'TEXT': {'REGEX': r'u\\/([[:word:]]|-){3,23}'}}]\n",
    "nlp.tokenizer.add_special_case('redditor', special_case)\n",
    "doc = nlp(' Hi, u/Tungsten_, Thanks for creating a section')\n",
    "\n",
    "print(print([w.text for w in doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "nlp.tokenizer = Tokenizer(nlp.vocab, token_match=re.compile(r'u/{1}\\w{3,23}').search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_comment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-0e4e9aa983af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_comment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sample_comment' is not defined"
     ]
    }
   ],
   "source": [
    "doc = nlp(sample_comment)\n",
    "\n",
    "print([w.text for w in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u', 'Tungsten', 'Thanks', 'for', 'creating', 'a', 'section', 'just', 'to', 'discuss', 'this', 'When', 'I', 'read', 'the', 'news', 'I', 'immediately', 'went', 'searching', 'for', 'a', 'forum', 'where', 'folks', 'might', 'have', 'civil', 'discourse', 'on', 'this', 'topic', 'Just', 'had', 'a', 'few', 'comments', 'questions', '1', 'Has', 'anyone', 'come', 'across', 'seemingly', 'legitimate', 'data', 'sets', 'on', 'asians', 'college', 'admission', 'with', 'respect', 'to', 'Affirmative', 'Action', 'AA', 'for', 'short', 'going', 'forward', '2', 'As', 'an', 'Asian', 'not', 'born', 'in', 'the', 'US', 'but', 'pretty', 'much', 'assimilated', 'here', 'for', '35', '+', 'years', 'I', 'am', 'conflicted', 'Research', 'results', 'like', 'this', 'one', 'show', 'https://www.pewresearch.org/race-ethnicity/2023/06/08/asian-americans-hold-mixed-views-around-affirmative-action/](https://www.pewresearch.org/race-ethnicity/2023/06/08/asian-americans-hold-mixed-views-around-affirmative-action/', 'that', 'something', 'like', '53', 'Asians', 'think', 'AA', 'is', 'a', 'good', 'thing', 'and', 'yet', 'when', 'you', 'scroll', 'down', 'and', 'look', 'at', 'the', 'question', 'of', 'Should', 'colleges', 'consider', 'race', 'ethnicity', 'in', 'college', 'admissions', 'the', 'percentage', 'of', 'Asians', 'that', 'say', 'yes', 'are', 'at', '21', 'no', 'at', '76', 'I', 'am', 'part', 'of', 'the', '76', 'and', 'I', \"'m\", 'conflicted', 'I', 'know', 'especially', 'for', 'the', 'underserved', 'AA', 'makes', 'a', 'significant', 'impact', 'in', 'giving', 'folks', 'better', 'chances', 'at', 'life', 'which', 'in', 'turn', 'translates', 'to', 'diversity', 'in', 'every', 'facet', 'of', 'work', 'society', 'life', 'in', 'general', 'which', 'I', 'view', 'is', 'a', 'good', 'thing', 'But', 'specifically', 'regarding', 'college', 'admissions', 'say', 'for', 'my', 'own', 'kids', 'not', 'college', 'aged', 'yet', 'I', 'would', 'like', 'to', 'see', 'more', 'data', 'on', 'whether', 'year', '2000', 'and', 'beyond', 'AA', 'in', 'college', 'admissions', 'was', 'harmful', 'to', 'Asians', 'in', 'general', 'In', 'my', 'own', 'experience', 'anecdotal', 'totally', 'not', 'data', 'science', 'driven', 'I', 'feel', 'like', 'AA', 'in', 'college', 'admissions', 'has', 'hurt', 'friends', 'and', 'family', 'in', 'a', 'reverse', 'sort', 'of', 'sense', 'But', 'for', 'the', 'sake', 'of', 'the', 'underserved', 'I', 'did', \"n't\", 'want', 'AA', 'to', 'go', 'away', 'So', 'I', 'am', 'deeply', 'conflicted', 'Your', 'thoughts']\n"
     ]
    }
   ],
   "source": [
    "print(comments_df['tokens'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from above that the tokenization got worse when we added the token_match criteria. We need to modify the English class-attribute before loading model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_comment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-9b7f7fe41d96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#nlp.tokenizer.token_match = None (run line if results are same)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdoc1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_comment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sample_comment' is not defined"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#nlp.tokenizer.token_match = None (run line if results are same)\n",
    "doc1 = nlp(sample_comment)\n",
    "print([w.text for w in doc1])\n",
    "\n",
    "# add token_match to tokenizer\n",
    "nlp.tokenizer.token_match = re.compile(r'u/{1}\\w{3,23}').match\n",
    "doc2 = nlp(sample_comment)\n",
    "print([w.text for w in doc2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason, the reddit username includes the comma.\n",
    "*IDEA: We can strip the string of any characters that are not valid in a username.*\n",
    "\n",
    "Our tokenization of reddit usernames is not perfect, it may include trailing punctuation. But let's move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 11), match='u/Tungsten_'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 11), match='u/Tungsten_'>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#redditor_regex = re.compile(r'u/{1}\\w{3,23}')\n",
    "#print(redditor_regex.search(sample_comment))\n",
    "#redditor_regex.match(sample_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "      <td>[Thanks, to, everyone, who, engaged, in, insig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "      <td>[I, would, prefer, using, a, process, that, ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "      <td>[u, Tungsten, Thanks, for, creating, a, sectio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "      <td>[As, with, anything, related, to, Asians, in, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pancake_muncher</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yet colleges will allow alumni and doners in e...</td>\n",
       "      <td>[Yet, colleges, will, allow, alumni, and, done...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          username        flair_text  \\\n",
       "0        Tungsten_               NaN   \n",
       "1   ProudBlackMatt  Chinese-American   \n",
       "2     TomatoCanned               NaN   \n",
       "3     bad-fengshui               NaN   \n",
       "4  Pancake_muncher               NaN   \n",
       "\n",
       "                                                body  \\\n",
       "0  Thanks to everyone who engaged in insightful a...   \n",
       "1  I would prefer using a process that takes into...   \n",
       "2  u/Tungsten_, Thanks for creating a section jus...   \n",
       "3  As with anything related to Asians in politics...   \n",
       "4  Yet colleges will allow alumni and doners in e...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [Thanks, to, everyone, who, engaged, in, insig...  \n",
       "1  [I, would, prefer, using, a, process, that, ta...  \n",
       "2  [u, Tungsten, Thanks, for, creating, a, sectio...  \n",
       "3  [As, with, anything, related, to, Asians, in, ...  \n",
       "4  [Yet, colleges, will, allow, alumni, and, done...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['tokens'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-4ffeb575b3ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# drop old tokens column from data frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mcomments_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokens'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3995\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3996\u001b[0m             \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3997\u001b[1;33m             \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3998\u001b[0m         )\n\u001b[0;32m   3999\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   3934\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3935\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3936\u001b[1;33m                 \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3938\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[1;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[0;32m   3968\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3969\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3970\u001b[1;33m                 \u001b[0mnew_axis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3971\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   5016\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5017\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5018\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5019\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5020\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['tokens'] not found in axis\""
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.tokenizer.token_match = re.compile(r'u/{1}\\w{3,23}').match # match reddit usernames\n",
    "\n",
    "# drop old tokens column from data frame\n",
    "comments_df.drop(['tokens'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column to data frame\n",
    "comments_df['tokens_new'] = comments_df['body'].apply(lambda x: tokenize_str(x)) # takes a minute and half to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>username</th>\n",
       "      <th>time_created</th>\n",
       "      <th>flair</th>\n",
       "      <th>body</th>\n",
       "      <th>tokens_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14m8mf4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-29 10:54:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Megathread] Supreme Court Ruling on Affirmati...</td>\n",
       "      <td>[Megathread, Supreme, Court, Ruling, on, Affir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jq5du0z</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-30 11:33:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "      <td>[Thanks, to, everyone, who, engaged, in, insig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jq0dgzx</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>2023-06-29 11:16:15</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "      <td>[I, would, prefer, using, a, process, that, ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jq0cg7k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>2023-06-29 11:09:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "      <td>[u/Tungsten_,, Thanks, for, creating, a, secti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jq0f52k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>2023-06-29 11:26:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "      <td>[As, with, anything, related, to, Asians, in, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jq0mhrx</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Pancake_muncher</td>\n",
       "      <td>2023-06-29 12:12:39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yet colleges will allow alumni and doners in e...</td>\n",
       "      <td>[Yet, colleges, will, allow, alumni, and, done...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>jq0mlbs</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>suberry</td>\n",
       "      <td>2023-06-29 12:13:16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I just hated Affirmative Action as a distracti...</td>\n",
       "      <td>[I, just, hated, Affirmative, Action, as, a, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jq0cvvn</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Puzzled-Painter3301</td>\n",
       "      <td>2023-06-29 11:12:31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My own feeling is that I was never in love wit...</td>\n",
       "      <td>[My, own, feeling, is, that, I, was, never, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jq0jtzk</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>e9967780</td>\n",
       "      <td>2023-06-29 11:55:52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anti Asian racism whether against East Asians ...</td>\n",
       "      <td>[Anti, Asian, racism, whether, against, East, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jq0kfzq</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-06-29 11:59:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Can we overturn legacy and athlete admissions ...</td>\n",
       "      <td>[Can, we, overturn, legacy, and, athlete, admi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>jq0iyws</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>OkartoIceCream</td>\n",
       "      <td>2023-06-29 11:50:34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I want to remind people that in California, on...</td>\n",
       "      <td>[I, want, to, remind, people, that, in, Califo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>jq20w8p</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>ShalomHasaeyo</td>\n",
       "      <td>2023-06-29 17:36:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I think this is great news for our nation. Whi...</td>\n",
       "      <td>[I, think, this, is, great, news, for, our, na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>jq0b8ts</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>SteadfastEnd</td>\n",
       "      <td>2023-06-29 11:02:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My main concern is that, since the Court did n...</td>\n",
       "      <td>[My, main, concern, is, that, since, the, Cour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>jq2fp75</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>MsNewKicks</td>\n",
       "      <td>2023-06-29 19:26:21</td>\n",
       "      <td>First Of Her Name, Queen ABG, 나쁜 기집애, Blocker ...</td>\n",
       "      <td>There is obviously a lot to it and no real eas...</td>\n",
       "      <td>[There, is, obviously, a, lot, to, it, and, no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>jq0dxui</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>pal2002</td>\n",
       "      <td>2023-06-29 11:19:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I’m sure selective schools like Harvard will f...</td>\n",
       "      <td>[I, ’m, sure, selective, schools, like, Harvar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id   parent_id             username         time_created  \\\n",
       "0   14m8mf4         NaN            Tungsten_  2023-06-29 10:54:44   \n",
       "1   jq5du0z  t3_14m8mf4            Tungsten_  2023-06-30 11:33:11   \n",
       "2   jq0dgzx  t3_14m8mf4       ProudBlackMatt  2023-06-29 11:16:15   \n",
       "3   jq0cg7k  t3_14m8mf4         TomatoCanned  2023-06-29 11:09:47   \n",
       "4   jq0f52k  t3_14m8mf4         bad-fengshui  2023-06-29 11:26:41   \n",
       "5   jq0mhrx  t3_14m8mf4      Pancake_muncher  2023-06-29 12:12:39   \n",
       "6   jq0mlbs  t3_14m8mf4              suberry  2023-06-29 12:13:16   \n",
       "7   jq0cvvn  t3_14m8mf4  Puzzled-Painter3301  2023-06-29 11:12:31   \n",
       "8   jq0jtzk  t3_14m8mf4             e9967780  2023-06-29 11:55:52   \n",
       "9   jq0kfzq  t3_14m8mf4                  NaN  2023-06-29 11:59:41   \n",
       "10  jq0iyws  t3_14m8mf4       OkartoIceCream  2023-06-29 11:50:34   \n",
       "11  jq20w8p  t3_14m8mf4        ShalomHasaeyo  2023-06-29 17:36:41   \n",
       "12  jq0b8ts  t3_14m8mf4         SteadfastEnd  2023-06-29 11:02:09   \n",
       "13  jq2fp75  t3_14m8mf4           MsNewKicks  2023-06-29 19:26:21   \n",
       "15  jq0dxui  t3_14m8mf4              pal2002  2023-06-29 11:19:14   \n",
       "\n",
       "                                                flair  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                    Chinese-American   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6                                                 NaN   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11                                                NaN   \n",
       "12                                                NaN   \n",
       "13  First Of Her Name, Queen ABG, 나쁜 기집애, Blocker ...   \n",
       "15                                                NaN   \n",
       "\n",
       "                                                 body  \\\n",
       "0   [Megathread] Supreme Court Ruling on Affirmati...   \n",
       "1   Thanks to everyone who engaged in insightful a...   \n",
       "2   I would prefer using a process that takes into...   \n",
       "3   u/Tungsten_, Thanks for creating a section jus...   \n",
       "4   As with anything related to Asians in politics...   \n",
       "5   Yet colleges will allow alumni and doners in e...   \n",
       "6   I just hated Affirmative Action as a distracti...   \n",
       "7   My own feeling is that I was never in love wit...   \n",
       "8   Anti Asian racism whether against East Asians ...   \n",
       "9   Can we overturn legacy and athlete admissions ...   \n",
       "10  I want to remind people that in California, on...   \n",
       "11  I think this is great news for our nation. Whi...   \n",
       "12  My main concern is that, since the Court did n...   \n",
       "13  There is obviously a lot to it and no real eas...   \n",
       "15  I’m sure selective schools like Harvard will f...   \n",
       "\n",
       "                                           tokens_new  \n",
       "0   [Megathread, Supreme, Court, Ruling, on, Affir...  \n",
       "1   [Thanks, to, everyone, who, engaged, in, insig...  \n",
       "2   [I, would, prefer, using, a, process, that, ta...  \n",
       "3   [u/Tungsten_,, Thanks, for, creating, a, secti...  \n",
       "4   [As, with, anything, related, to, Asians, in, ...  \n",
       "5   [Yet, colleges, will, allow, alumni, and, done...  \n",
       "6   [I, just, hated, Affirmative, Action, as, a, d...  \n",
       "7   [My, own, feeling, is, that, I, was, never, in...  \n",
       "8   [Anti, Asian, racism, whether, against, East, ...  \n",
       "9   [Can, we, overturn, legacy, and, athlete, admi...  \n",
       "10  [I, want, to, remind, people, that, in, Califo...  \n",
       "11  [I, think, this, is, great, news, for, our, na...  \n",
       "12  [My, main, concern, is, that, since, the, Cour...  \n",
       "13  [There, is, obviously, a, lot, to, it, and, no...  \n",
       "15  [I, ’m, sure, selective, schools, like, Harvar...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and normalize text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make all words lowercase (trivial)\n",
    "2. Drop non-word tokens (may already be done in tokenize_str() function)\n",
    "3. Remove stop-words (in a sophisticated manner)\n",
    "4. Stem words to remove suffixes, prefixes, infixes OR Lemmatize tokens (intelligently)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Remove stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 10504),\n",
       " ('to', 7373),\n",
       " ('of', 5903),\n",
       " ('and', 5694),\n",
       " ('a', 5293),\n",
       " ('that', 4814),\n",
       " ('in', 4436),\n",
       " ('i', 4103),\n",
       " ('is', 3984),\n",
       " ('it', 3495),\n",
       " ('are', 2626),\n",
       " ('you', 2537),\n",
       " ('for', 2504),\n",
       " ('asian', 2033),\n",
       " ('not', 2024),\n",
       " (\"'s\", 2019),\n",
       " ('this', 1859),\n",
       " ('as', 1843),\n",
       " (\"n't\", 1806),\n",
       " ('but', 1744),\n",
       " ('they', 1688),\n",
       " ('be', 1683),\n",
       " ('do', 1644),\n",
       " ('on', 1624),\n",
       " ('have', 1560),\n",
       " ('action', 1439),\n",
       " ('with', 1432),\n",
       " ('affirmative', 1415),\n",
       " ('we', 1334),\n",
       " ('asians', 1321),\n",
       " ('or', 1244),\n",
       " ('people', 1224),\n",
       " ('at', 1217),\n",
       " ('if', 1192),\n",
       " ('about', 1101),\n",
       " ('from', 1096),\n",
       " ('white', 1039),\n",
       " ('who', 1017),\n",
       " ('their', 981),\n",
       " ('more', 980),\n",
       " ('was', 959),\n",
       " ('there', 935),\n",
       " ('so', 900),\n",
       " ('an', 873),\n",
       " ('like', 846),\n",
       " ('just', 846),\n",
       " ('by', 844),\n",
       " ('what', 838),\n",
       " ('other', 837),\n",
       " ('because', 830)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stop words using word counts\n",
    "counts_dict = {}\n",
    "for word in comments_df['tokens_new'].sum():\n",
    "    word = word.lower()\n",
    "    if word in counts_dict:\n",
    "        counts_dict[word]+=1\n",
    "    else:\n",
    "        counts_dict[word] = 1\n",
    "\n",
    "word_counts = sorted(counts_dict.items(), key = lambda x: x[1], reverse=True)\n",
    "word_counts[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 11882),\n",
       " ('to', 8178),\n",
       " ('of', 6683),\n",
       " ('and', 6456),\n",
       " ('a', 5916),\n",
       " ('that', 5361),\n",
       " ('in', 4994),\n",
       " ('is', 4546),\n",
       " ('i', 4546),\n",
       " ('it', 4017),\n",
       " ('are', 2945),\n",
       " ('you', 2838),\n",
       " ('for', 2794),\n",
       " ('not', 2295),\n",
       " (\"'s\", 2276),\n",
       " ('asian', 2240),\n",
       " ('as', 2076),\n",
       " (\"n't\", 2060),\n",
       " ('this', 2042),\n",
       " ('but', 1954),\n",
       " ('they', 1902),\n",
       " ('be', 1866),\n",
       " ('do', 1863),\n",
       " ('on', 1860),\n",
       " ('have', 1776),\n",
       " ('action', 1647),\n",
       " ('affirmative', 1630),\n",
       " ('with', 1589),\n",
       " ('we', 1467),\n",
       " ('asians', 1454),\n",
       " ('people', 1420),\n",
       " ('or', 1383),\n",
       " ('at', 1350),\n",
       " ('if', 1316),\n",
       " ('from', 1274)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts[:35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mark/remove words as stop words that are more frequent than the first noun ('i')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'to', 'of', 'and', 'a', 'that', 'in']\n"
     ]
    }
   ],
   "source": [
    "stop_words_freq = []\n",
    "for word, count in word_counts:\n",
    "    if word == 'i':\n",
    "        break\n",
    "    else:\n",
    "        stop_words_freq.append(word)\n",
    "\n",
    "print(stop_words_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging from the list, we can add 'is' and 'it' to list of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'to', 'of', 'and', 'a', 'that', 'in', 'is', 'it', \"'m\", '>', '<']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_freq += ['is', 'it', \"'m\", '>', '<']\n",
    "stop_words_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to normalize tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tokens(word_list, extra_stop_words=[]):\n",
    "    normalized = []\n",
    "    if type(word_list) == list and len(word_list) == 1:\n",
    "        word_list = word_list[0]\n",
    "\n",
    "    if type(word_list) == list:\n",
    "        word_list = ' '.join([str(elem) for elem in word_list])\n",
    "\n",
    "    doc = nlp(word_list.lower()) # lowercase words in word_list\n",
    "\n",
    "    # add lexeme property of stopword to words considered as stopwords\n",
    "    if len(extra_stop_words) > 0:\n",
    "        for stopword in extra_stop_words:\n",
    "            lexeme = nlp.vocab[stopword]\n",
    "            lexeme.is_stop = True\n",
    "\n",
    "    for w in doc:\n",
    "        # if not stop word or punctuation, add it to normalized\n",
    "        if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num and len(w.text.strip()) > 0:\n",
    "            # add lemmatized version of word\n",
    "            normalized.append(str(w.lemma_))\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>username</th>\n",
       "      <th>time_created</th>\n",
       "      <th>flair</th>\n",
       "      <th>body</th>\n",
       "      <th>tokens_new</th>\n",
       "      <th>normalized_tokens</th>\n",
       "      <th>normalized_tokens_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14m8mf4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-29 10:54:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Megathread] Supreme Court Ruling on Affirmati...</td>\n",
       "      <td>[Megathread, Supreme, Court, Ruling, on, Affir...</td>\n",
       "      <td>[megathread, supreme, court, ruling, affirmati...</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jq5du0z</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-30 11:33:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "      <td>[Thanks, to, everyone, who, engaged, in, insig...</td>\n",
       "      <td>[thank, engage, insightful, respectful, discou...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jq0dgzx</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>2023-06-29 11:16:15</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "      <td>[I, would, prefer, using, a, process, that, ta...</td>\n",
       "      <td>[prefer, process, take, account, poverty, inst...</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jq0cg7k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>2023-06-29 11:09:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "      <td>[u/Tungsten_,, Thanks, for, creating, a, secti...</td>\n",
       "      <td>[u/tungsten_,, thank, create, section, discuss...</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jq0f52k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>2023-06-29 11:26:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "      <td>[As, with, anything, related, to, Asians, in, ...</td>\n",
       "      <td>[relate, asians, politic, m, see, lot, non, as...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jq0mhrx</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Pancake_muncher</td>\n",
       "      <td>2023-06-29 12:12:39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yet colleges will allow alumni and doners in e...</td>\n",
       "      <td>[Yet, colleges, will, allow, alumni, and, done...</td>\n",
       "      <td>[college, allow, alumnus, doner, easily, consi...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>jq0mlbs</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>suberry</td>\n",
       "      <td>2023-06-29 12:13:16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I just hated Affirmative Action as a distracti...</td>\n",
       "      <td>[I, just, hated, Affirmative, Action, as, a, d...</td>\n",
       "      <td>[hate, affirmative, action, distraction, banda...</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jq0cvvn</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Puzzled-Painter3301</td>\n",
       "      <td>2023-06-29 11:12:31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My own feeling is that I was never in love wit...</td>\n",
       "      <td>[My, own, feeling, is, that, I, was, never, in...</td>\n",
       "      <td>[feeling, love, affirmative, action, possible,...</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>jq0jtzk</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>e9967780</td>\n",
       "      <td>2023-06-29 11:55:52</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anti Asian racism whether against East Asians ...</td>\n",
       "      <td>[Anti, Asian, racism, whether, against, East, ...</td>\n",
       "      <td>[anti, asian, racism, east, asians, south, asi...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>jq0kfzq</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-06-29 11:59:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Can we overturn legacy and athlete admissions ...</td>\n",
       "      <td>[Can, we, overturn, legacy, and, athlete, admi...</td>\n",
       "      <td>[overturn, legacy, athlete, admission, point, ...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>jq0iyws</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>OkartoIceCream</td>\n",
       "      <td>2023-06-29 11:50:34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I want to remind people that in California, on...</td>\n",
       "      <td>[I, want, to, remind, people, that, in, Califo...</td>\n",
       "      <td>[want, remind, people, california, progressive...</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>jq20w8p</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>ShalomHasaeyo</td>\n",
       "      <td>2023-06-29 17:36:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I think this is great news for our nation. Whi...</td>\n",
       "      <td>[I, think, this, is, great, news, for, our, na...</td>\n",
       "      <td>[think, great, news, nation, abolish, legacy, ...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>jq0b8ts</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>SteadfastEnd</td>\n",
       "      <td>2023-06-29 11:02:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My main concern is that, since the Court did n...</td>\n",
       "      <td>[My, main, concern, is, that, since, the, Cour...</td>\n",
       "      <td>[main, concern, court, strike, grutter, outrig...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>jq2fp75</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>MsNewKicks</td>\n",
       "      <td>2023-06-29 19:26:21</td>\n",
       "      <td>First Of Her Name, Queen ABG, 나쁜 기집애, Blocker ...</td>\n",
       "      <td>There is obviously a lot to it and no real eas...</td>\n",
       "      <td>[There, is, obviously, a, lot, to, it, and, no...</td>\n",
       "      <td>[obviously, lot, real, easy, answer, satisfy, ...</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>jq0dxui</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>pal2002</td>\n",
       "      <td>2023-06-29 11:19:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I’m sure selective schools like Harvard will f...</td>\n",
       "      <td>[I, ’m, sure, selective, schools, like, Harvar...</td>\n",
       "      <td>[m, sure, selective, school, like, harvard, fi...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>jq0w387</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Ok-Value5827</td>\n",
       "      <td>2023-06-29 13:12:38</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TBH, I don't know the depth of how affirmative...</td>\n",
       "      <td>[TBH, I, do, n't, know, the, depth, of, how, a...</td>\n",
       "      <td>[tbh, know, depth, affirmative, action, colleg...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>jq22me5</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>j3ychen</td>\n",
       "      <td>2023-06-29 17:49:07</td>\n",
       "      <td>Taiwanese</td>\n",
       "      <td>People can continue to debate the legal implic...</td>\n",
       "      <td>[People, can, continue, to, debate, the, legal...</td>\n",
       "      <td>[people, continue, debate, legal, implication,...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>jq28kbb</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>HappyPineapple11</td>\n",
       "      <td>2023-06-29 18:32:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The only issue with affirmative action was tha...</td>\n",
       "      <td>[The, only, issue, with, affirmative, action, ...</td>\n",
       "      <td>[issue, affirmative, action, explicit, quota, ...</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>jq0heyu</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>alandizzle</td>\n",
       "      <td>2023-06-29 11:40:55</td>\n",
       "      <td>I'm Asian. Hi.</td>\n",
       "      <td>You know what I hate?\\n\\nThe main subreddits w...</td>\n",
       "      <td>[You, know, what, I, hate, The, main, subreddi...</td>\n",
       "      <td>[know, hate, main, subreddit, certain, white, ...</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>jq28fcf</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>gamesrgreat</td>\n",
       "      <td>2023-06-29 18:31:37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I’m not a fan of affirmative action even tho i...</td>\n",
       "      <td>[I, ’m, not, a, fan, of, affirmative, action, ...</td>\n",
       "      <td>[m, fan, affirmative, action, tho, help, minor...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id   parent_id             username         time_created  \\\n",
       "0   14m8mf4         NaN            Tungsten_  2023-06-29 10:54:44   \n",
       "1   jq5du0z  t3_14m8mf4            Tungsten_  2023-06-30 11:33:11   \n",
       "2   jq0dgzx  t3_14m8mf4       ProudBlackMatt  2023-06-29 11:16:15   \n",
       "3   jq0cg7k  t3_14m8mf4         TomatoCanned  2023-06-29 11:09:47   \n",
       "4   jq0f52k  t3_14m8mf4         bad-fengshui  2023-06-29 11:26:41   \n",
       "5   jq0mhrx  t3_14m8mf4      Pancake_muncher  2023-06-29 12:12:39   \n",
       "6   jq0mlbs  t3_14m8mf4              suberry  2023-06-29 12:13:16   \n",
       "7   jq0cvvn  t3_14m8mf4  Puzzled-Painter3301  2023-06-29 11:12:31   \n",
       "8   jq0jtzk  t3_14m8mf4             e9967780  2023-06-29 11:55:52   \n",
       "9   jq0kfzq  t3_14m8mf4                  NaN  2023-06-29 11:59:41   \n",
       "10  jq0iyws  t3_14m8mf4       OkartoIceCream  2023-06-29 11:50:34   \n",
       "11  jq20w8p  t3_14m8mf4        ShalomHasaeyo  2023-06-29 17:36:41   \n",
       "12  jq0b8ts  t3_14m8mf4         SteadfastEnd  2023-06-29 11:02:09   \n",
       "13  jq2fp75  t3_14m8mf4           MsNewKicks  2023-06-29 19:26:21   \n",
       "15  jq0dxui  t3_14m8mf4              pal2002  2023-06-29 11:19:14   \n",
       "16  jq0w387  t3_14m8mf4         Ok-Value5827  2023-06-29 13:12:38   \n",
       "17  jq22me5  t3_14m8mf4              j3ychen  2023-06-29 17:49:07   \n",
       "18  jq28kbb  t3_14m8mf4     HappyPineapple11  2023-06-29 18:32:41   \n",
       "19  jq0heyu  t3_14m8mf4           alandizzle  2023-06-29 11:40:55   \n",
       "20  jq28fcf  t3_14m8mf4          gamesrgreat  2023-06-29 18:31:37   \n",
       "\n",
       "                                                flair  \\\n",
       "0                                                 NaN   \n",
       "1                                                 NaN   \n",
       "2                                    Chinese-American   \n",
       "3                                                 NaN   \n",
       "4                                                 NaN   \n",
       "5                                                 NaN   \n",
       "6                                                 NaN   \n",
       "7                                                 NaN   \n",
       "8                                                 NaN   \n",
       "9                                                 NaN   \n",
       "10                                                NaN   \n",
       "11                                                NaN   \n",
       "12                                                NaN   \n",
       "13  First Of Her Name, Queen ABG, 나쁜 기집애, Blocker ...   \n",
       "15                                                NaN   \n",
       "16                                                NaN   \n",
       "17                                          Taiwanese   \n",
       "18                                                NaN   \n",
       "19                                     I'm Asian. Hi.   \n",
       "20                                                NaN   \n",
       "\n",
       "                                                 body  \\\n",
       "0   [Megathread] Supreme Court Ruling on Affirmati...   \n",
       "1   Thanks to everyone who engaged in insightful a...   \n",
       "2   I would prefer using a process that takes into...   \n",
       "3   u/Tungsten_, Thanks for creating a section jus...   \n",
       "4   As with anything related to Asians in politics...   \n",
       "5   Yet colleges will allow alumni and doners in e...   \n",
       "6   I just hated Affirmative Action as a distracti...   \n",
       "7   My own feeling is that I was never in love wit...   \n",
       "8   Anti Asian racism whether against East Asians ...   \n",
       "9   Can we overturn legacy and athlete admissions ...   \n",
       "10  I want to remind people that in California, on...   \n",
       "11  I think this is great news for our nation. Whi...   \n",
       "12  My main concern is that, since the Court did n...   \n",
       "13  There is obviously a lot to it and no real eas...   \n",
       "15  I’m sure selective schools like Harvard will f...   \n",
       "16  TBH, I don't know the depth of how affirmative...   \n",
       "17  People can continue to debate the legal implic...   \n",
       "18  The only issue with affirmative action was tha...   \n",
       "19  You know what I hate?\\n\\nThe main subreddits w...   \n",
       "20  I’m not a fan of affirmative action even tho i...   \n",
       "\n",
       "                                           tokens_new  \\\n",
       "0   [Megathread, Supreme, Court, Ruling, on, Affir...   \n",
       "1   [Thanks, to, everyone, who, engaged, in, insig...   \n",
       "2   [I, would, prefer, using, a, process, that, ta...   \n",
       "3   [u/Tungsten_,, Thanks, for, creating, a, secti...   \n",
       "4   [As, with, anything, related, to, Asians, in, ...   \n",
       "5   [Yet, colleges, will, allow, alumni, and, done...   \n",
       "6   [I, just, hated, Affirmative, Action, as, a, d...   \n",
       "7   [My, own, feeling, is, that, I, was, never, in...   \n",
       "8   [Anti, Asian, racism, whether, against, East, ...   \n",
       "9   [Can, we, overturn, legacy, and, athlete, admi...   \n",
       "10  [I, want, to, remind, people, that, in, Califo...   \n",
       "11  [I, think, this, is, great, news, for, our, na...   \n",
       "12  [My, main, concern, is, that, since, the, Cour...   \n",
       "13  [There, is, obviously, a, lot, to, it, and, no...   \n",
       "15  [I, ’m, sure, selective, schools, like, Harvar...   \n",
       "16  [TBH, I, do, n't, know, the, depth, of, how, a...   \n",
       "17  [People, can, continue, to, debate, the, legal...   \n",
       "18  [The, only, issue, with, affirmative, action, ...   \n",
       "19  [You, know, what, I, hate, The, main, subreddi...   \n",
       "20  [I, ’m, not, a, fan, of, affirmative, action, ...   \n",
       "\n",
       "                                    normalized_tokens  normalized_tokens_count  \n",
       "0   [megathread, supreme, court, ruling, affirmati...                       62  \n",
       "1   [thank, engage, insightful, respectful, discou...                        9  \n",
       "2   [prefer, process, take, account, poverty, inst...                       52  \n",
       "3   [u/tungsten_,, thank, create, section, discuss...                      126  \n",
       "4   [relate, asians, politic, m, see, lot, non, as...                       25  \n",
       "5   [college, allow, alumnus, doner, easily, consi...                       19  \n",
       "6   [hate, affirmative, action, distraction, banda...                       78  \n",
       "7   [feeling, love, affirmative, action, possible,...                      102  \n",
       "8   [anti, asian, racism, east, asians, south, asi...                       21  \n",
       "9   [overturn, legacy, athlete, admission, point, ...                       15  \n",
       "10  [want, remind, people, california, progressive...                      104  \n",
       "11  [think, great, news, nation, abolish, legacy, ...                       58  \n",
       "12  [main, concern, court, strike, grutter, outrig...                       72  \n",
       "13  [obviously, lot, real, easy, answer, satisfy, ...                       46  \n",
       "15  [m, sure, selective, school, like, harvard, fi...                       20  \n",
       "16  [tbh, know, depth, affirmative, action, colleg...                       39  \n",
       "17  [people, continue, debate, legal, implication,...                       42  \n",
       "18  [issue, affirmative, action, explicit, quota, ...                       84  \n",
       "19  [know, hate, main, subreddit, certain, white, ...                      221  \n",
       "20  [m, fan, affirmative, action, tho, help, minor...                       40  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df['normalized_tokens'] = comments_df['tokens_new'].apply(lambda x: normalize_tokens(x, stop_words_freq))\n",
    "comments_df['normalized_tokens_count'] = comments_df['normalized_tokens'].apply(lambda x: len(x))\n",
    "\n",
    "comments_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add word count column\n",
    "comments_df['word_count'] = comments_df['tokens_new'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>username</th>\n",
       "      <th>time_created</th>\n",
       "      <th>flair</th>\n",
       "      <th>body</th>\n",
       "      <th>tokens_new</th>\n",
       "      <th>normalized_tokens</th>\n",
       "      <th>normalized_tokens_count</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14m8mf4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-29 10:54:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Megathread] Supreme Court Ruling on Affirmati...</td>\n",
       "      <td>[Megathread, Supreme, Court, Ruling, on, Affir...</td>\n",
       "      <td>[megathread, supreme, court, ruling, affirmati...</td>\n",
       "      <td>62</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jq5du0z</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-30 11:33:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "      <td>[Thanks, to, everyone, who, engaged, in, insig...</td>\n",
       "      <td>[thank, engage, insightful, respectful, discou...</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jq0dgzx</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>2023-06-29 11:16:15</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "      <td>[I, would, prefer, using, a, process, that, ta...</td>\n",
       "      <td>[prefer, process, take, account, poverty, inst...</td>\n",
       "      <td>52</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jq0cg7k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>2023-06-29 11:09:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "      <td>[u/Tungsten_,, Thanks, for, creating, a, secti...</td>\n",
       "      <td>[u/tungsten_,, thank, create, section, discuss...</td>\n",
       "      <td>126</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jq0f52k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>2023-06-29 11:26:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "      <td>[As, with, anything, related, to, Asians, in, ...</td>\n",
       "      <td>[relate, asians, politic, m, see, lot, non, as...</td>\n",
       "      <td>25</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3311</th>\n",
       "      <td>epcqlfa</td>\n",
       "      <td>t1_elt4c17</td>\n",
       "      <td>arientyse</td>\n",
       "      <td>2019-05-29 14:15:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Please don't refer to us as \"the blacks.\" It's...</td>\n",
       "      <td>[Please, do, n't, refer, to, us, as, the, blac...</td>\n",
       "      <td>[refer, black, okay, black, people, harm]</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3312</th>\n",
       "      <td>4ziaf0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unkle</td>\n",
       "      <td>2016-08-25 07:44:54</td>\n",
       "      <td>senyorito</td>\n",
       "      <td>From Self-Interest to Collective Morality: How...</td>\n",
       "      <td>[From, Self, Interest, to, Collective, Moralit...</td>\n",
       "      <td>[self, interest, collective, morality, reframe...</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3313</th>\n",
       "      <td>d6witvi</td>\n",
       "      <td>t3_4ziaf0</td>\n",
       "      <td>Swordbow</td>\n",
       "      <td>2016-08-25 14:47:22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The writer takes good nuggets but integrates t...</td>\n",
       "      <td>[The, writer, takes, good, nuggets, but, integ...</td>\n",
       "      <td>[writer, take, good, nugget, integrate, questi...</td>\n",
       "      <td>156</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3314</th>\n",
       "      <td>d6z9lhb</td>\n",
       "      <td>t3_4ziaf0</td>\n",
       "      <td>IRVCath</td>\n",
       "      <td>2016-08-27 17:02:04</td>\n",
       "      <td>Fil-Am, 1.5 Generation</td>\n",
       "      <td>Interestingly enough, even though Filipinos ar...</td>\n",
       "      <td>[Interestingly, enough, even, though, Filipino...</td>\n",
       "      <td>[interestingly, filipino, relatively, disadvan...</td>\n",
       "      <td>51</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3315</th>\n",
       "      <td>d6w9qin</td>\n",
       "      <td>t3_4ziaf0</td>\n",
       "      <td>BletchTheWalrus</td>\n",
       "      <td>2016-08-25 11:40:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nope</td>\n",
       "      <td>[Nope]</td>\n",
       "      <td>[nope]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3029 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id   parent_id         username         time_created  \\\n",
       "0     14m8mf4         NaN        Tungsten_  2023-06-29 10:54:44   \n",
       "1     jq5du0z  t3_14m8mf4        Tungsten_  2023-06-30 11:33:11   \n",
       "2     jq0dgzx  t3_14m8mf4   ProudBlackMatt  2023-06-29 11:16:15   \n",
       "3     jq0cg7k  t3_14m8mf4     TomatoCanned  2023-06-29 11:09:47   \n",
       "4     jq0f52k  t3_14m8mf4     bad-fengshui  2023-06-29 11:26:41   \n",
       "...       ...         ...              ...                  ...   \n",
       "3311  epcqlfa  t1_elt4c17        arientyse  2019-05-29 14:15:14   \n",
       "3312   4ziaf0         NaN            unkle  2016-08-25 07:44:54   \n",
       "3313  d6witvi   t3_4ziaf0         Swordbow  2016-08-25 14:47:22   \n",
       "3314  d6z9lhb   t3_4ziaf0          IRVCath  2016-08-27 17:02:04   \n",
       "3315  d6w9qin   t3_4ziaf0  BletchTheWalrus  2016-08-25 11:40:47   \n",
       "\n",
       "                       flair  \\\n",
       "0                        NaN   \n",
       "1                        NaN   \n",
       "2           Chinese-American   \n",
       "3                        NaN   \n",
       "4                        NaN   \n",
       "...                      ...   \n",
       "3311                     NaN   \n",
       "3312               senyorito   \n",
       "3313                     NaN   \n",
       "3314  Fil-Am, 1.5 Generation   \n",
       "3315                     NaN   \n",
       "\n",
       "                                                   body  \\\n",
       "0     [Megathread] Supreme Court Ruling on Affirmati...   \n",
       "1     Thanks to everyone who engaged in insightful a...   \n",
       "2     I would prefer using a process that takes into...   \n",
       "3     u/Tungsten_, Thanks for creating a section jus...   \n",
       "4     As with anything related to Asians in politics...   \n",
       "...                                                 ...   \n",
       "3311  Please don't refer to us as \"the blacks.\" It's...   \n",
       "3312  From Self-Interest to Collective Morality: How...   \n",
       "3313  The writer takes good nuggets but integrates t...   \n",
       "3314  Interestingly enough, even though Filipinos ar...   \n",
       "3315                                               Nope   \n",
       "\n",
       "                                             tokens_new  \\\n",
       "0     [Megathread, Supreme, Court, Ruling, on, Affir...   \n",
       "1     [Thanks, to, everyone, who, engaged, in, insig...   \n",
       "2     [I, would, prefer, using, a, process, that, ta...   \n",
       "3     [u/Tungsten_,, Thanks, for, creating, a, secti...   \n",
       "4     [As, with, anything, related, to, Asians, in, ...   \n",
       "...                                                 ...   \n",
       "3311  [Please, do, n't, refer, to, us, as, the, blac...   \n",
       "3312  [From, Self, Interest, to, Collective, Moralit...   \n",
       "3313  [The, writer, takes, good, nuggets, but, integ...   \n",
       "3314  [Interestingly, enough, even, though, Filipino...   \n",
       "3315                                             [Nope]   \n",
       "\n",
       "                                      normalized_tokens  \\\n",
       "0     [megathread, supreme, court, ruling, affirmati...   \n",
       "1     [thank, engage, insightful, respectful, discou...   \n",
       "2     [prefer, process, take, account, poverty, inst...   \n",
       "3     [u/tungsten_,, thank, create, section, discuss...   \n",
       "4     [relate, asians, politic, m, see, lot, non, as...   \n",
       "...                                                 ...   \n",
       "3311          [refer, black, okay, black, people, harm]   \n",
       "3312  [self, interest, collective, morality, reframe...   \n",
       "3313  [writer, take, good, nugget, integrate, questi...   \n",
       "3314  [interestingly, filipino, relatively, disadvan...   \n",
       "3315                                             [nope]   \n",
       "\n",
       "      normalized_tokens_count  word_count  \n",
       "0                          62          78  \n",
       "1                           9          20  \n",
       "2                          52         103  \n",
       "3                         126         269  \n",
       "4                          25          59  \n",
       "...                       ...         ...  \n",
       "3311                        6          20  \n",
       "3312                       11          20  \n",
       "3313                      156         354  \n",
       "3314                       51         101  \n",
       "3315                        1           1  \n",
       "\n",
       "[3029 rows x 10 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# check if normalized_tokens column is list or string - Result: list\n",
    "print(type(comments_df['normalized_tokens'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save above df as csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df.to_csv('../data/comments_no_chinese_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comments_processed.csv\n"
     ]
    }
   ],
   "source": [
    "input_file = 'comments.csv'\n",
    "output_file = input_file[0:-4] + '_processed' + input_file[-4:]\n",
    "\n",
    "print(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_fq_dist = nltk.ConditionalFreqDist(((len(w), w) for w in comments_df['normalized_tokens'].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130955\n"
     ]
    }
   ],
   "source": [
    "print(comments_fq_dist.N()) # number of total words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('asian', 2442),\n",
       " ('white', 1321),\n",
       " ('think', 956),\n",
       " ('black', 787),\n",
       " ('group', 573)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_fq_dist[5].most_common(5) #most common 5-letter words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d9defa72c2715dab9f7f172572cd30a1ab1a2083462d32ef96aadb7c6e0c73b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
