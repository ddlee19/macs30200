{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn\n",
    "import json\n",
    "import urllib.parse\n",
    "\n",
    "import spacy\n",
    "import spacy.symbols\n",
    "\n",
    "import graphviz\n",
    "\n",
    "import os.path\n",
    "import subprocess\n",
    "import io\n",
    "import tempfile\n",
    "\n",
    "%matplotlib inline\n",
    "#import gensim\n",
    "#from gensim.corpora import Dictionary\n",
    "#from gensim.models import LdaModel, CoherenceModel, LsiModel, HdpModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3623, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pancake_muncher</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yet colleges will allow alumni and doners in e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>suberry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I just hated Affirmative Action as a distracti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Puzzled-Painter3301</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My own feeling is that I was never in love wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>e9967780</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anti Asian racism whether against East Asians ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Can we overturn legacy and athlete admissions ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OkartoIceCream</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I want to remind people that in California, on...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              username        flair_text  \\\n",
       "0            Tungsten_               NaN   \n",
       "1       ProudBlackMatt  Chinese-American   \n",
       "2         TomatoCanned               NaN   \n",
       "3         bad-fengshui               NaN   \n",
       "4      Pancake_muncher               NaN   \n",
       "5              suberry               NaN   \n",
       "6  Puzzled-Painter3301               NaN   \n",
       "7             e9967780               NaN   \n",
       "8                  NaN               NaN   \n",
       "9       OkartoIceCream               NaN   \n",
       "\n",
       "                                                body  \n",
       "0  Thanks to everyone who engaged in insightful a...  \n",
       "1  I would prefer using a process that takes into...  \n",
       "2  u/Tungsten_, Thanks for creating a section jus...  \n",
       "3  As with anything related to Asians in politics...  \n",
       "4  Yet colleges will allow alumni and doners in e...  \n",
       "5  I just hated Affirmative Action as a distracti...  \n",
       "6  My own feeling is that I was never in love wit...  \n",
       "7  Anti Asian racism whether against East Asians ...  \n",
       "8  Can we overturn legacy and athlete admissions ...  \n",
       "9  I want to remind people that in California, on...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df = pd.read_csv('../data/top_100_post_comments_user_flair.txt', header=None, names=['username', 'flair_text', 'body'])\n",
    "\n",
    "print(comments_df.shape)\n",
    "comments_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop rows with deleted text body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(306,)\n",
      "(3317, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pancake_muncher</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yet colleges will allow alumni and doners in e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>suberry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I just hated Affirmative Action as a distracti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Puzzled-Painter3301</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My own feeling is that I was never in love wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>e9967780</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anti Asian racism whether against East Asians ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Can we overturn legacy and athlete admissions ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OkartoIceCream</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I want to remind people that in California, on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ShalomHasaeyo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I think this is great news for our nation. Whi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SteadfastEnd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My main concern is that, since the Court did n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MsNewKicks</td>\n",
       "      <td>First Of Her Name, Queen ABG, 나쁜 기집애, Blocker ...</td>\n",
       "      <td>There is obviously a lot to it and no real eas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pal2002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I’m sure selective schools like Harvard will f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ok-Value5827</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TBH, I don't know the depth of how affirmative...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               username                                         flair_text  \\\n",
       "0             Tungsten_                                                NaN   \n",
       "1        ProudBlackMatt                                   Chinese-American   \n",
       "2          TomatoCanned                                                NaN   \n",
       "3          bad-fengshui                                                NaN   \n",
       "4       Pancake_muncher                                                NaN   \n",
       "5               suberry                                                NaN   \n",
       "6   Puzzled-Painter3301                                                NaN   \n",
       "7              e9967780                                                NaN   \n",
       "8                   NaN                                                NaN   \n",
       "9        OkartoIceCream                                                NaN   \n",
       "10        ShalomHasaeyo                                                NaN   \n",
       "11         SteadfastEnd                                                NaN   \n",
       "12           MsNewKicks  First Of Her Name, Queen ABG, 나쁜 기집애, Blocker ...   \n",
       "14              pal2002                                                NaN   \n",
       "15         Ok-Value5827                                                NaN   \n",
       "\n",
       "                                                 body  \n",
       "0   Thanks to everyone who engaged in insightful a...  \n",
       "1   I would prefer using a process that takes into...  \n",
       "2   u/Tungsten_, Thanks for creating a section jus...  \n",
       "3   As with anything related to Asians in politics...  \n",
       "4   Yet colleges will allow alumni and doners in e...  \n",
       "5   I just hated Affirmative Action as a distracti...  \n",
       "6   My own feeling is that I was never in love wit...  \n",
       "7   Anti Asian racism whether against East Asians ...  \n",
       "8   Can we overturn legacy and athlete admissions ...  \n",
       "9   I want to remind people that in California, on...  \n",
       "10  I think this is great news for our nation. Whi...  \n",
       "11  My main concern is that, since the Court did n...  \n",
       "12  There is obviously a lot to it and no real eas...  \n",
       "14  I’m sure selective schools like Harvard will f...  \n",
       "15  TBH, I don't know the depth of how affirmative...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deleted_rows = comments_df[comments_df['body'].isin(['[deleted]','[removed]'])]\n",
    "\n",
    "# drop rows\n",
    "comments_df.drop(deleted_rows.index, inplace=True)\n",
    "print(comments_df.shape)\n",
    "comments_df.reset_index(drop=True)\n",
    "comments_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: 306 rows were of deleted comments and so dropped from dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop rows that have been filtered by the AutoModerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3283, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>tokens_new</th>\n",
       "      <th>normalized_tokens</th>\n",
       "      <th>normalized_tokens_count</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "      <td>[Thanks, to, everyone, who, engaged, in, insig...</td>\n",
       "      <td>[thank, engage, insightful, respectful, discou...</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "      <td>[I, would, prefer, using, a, process, that, ta...</td>\n",
       "      <td>[prefer, process, take, account, poverty, inst...</td>\n",
       "      <td>52</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "      <td>[u/Tungsten_,, Thanks, for, creating, a, secti...</td>\n",
       "      <td>[u/tungsten_,, thank, create, section, discuss...</td>\n",
       "      <td>126</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "      <td>[As, with, anything, related, to, Asians, in, ...</td>\n",
       "      <td>[relate, asians, politic, m, see, lot, non, as...</td>\n",
       "      <td>25</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pancake_muncher</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yet colleges will allow alumni and doners in e...</td>\n",
       "      <td>[Yet, colleges, will, allow, alumni, and, done...</td>\n",
       "      <td>[college, allow, alumnus, doner, easily, consi...</td>\n",
       "      <td>19</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>suberry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I just hated Affirmative Action as a distracti...</td>\n",
       "      <td>[I, just, hated, Affirmative, Action, as, a, d...</td>\n",
       "      <td>[hate, affirmative, action, distraction, banda...</td>\n",
       "      <td>78</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Puzzled-Painter3301</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My own feeling is that I was never in love wit...</td>\n",
       "      <td>[My, own, feeling, is, that, I, was, never, in...</td>\n",
       "      <td>[feeling, love, affirmative, action, possible,...</td>\n",
       "      <td>102</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>e9967780</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anti Asian racism whether against East Asians ...</td>\n",
       "      <td>[Anti, Asian, racism, whether, against, East, ...</td>\n",
       "      <td>[anti, asian, racism, east, asians, south, asi...</td>\n",
       "      <td>21</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Can we overturn legacy and athlete admissions ...</td>\n",
       "      <td>[Can, we, overturn, legacy, and, athlete, admi...</td>\n",
       "      <td>[overturn, legacy, athlete, admission, point, ...</td>\n",
       "      <td>15</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OkartoIceCream</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I want to remind people that in California, on...</td>\n",
       "      <td>[I, want, to, remind, people, that, in, Califo...</td>\n",
       "      <td>[want, remind, people, california, progressive...</td>\n",
       "      <td>104</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ShalomHasaeyo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I think this is great news for our nation. Whi...</td>\n",
       "      <td>[I, think, this, is, great, news, for, our, na...</td>\n",
       "      <td>[think, great, news, nation, abolish, legacy, ...</td>\n",
       "      <td>58</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SteadfastEnd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My main concern is that, since the Court did n...</td>\n",
       "      <td>[My, main, concern, is, that, since, the, Cour...</td>\n",
       "      <td>[main, concern, court, strike, grutter, outrig...</td>\n",
       "      <td>72</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MsNewKicks</td>\n",
       "      <td>First Of Her Name, Queen ABG, 나쁜 기집애, Blocker ...</td>\n",
       "      <td>There is obviously a lot to it and no real eas...</td>\n",
       "      <td>[There, is, obviously, a, lot, to, it, and, no...</td>\n",
       "      <td>[obviously, lot, real, easy, answer, satisfy, ...</td>\n",
       "      <td>46</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pal2002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I’m sure selective schools like Harvard will f...</td>\n",
       "      <td>[I, ’m, sure, selective, schools, like, Harvar...</td>\n",
       "      <td>[m, sure, selective, school, like, harvard, fi...</td>\n",
       "      <td>20</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ok-Value5827</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TBH, I don't know the depth of how affirmative...</td>\n",
       "      <td>[TBH, I, do, n't, know, the, depth, of, how, a...</td>\n",
       "      <td>[tbh, know, depth, affirmative, action, colleg...</td>\n",
       "      <td>39</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               username                                         flair_text  \\\n",
       "0             Tungsten_                                                NaN   \n",
       "1        ProudBlackMatt                                   Chinese-American   \n",
       "2          TomatoCanned                                                NaN   \n",
       "3          bad-fengshui                                                NaN   \n",
       "4       Pancake_muncher                                                NaN   \n",
       "5               suberry                                                NaN   \n",
       "6   Puzzled-Painter3301                                                NaN   \n",
       "7              e9967780                                                NaN   \n",
       "8                   NaN                                                NaN   \n",
       "9        OkartoIceCream                                                NaN   \n",
       "10        ShalomHasaeyo                                                NaN   \n",
       "11         SteadfastEnd                                                NaN   \n",
       "12           MsNewKicks  First Of Her Name, Queen ABG, 나쁜 기집애, Blocker ...   \n",
       "14              pal2002                                                NaN   \n",
       "15         Ok-Value5827                                                NaN   \n",
       "\n",
       "                                                 body  \\\n",
       "0   Thanks to everyone who engaged in insightful a...   \n",
       "1   I would prefer using a process that takes into...   \n",
       "2   u/Tungsten_, Thanks for creating a section jus...   \n",
       "3   As with anything related to Asians in politics...   \n",
       "4   Yet colleges will allow alumni and doners in e...   \n",
       "5   I just hated Affirmative Action as a distracti...   \n",
       "6   My own feeling is that I was never in love wit...   \n",
       "7   Anti Asian racism whether against East Asians ...   \n",
       "8   Can we overturn legacy and athlete admissions ...   \n",
       "9   I want to remind people that in California, on...   \n",
       "10  I think this is great news for our nation. Whi...   \n",
       "11  My main concern is that, since the Court did n...   \n",
       "12  There is obviously a lot to it and no real eas...   \n",
       "14  I’m sure selective schools like Harvard will f...   \n",
       "15  TBH, I don't know the depth of how affirmative...   \n",
       "\n",
       "                                           tokens_new  \\\n",
       "0   [Thanks, to, everyone, who, engaged, in, insig...   \n",
       "1   [I, would, prefer, using, a, process, that, ta...   \n",
       "2   [u/Tungsten_,, Thanks, for, creating, a, secti...   \n",
       "3   [As, with, anything, related, to, Asians, in, ...   \n",
       "4   [Yet, colleges, will, allow, alumni, and, done...   \n",
       "5   [I, just, hated, Affirmative, Action, as, a, d...   \n",
       "6   [My, own, feeling, is, that, I, was, never, in...   \n",
       "7   [Anti, Asian, racism, whether, against, East, ...   \n",
       "8   [Can, we, overturn, legacy, and, athlete, admi...   \n",
       "9   [I, want, to, remind, people, that, in, Califo...   \n",
       "10  [I, think, this, is, great, news, for, our, na...   \n",
       "11  [My, main, concern, is, that, since, the, Cour...   \n",
       "12  [There, is, obviously, a, lot, to, it, and, no...   \n",
       "14  [I, ’m, sure, selective, schools, like, Harvar...   \n",
       "15  [TBH, I, do, n't, know, the, depth, of, how, a...   \n",
       "\n",
       "                                    normalized_tokens  \\\n",
       "0   [thank, engage, insightful, respectful, discou...   \n",
       "1   [prefer, process, take, account, poverty, inst...   \n",
       "2   [u/tungsten_,, thank, create, section, discuss...   \n",
       "3   [relate, asians, politic, m, see, lot, non, as...   \n",
       "4   [college, allow, alumnus, doner, easily, consi...   \n",
       "5   [hate, affirmative, action, distraction, banda...   \n",
       "6   [feeling, love, affirmative, action, possible,...   \n",
       "7   [anti, asian, racism, east, asians, south, asi...   \n",
       "8   [overturn, legacy, athlete, admission, point, ...   \n",
       "9   [want, remind, people, california, progressive...   \n",
       "10  [think, great, news, nation, abolish, legacy, ...   \n",
       "11  [main, concern, court, strike, grutter, outrig...   \n",
       "12  [obviously, lot, real, easy, answer, satisfy, ...   \n",
       "14  [m, sure, selective, school, like, harvard, fi...   \n",
       "15  [tbh, know, depth, affirmative, action, colleg...   \n",
       "\n",
       "    normalized_tokens_count  word_count  \n",
       "0                         9          20  \n",
       "1                        52         103  \n",
       "2                       126         269  \n",
       "3                        25          59  \n",
       "4                        19          40  \n",
       "5                        78         171  \n",
       "6                       102         231  \n",
       "7                        21          46  \n",
       "8                        15          29  \n",
       "9                       104         200  \n",
       "10                       58         127  \n",
       "11                       72         141  \n",
       "12                       46         147  \n",
       "14                       20          48  \n",
       "15                       39          84  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moderated_rows = comments_df[comments_df['username']=='AutoModerator']\n",
    "moderated_rows\n",
    "\n",
    "comments_df.drop(moderated_rows.index, inplace=True)\n",
    "print(comments_df.shape)\n",
    "comments_df.reset_index(drop=True)\n",
    "comments_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_str(str):\n",
    "    tokenized = []\n",
    "    doc = nlp(str)\n",
    "    for token in doc:\n",
    "        if not token.is_punct and len(token.text.strip()) > 0:\n",
    "            tokenized.append(token.text)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column of tokens\n",
    "comments_df['tokens'] = comments_df['body'].apply(lambda x: tokenize_str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "      <td>[Thanks, to, everyone, who, engaged, in, insig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "      <td>[I, would, prefer, using, a, process, that, ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "      <td>[u, Tungsten, Thanks, for, creating, a, sectio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "      <td>[As, with, anything, related, to, Asians, in, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pancake_muncher</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yet colleges will allow alumni and doners in e...</td>\n",
       "      <td>[Yet, colleges, will, allow, alumni, and, done...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          username        flair_text  \\\n",
       "0        Tungsten_               NaN   \n",
       "1   ProudBlackMatt  Chinese-American   \n",
       "2     TomatoCanned               NaN   \n",
       "3     bad-fengshui               NaN   \n",
       "4  Pancake_muncher               NaN   \n",
       "\n",
       "                                                body  \\\n",
       "0  Thanks to everyone who engaged in insightful a...   \n",
       "1  I would prefer using a process that takes into...   \n",
       "2  u/Tungsten_, Thanks for creating a section jus...   \n",
       "3  As with anything related to Asians in politics...   \n",
       "4  Yet colleges will allow alumni and doners in e...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [Thanks, to, everyone, who, engaged, in, insig...  \n",
       "1  [I, would, prefer, using, a, process, that, ta...  \n",
       "2  [u, Tungsten, Thanks, for, creating, a, sectio...  \n",
       "3  [As, with, anything, related, to, Asians, in, ...  \n",
       "4  [Yet, colleges, will, allow, alumni, and, done...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(comments_df['tokens'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem with Reddit comments: We don't want to get split on (get rid of) forward slashes nor get rid of punctuations on usernames like \"u/Tungsten_\"\n",
    "\n",
    "So, instead of using the native Spacy package, we use RedditScore which is built on Spacy but modified for Reddit/Twitter comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u/Tungsten_, Thanks for creating a section just to discuss this. When I read the news I immediately went searching for a forum where folks might have civil discourse on this topic.\n",
      "\n",
      "Just had a few comments/questions:\n",
      "\n",
      "1. Has anyone come across seemingly legitimate data sets on asians & college admission with respect to Affirmative Action (AA for short going forward)\n",
      "2. As an Asian (not born in the US but pretty much assimilated here for 35+ years), I am conflicted. Research results like this one show: [https://www.pewresearch.org/race-ethnicity/2023/06/08/asian-americans-hold-mixed-views-around-affirmative-action/](https://www.pewresearch.org/race-ethnicity/2023/06/08/asian-americans-hold-mixed-views-around-affirmative-action/) that something like 53% Asians think AA is a good thing, and yet when you scroll down and look at the question of \"Should colleges consider race/ethnicity in college admissions,\" the percentage of Asians that say yes are at 21%, no at 76%.\n",
      "\n",
      "I am part of the 76%.... and I'm conflicted. I know especially for the underserved, AA makes a significant impact in giving folks better chances at life which in turn translates to diversity in every facet of work, society, life in general, which I view is a good thing.\n",
      "\n",
      "But specifically regarding college admissions.. say for my own kids? (not college aged yet)  I would like to see more data on whether year 2000 and beyond AA in college admissions was harmful to Asians in general. In my own experience (anecdotal, totally not data science driven), I feel like AA in college admissions has hurt friends and family, in a reverse sort of sense.\n",
      "\n",
      "But for the sake of the underserved, I didn't want AA to go away. So I am deeply conflicted.\n",
      "\n",
      "Your thoughts?\n"
     ]
    }
   ],
   "source": [
    "sample_comment = comments_df['body'][2]\n",
    "print(sample_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RedditScore \n",
    "(not using anymore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/crazyfrogspb/RedditScore.git\n",
      "  Cloning https://github.com/crazyfrogspb/RedditScore.git to c:\\users\\danie\\appdata\\local\\temp\\pip-req-build-l3ypk4yu\n",
      "Requirement already satisfied: setuptools in c:\\users\\danie\\anaconda3\\lib\\site-packages (from redditscore==0.7.3) (45.2.0.post20200210)\n",
      "Requirement already satisfied: spacy>=2.0.11 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from redditscore==0.7.3) (3.7.4)\n",
      "Collecting tldextract>=2.1.0\n",
      "  Downloading tldextract-4.0.0-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: requests>=2.18.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from redditscore==0.7.3) (2.22.0)\n",
      "Requirement already satisfied: scikit-learn>=0.19.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from redditscore==0.7.3) (0.22.1)\n",
      "Requirement already satisfied: pandas>=0.22.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from redditscore==0.7.3) (1.0.1)\n",
      "Requirement already satisfied: scipy>=1.0.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from redditscore==0.7.3) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from redditscore==0.7.3) (1.18.1)\n",
      "Requirement already satisfied: beautifulsoup4>=4.6.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from redditscore==0.7.3) (4.8.2)\n",
      "Requirement already satisfied: adjustText>=0.6.3 in c:\\users\\danie\\anaconda3\\lib\\site-packages\\adjusttext-0.8b2-py3.7.egg (from redditscore==0.7.3) (0.8b2)\n",
      "Collecting eventlet>=0.22.1\n",
      "  Downloading eventlet-0.35.2-py3-none-any.whl (359 kB)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (20.1)\n",
      "Requirement already satisfied: typing-extensions<4.5.0,>=3.7.4.1; python_version < \"3.8\" in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (4.4.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (1.0.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (2.4.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (2.11.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (3.0.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (4.42.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (3.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (2.5.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (1.1.2)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (8.2.3)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (0.9.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (2.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (1.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (0.3.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (6.4.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from spacy>=2.0.11->redditscore==0.7.3) (2.0.10)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from tldextract>=2.1.0->redditscore==0.7.3) (3.0.12)\n",
      "Requirement already satisfied: idna in c:\\users\\danie\\anaconda3\\lib\\site-packages (from tldextract>=2.1.0->redditscore==0.7.3) (2.8)\n",
      "Collecting requests-file>=1.4\n",
      "  Downloading requests_file-2.0.0-py2.py3-none-any.whl (4.2 kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from requests>=2.18.0->redditscore==0.7.3) (1.25.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from requests>=2.18.0->redditscore==0.7.3) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from requests>=2.18.0->redditscore==0.7.3) (2022.6.15)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from scikit-learn>=0.19.0->redditscore==0.7.3) (0.14.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from pandas>=0.22.0->redditscore==0.7.3) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from pandas>=0.22.0->redditscore==0.7.3) (2019.3)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.6.0->redditscore==0.7.3) (1.9.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\danie\\anaconda3\\lib\\site-packages (from adjustText>=0.6.3->redditscore==0.7.3) (3.1.3)\n",
      "Collecting dnspython>=1.15.0\n",
      "  Downloading dnspython-2.3.0-py3-none-any.whl (283 kB)\n",
      "Collecting greenlet>=1.0\n",
      "  Downloading greenlet-3.0.3-cp37-cp37m-win_amd64.whl (291 kB)\n",
      "Requirement already satisfied: six in c:\\users\\danie\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy>=2.0.11->redditscore==0.7.3) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy>=2.0.11->redditscore==0.7.3) (2.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from jinja2->spacy>=2.0.11->redditscore==0.7.3) (1.1.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.0.11->redditscore==0.7.3) (0.5.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version == \"3.7\" in c:\\users\\danie\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.0.11->redditscore==0.7.3) (1.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.0.11->redditscore==0.7.3) (2.14.6)\n",
      "Requirement already satisfied: colorama>=0.4.6; sys_platform == \"win32\" and python_version >= \"3.7\" in c:\\users\\danie\\anaconda3\\lib\\site-packages (from wasabi<1.2.0,>=0.9.1->spacy>=2.0.11->redditscore==0.7.3) (0.4.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.0.11->redditscore==0.7.3) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.0.11->redditscore==0.7.3) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy>=2.0.11->redditscore==0.7.3) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy>=2.0.11->redditscore==0.7.3) (0.16.0)\n",
      "Requirement already satisfied: zipp>=0.5; python_version < \"3.8\" in c:\\users\\danie\\anaconda3\\lib\\site-packages (from catalogue<2.1.0,>=2.0.6->spacy>=2.0.11->redditscore==0.7.3) (2.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from matplotlib->adjustText>=0.6.3->redditscore==0.7.3) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\danie\\anaconda3\\lib\\site-packages (from matplotlib->adjustText>=0.6.3->redditscore==0.7.3) (0.10.0)\n",
      "Building wheels for collected packages: redditscore\n",
      "  Building wheel for redditscore (setup.py): started\n",
      "  Building wheel for redditscore (setup.py): finished with status 'done'\n",
      "  Created wheel for redditscore: filename=redditscore-0.7.3-py3-none-any.whl size=7852150 sha256=affc20c003c412f8a666e5207c543b14bc9fd9152ef7420421ec55bb62eb8563\n",
      "  Stored in directory: C:\\Users\\danie\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-vd0158ij\\wheels\\18\\6a\\1b\\4be247e0dd084d11afc3c7abf830b0fe5a7b5c2f165876803e\n",
      "Successfully built redditscore\n",
      "Installing collected packages: requests-file, tldextract, dnspython, greenlet, eventlet, dill, redditscore\n",
      "  Attempting uninstall: greenlet\n",
      "    Found existing installation: greenlet 0.4.15\n",
      "    Uninstalling greenlet-0.4.15:\n",
      "      Successfully uninstalled greenlet-0.4.15\n",
      "Successfully installed dill-0.3.7 dnspython-2.3.0 eventlet-0.35.2 greenlet-3.0.3 redditscore-0.7.3 requests-file-2.0.0 tldextract-4.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/crazyfrogspb/RedditScore.git 'C:\\Users\\danie\\AppData\\Local\\Temp\\pip-req-build-l3ypk4yu'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/crazyfrogspb/RedditScore.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "add() takes exactly 2 positional arguments (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-c65f745d9e40>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mredditscore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCrazyTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCrazyTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhashtags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlowercase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreddit_usernames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtokenize_str_reddit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\redditscore\\tokenizer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, lowercase, keepcaps, normalize, ignore_quotes, ignore_reddit_quotes, ignore_stopwords, stem, remove_punct, remove_breaks, decontract, twitter_handles, urls, hashtags, numbers, subreddits, reddit_usernames, emails, extra_patterns, keep_untokenized, whitespaces_to_underscores, remove_nonunicode, pos_emojis, neg_emojis, neutral_emojis, print_url_warnings, latin_chars_fix, ngrams)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         self._merging_matcher.add(\n\u001b[1;32m--> 359\u001b[1;33m             'HASHTAG', None, [{'ORTH': '#'}, {'IS_ASCII': True}])\n\u001b[0m\u001b[0;32m    360\u001b[0m         self._merging_matcher.add(\n\u001b[0;32m    361\u001b[0m             \u001b[1;34m'SUBREDDIT'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\spacy\\matcher\\matcher.pyx\u001b[0m in \u001b[0;36mspacy.matcher.matcher.Matcher.add\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: add() takes exactly 2 positional arguments (3 given)"
     ]
    }
   ],
   "source": [
    "from redditscore.tokenizer import CrazyTokenizer\n",
    "\n",
    "tokenizer = CrazyTokenizer(hashtags=False, lowercase=True, reddit_usernames=True, normalize=False)\n",
    "\n",
    "def tokenize_str_reddit(str):\n",
    "    tokenized = []\n",
    "    doc = tokenizer.tokenizer(str)\n",
    "    for token in doc:\n",
    "        if not token.is_punct and len(token.text.strip()) > 0:\n",
    "            tokenized.append(token.text)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "add() takes exactly 2 positional arguments (3 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-dcb2731b9d1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcomments_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokens'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomments_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'body'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtokenize_str_reddit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   3846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3848\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3850\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-dcb2731b9d1f>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcomments_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokens'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomments_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'body'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtokenize_str_reddit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-21-374fc19dc617>\u001b[0m in \u001b[0;36mtokenize_str_reddit\u001b[1;34m(str)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtokenize_str_reddit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtokenized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCrazyTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_punct\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\redditscore\\tokenizer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, lowercase, keepcaps, normalize, ignore_quotes, ignore_reddit_quotes, ignore_stopwords, stem, remove_punct, remove_breaks, decontract, twitter_handles, urls, hashtags, numbers, subreddits, reddit_usernames, emails, extra_patterns, keep_untokenized, whitespaces_to_underscores, remove_nonunicode, pos_emojis, neg_emojis, neutral_emojis, print_url_warnings, latin_chars_fix, ngrams)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         self._merging_matcher.add(\n\u001b[1;32m--> 359\u001b[1;33m             'HASHTAG', None, [{'ORTH': '#'}, {'IS_ASCII': True}])\n\u001b[0m\u001b[0;32m    360\u001b[0m         self._merging_matcher.add(\n\u001b[0;32m    361\u001b[0m             \u001b[1;34m'SUBREDDIT'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\spacy\\matcher\\matcher.pyx\u001b[0m in \u001b[0;36mspacy.matcher.matcher.Matcher.add\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: add() takes exactly 2 positional arguments (3 given)"
     ]
    }
   ],
   "source": [
    "comments_df['tokens'] = comments_df['body'].apply(lambda x: tokenize_str_reddit(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special case in Spacy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'CrazyTokenizer' doesn't work bc the code is reliant on an old version of Spacy. So, we will create a special case in Spacy's tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', 'Hi', ',', 'u', '/', 'Tungsten', '_', ',', 'Thanks', 'for', 'creating', 'a', 'section']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "65",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0950b2f4b101>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mspecial_case\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'TEXT'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'REGEX'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34mr'u\\/([[:word:]]|-){3,23}'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_special_case\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'redditor'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspecial_case\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' Hi, u/Tungsten_, Thanks for creating a section'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\spacy\\tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.add_special_case\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\spacy\\tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._validate_special_case\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 65"
     ]
    }
   ],
   "source": [
    "from spacy.symbols import ORTH\n",
    "\n",
    "doc = nlp(' Hi, u/Tungsten_, Thanks for creating a section')\n",
    "print([w.text for w in doc])\n",
    "\n",
    "special_case = [{'TEXT': {'REGEX': r'u\\/([[:word:]]|-){3,23}'}}]\n",
    "nlp.tokenizer.add_special_case('redditor', special_case)\n",
    "doc = nlp(' Hi, u/Tungsten_, Thanks for creating a section')\n",
    "\n",
    "print(print([w.text for w in doc]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "nlp.tokenizer = Tokenizer(nlp.vocab, token_match=re.compile(r'u/{1}\\w{3,23}').search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_comment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-0e4e9aa983af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_comment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sample_comment' is not defined"
     ]
    }
   ],
   "source": [
    "doc = nlp(sample_comment)\n",
    "\n",
    "print([w.text for w in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u', 'Tungsten', 'Thanks', 'for', 'creating', 'a', 'section', 'just', 'to', 'discuss', 'this', 'When', 'I', 'read', 'the', 'news', 'I', 'immediately', 'went', 'searching', 'for', 'a', 'forum', 'where', 'folks', 'might', 'have', 'civil', 'discourse', 'on', 'this', 'topic', 'Just', 'had', 'a', 'few', 'comments', 'questions', '1', 'Has', 'anyone', 'come', 'across', 'seemingly', 'legitimate', 'data', 'sets', 'on', 'asians', 'college', 'admission', 'with', 'respect', 'to', 'Affirmative', 'Action', 'AA', 'for', 'short', 'going', 'forward', '2', 'As', 'an', 'Asian', 'not', 'born', 'in', 'the', 'US', 'but', 'pretty', 'much', 'assimilated', 'here', 'for', '35', '+', 'years', 'I', 'am', 'conflicted', 'Research', 'results', 'like', 'this', 'one', 'show', 'https://www.pewresearch.org/race-ethnicity/2023/06/08/asian-americans-hold-mixed-views-around-affirmative-action/](https://www.pewresearch.org/race-ethnicity/2023/06/08/asian-americans-hold-mixed-views-around-affirmative-action/', 'that', 'something', 'like', '53', 'Asians', 'think', 'AA', 'is', 'a', 'good', 'thing', 'and', 'yet', 'when', 'you', 'scroll', 'down', 'and', 'look', 'at', 'the', 'question', 'of', 'Should', 'colleges', 'consider', 'race', 'ethnicity', 'in', 'college', 'admissions', 'the', 'percentage', 'of', 'Asians', 'that', 'say', 'yes', 'are', 'at', '21', 'no', 'at', '76', 'I', 'am', 'part', 'of', 'the', '76', 'and', 'I', \"'m\", 'conflicted', 'I', 'know', 'especially', 'for', 'the', 'underserved', 'AA', 'makes', 'a', 'significant', 'impact', 'in', 'giving', 'folks', 'better', 'chances', 'at', 'life', 'which', 'in', 'turn', 'translates', 'to', 'diversity', 'in', 'every', 'facet', 'of', 'work', 'society', 'life', 'in', 'general', 'which', 'I', 'view', 'is', 'a', 'good', 'thing', 'But', 'specifically', 'regarding', 'college', 'admissions', 'say', 'for', 'my', 'own', 'kids', 'not', 'college', 'aged', 'yet', 'I', 'would', 'like', 'to', 'see', 'more', 'data', 'on', 'whether', 'year', '2000', 'and', 'beyond', 'AA', 'in', 'college', 'admissions', 'was', 'harmful', 'to', 'Asians', 'in', 'general', 'In', 'my', 'own', 'experience', 'anecdotal', 'totally', 'not', 'data', 'science', 'driven', 'I', 'feel', 'like', 'AA', 'in', 'college', 'admissions', 'has', 'hurt', 'friends', 'and', 'family', 'in', 'a', 'reverse', 'sort', 'of', 'sense', 'But', 'for', 'the', 'sake', 'of', 'the', 'underserved', 'I', 'did', \"n't\", 'want', 'AA', 'to', 'go', 'away', 'So', 'I', 'am', 'deeply', 'conflicted', 'Your', 'thoughts']\n"
     ]
    }
   ],
   "source": [
    "print(comments_df['tokens'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from above that the tokenization got worse when we added the token_match criteria. We need to modify the English class-attribute before loading model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_comment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-9b7f7fe41d96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#nlp.tokenizer.token_match = None (run line if results are same)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdoc1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_comment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sample_comment' is not defined"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "#nlp.tokenizer.token_match = None (run line if results are same)\n",
    "doc1 = nlp(sample_comment)\n",
    "print([w.text for w in doc1])\n",
    "\n",
    "# add token_match to tokenizer\n",
    "nlp.tokenizer.token_match = re.compile(r'u/{1}\\w{3,23}').match\n",
    "doc2 = nlp(sample_comment)\n",
    "print([w.text for w in doc2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason, the reddit username includes the comma.\n",
    "*IDEA: We can strip the string of any characters that are not valid in a username.*\n",
    "\n",
    "Our tokenization of reddit usernames is not perfect, it may include trailing punctuation. But let's move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 11), match='u/Tungsten_'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 11), match='u/Tungsten_'>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#redditor_regex = re.compile(r'u/{1}\\w{3,23}')\n",
    "#print(redditor_regex.search(sample_comment))\n",
    "#redditor_regex.match(sample_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "      <td>[Thanks, to, everyone, who, engaged, in, insig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "      <td>[I, would, prefer, using, a, process, that, ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "      <td>[u, Tungsten, Thanks, for, creating, a, sectio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "      <td>[As, with, anything, related, to, Asians, in, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pancake_muncher</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yet colleges will allow alumni and doners in e...</td>\n",
       "      <td>[Yet, colleges, will, allow, alumni, and, done...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          username        flair_text  \\\n",
       "0        Tungsten_               NaN   \n",
       "1   ProudBlackMatt  Chinese-American   \n",
       "2     TomatoCanned               NaN   \n",
       "3     bad-fengshui               NaN   \n",
       "4  Pancake_muncher               NaN   \n",
       "\n",
       "                                                body  \\\n",
       "0  Thanks to everyone who engaged in insightful a...   \n",
       "1  I would prefer using a process that takes into...   \n",
       "2  u/Tungsten_, Thanks for creating a section jus...   \n",
       "3  As with anything related to Asians in politics...   \n",
       "4  Yet colleges will allow alumni and doners in e...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [Thanks, to, everyone, who, engaged, in, insig...  \n",
       "1  [I, would, prefer, using, a, process, that, ta...  \n",
       "2  [u, Tungsten, Thanks, for, creating, a, sectio...  \n",
       "3  [As, with, anything, related, to, Asians, in, ...  \n",
       "4  [Yet, colleges, will, allow, alumni, and, done...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.tokenizer.token_match = re.compile(r'u/{1}\\w{3,23}').match # match reddit usernames\n",
    "\n",
    "# drop old tokens column from data frame\n",
    "comments_df.drop(['tokens'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column to data frame\n",
    "comments_df['tokens_new'] = comments_df['body'].apply(lambda x: tokenize_str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>tokens_new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "      <td>[Thanks, to, everyone, who, engaged, in, insig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "      <td>[I, would, prefer, using, a, process, that, ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "      <td>[u/Tungsten_,, Thanks, for, creating, a, secti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "      <td>[As, with, anything, related, to, Asians, in, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pancake_muncher</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yet colleges will allow alumni and doners in e...</td>\n",
       "      <td>[Yet, colleges, will, allow, alumni, and, done...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          username        flair_text  \\\n",
       "0        Tungsten_               NaN   \n",
       "1   ProudBlackMatt  Chinese-American   \n",
       "2     TomatoCanned               NaN   \n",
       "3     bad-fengshui               NaN   \n",
       "4  Pancake_muncher               NaN   \n",
       "\n",
       "                                                body  \\\n",
       "0  Thanks to everyone who engaged in insightful a...   \n",
       "1  I would prefer using a process that takes into...   \n",
       "2  u/Tungsten_, Thanks for creating a section jus...   \n",
       "3  As with anything related to Asians in politics...   \n",
       "4  Yet colleges will allow alumni and doners in e...   \n",
       "\n",
       "                                          tokens_new  \n",
       "0  [Thanks, to, everyone, who, engaged, in, insig...  \n",
       "1  [I, would, prefer, using, a, process, that, ta...  \n",
       "2  [u/Tungsten_,, Thanks, for, creating, a, secti...  \n",
       "3  [As, with, anything, related, to, Asians, in, ...  \n",
       "4  [Yet, colleges, will, allow, alumni, and, done...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and normalize text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make all words lowercase (trivial)\n",
    "2. Drop non-word tokens (may already be done in tokenize_str() function)\n",
    "3. Remove stop-words (in a sophisticated manner)\n",
    "4. Stem words to remove suffixes, prefixes, infixes OR Lemmatize tokens (intelligently)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Remove stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 11177),\n",
       " ('to', 7709),\n",
       " ('of', 6362),\n",
       " ('and', 6105),\n",
       " ('a', 5681),\n",
       " ('that', 5142),\n",
       " ('in', 4681),\n",
       " ('i', 4373),\n",
       " ('is', 4354),\n",
       " ('it', 3855),\n",
       " ('you', 2845),\n",
       " ('are', 2787),\n",
       " ('for', 2679),\n",
       " ('not', 2189),\n",
       " (\"'s\", 2185),\n",
       " ('asian', 2062),\n",
       " (\"n't\", 2018),\n",
       " ('as', 1991),\n",
       " ('this', 1988),\n",
       " ('but', 1881),\n",
       " ('be', 1822),\n",
       " ('they', 1805),\n",
       " ('do', 1798),\n",
       " ('on', 1749),\n",
       " ('have', 1703),\n",
       " ('action', 1527),\n",
       " ('with', 1507),\n",
       " ('affirmative', 1474),\n",
       " ('we', 1417),\n",
       " ('asians', 1386),\n",
       " ('people', 1363),\n",
       " ('or', 1353),\n",
       " ('if', 1310),\n",
       " ('at', 1282),\n",
       " ('from', 1216),\n",
       " ('about', 1145),\n",
       " ('who', 1088),\n",
       " ('more', 1065),\n",
       " ('was', 1045),\n",
       " ('white', 1045),\n",
       " ('their', 990),\n",
       " ('there', 984),\n",
       " ('so', 953),\n",
       " ('other', 937),\n",
       " ('like', 914),\n",
       " ('because', 913),\n",
       " ('just', 908),\n",
       " ('an', 902),\n",
       " ('what', 881),\n",
       " ('all', 879)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove stop words using word counts\n",
    "counts_dict = {}\n",
    "for word in comments_df['tokens_new'].sum():\n",
    "    word = word.lower()\n",
    "    if word in counts_dict:\n",
    "        counts_dict[word]+=1\n",
    "    else:\n",
    "        counts_dict[word] = 1\n",
    "\n",
    "word_counts = sorted(counts_dict.items(), key = lambda x: x[1], reverse=True)\n",
    "word_counts[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 11177),\n",
       " ('to', 7709),\n",
       " ('of', 6362),\n",
       " ('and', 6105),\n",
       " ('a', 5681),\n",
       " ('that', 5142),\n",
       " ('in', 4681),\n",
       " ('i', 4373),\n",
       " ('is', 4354),\n",
       " ('it', 3855),\n",
       " ('you', 2845),\n",
       " ('are', 2787),\n",
       " ('for', 2679),\n",
       " ('not', 2189),\n",
       " (\"'s\", 2185),\n",
       " ('asian', 2062),\n",
       " (\"n't\", 2018),\n",
       " ('as', 1991),\n",
       " ('this', 1988),\n",
       " ('but', 1881),\n",
       " ('be', 1822),\n",
       " ('they', 1805),\n",
       " ('do', 1798),\n",
       " ('on', 1749),\n",
       " ('have', 1703),\n",
       " ('action', 1527),\n",
       " ('with', 1507),\n",
       " ('affirmative', 1474),\n",
       " ('we', 1417),\n",
       " ('asians', 1386),\n",
       " ('people', 1363),\n",
       " ('or', 1353),\n",
       " ('if', 1310),\n",
       " ('at', 1282),\n",
       " ('from', 1216)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts[:35]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mark/remove words as stop words that are more frequent than the first noun ('i')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'to', 'of', 'and', 'a', 'that', 'in']\n"
     ]
    }
   ],
   "source": [
    "stop_words_freq = []\n",
    "for word, count in word_counts:\n",
    "    if word == 'i':\n",
    "        break\n",
    "    else:\n",
    "        stop_words_freq.append(word)\n",
    "\n",
    "print(stop_words_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging from the list, we can add 'is' and 'it' to list of stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'to', 'of', 'and', 'a', 'that', 'in', 'is', 'it']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stop_words_freq += ['is', 'it']\n",
    "#stop_words_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to normalize tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tokens(word_list, extra_stop_words=[]):\n",
    "    normalized = []\n",
    "    if type(word_list) == list and len(word_list) == 1:\n",
    "        word_list = word_list[0]\n",
    "\n",
    "    if type(word_list) == list:\n",
    "        word_list = ' '.join([str(elem) for elem in word_list])\n",
    "\n",
    "    doc = nlp(word_list.lower()) # lowercase words in word_list\n",
    "\n",
    "    # add lexeme property of stopword to words considered as stopwords\n",
    "    if len(extra_stop_words) > 0:\n",
    "        for stopword in extra_stop_words:\n",
    "            lexeme = nlp.vocab[stopword]\n",
    "            lexeme.is_stop = True\n",
    "\n",
    "    for w in doc:\n",
    "        # if not stop word or punctuation, add it to normalized\n",
    "        if w.text != '\\n' and not w.is_stop and not w.is_punct and not w.like_num and len(w.text.strip()) > 0:\n",
    "            # add lemmatized version of word\n",
    "            normalized.append(str(w.lemma_))\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>tokens_new</th>\n",
       "      <th>normalized_tokens</th>\n",
       "      <th>normalized_tokens_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "      <td>[Thanks, to, everyone, who, engaged, in, insig...</td>\n",
       "      <td>[thank, engage, insightful, respectful, discou...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "      <td>[I, would, prefer, using, a, process, that, ta...</td>\n",
       "      <td>[prefer, process, take, account, poverty, inst...</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "      <td>[u/Tungsten_,, Thanks, for, creating, a, secti...</td>\n",
       "      <td>[u/tungsten_,, thank, create, section, discuss...</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "      <td>[As, with, anything, related, to, Asians, in, ...</td>\n",
       "      <td>[relate, asians, politic, m, see, lot, non, as...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pancake_muncher</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yet colleges will allow alumni and doners in e...</td>\n",
       "      <td>[Yet, colleges, will, allow, alumni, and, done...</td>\n",
       "      <td>[college, allow, alumnus, doner, easily, consi...</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>suberry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I just hated Affirmative Action as a distracti...</td>\n",
       "      <td>[I, just, hated, Affirmative, Action, as, a, d...</td>\n",
       "      <td>[hate, affirmative, action, distraction, banda...</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Puzzled-Painter3301</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My own feeling is that I was never in love wit...</td>\n",
       "      <td>[My, own, feeling, is, that, I, was, never, in...</td>\n",
       "      <td>[feeling, love, affirmative, action, possible,...</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>e9967780</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anti Asian racism whether against East Asians ...</td>\n",
       "      <td>[Anti, Asian, racism, whether, against, East, ...</td>\n",
       "      <td>[anti, asian, racism, east, asians, south, asi...</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Can we overturn legacy and athlete admissions ...</td>\n",
       "      <td>[Can, we, overturn, legacy, and, athlete, admi...</td>\n",
       "      <td>[overturn, legacy, athlete, admission, point, ...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OkartoIceCream</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I want to remind people that in California, on...</td>\n",
       "      <td>[I, want, to, remind, people, that, in, Califo...</td>\n",
       "      <td>[want, remind, people, california, progressive...</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ShalomHasaeyo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I think this is great news for our nation. Whi...</td>\n",
       "      <td>[I, think, this, is, great, news, for, our, na...</td>\n",
       "      <td>[think, great, news, nation, abolish, legacy, ...</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SteadfastEnd</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My main concern is that, since the Court did n...</td>\n",
       "      <td>[My, main, concern, is, that, since, the, Cour...</td>\n",
       "      <td>[main, concern, court, strike, grutter, outrig...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MsNewKicks</td>\n",
       "      <td>First Of Her Name, Queen ABG, 나쁜 기집애, Blocker ...</td>\n",
       "      <td>There is obviously a lot to it and no real eas...</td>\n",
       "      <td>[There, is, obviously, a, lot, to, it, and, no...</td>\n",
       "      <td>[obviously, lot, real, easy, answer, satisfy, ...</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pal2002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I’m sure selective schools like Harvard will f...</td>\n",
       "      <td>[I, ’m, sure, selective, schools, like, Harvar...</td>\n",
       "      <td>[m, sure, selective, school, like, harvard, fi...</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Ok-Value5827</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TBH, I don't know the depth of how affirmative...</td>\n",
       "      <td>[TBH, I, do, n't, know, the, depth, of, how, a...</td>\n",
       "      <td>[tbh, know, depth, affirmative, action, colleg...</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>j3ychen</td>\n",
       "      <td>Taiwanese</td>\n",
       "      <td>People can continue to debate the legal implic...</td>\n",
       "      <td>[People, can, continue, to, debate, the, legal...</td>\n",
       "      <td>[people, continue, debate, legal, implication,...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>HappyPineapple11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The only issue with affirmative action was tha...</td>\n",
       "      <td>[The, only, issue, with, affirmative, action, ...</td>\n",
       "      <td>[issue, affirmative, action, explicit, quota, ...</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>alandizzle</td>\n",
       "      <td>I'm Asian. Hi.</td>\n",
       "      <td>You know what I hate?\\n\\nThe main subreddits w...</td>\n",
       "      <td>[You, know, what, I, hate, The, main, subreddi...</td>\n",
       "      <td>[know, hate, main, subreddit, certain, white, ...</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gamesrgreat</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I’m not a fan of affirmative action even tho i...</td>\n",
       "      <td>[I, ’m, not, a, fan, of, affirmative, action, ...</td>\n",
       "      <td>[m, fan, affirmative, action, tho, help, minor...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>jademing4</td>\n",
       "      <td>Deaf Asian</td>\n",
       "      <td>If we really want to be fair, let's strike dow...</td>\n",
       "      <td>[If, we, really, want, to, be, fair, let, 's, ...</td>\n",
       "      <td>[want, fair, let, strike, legacy, admission, a...</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               username                                         flair_text  \\\n",
       "0             Tungsten_                                                NaN   \n",
       "1        ProudBlackMatt                                   Chinese-American   \n",
       "2          TomatoCanned                                                NaN   \n",
       "3          bad-fengshui                                                NaN   \n",
       "4       Pancake_muncher                                                NaN   \n",
       "5               suberry                                                NaN   \n",
       "6   Puzzled-Painter3301                                                NaN   \n",
       "7              e9967780                                                NaN   \n",
       "8                   NaN                                                NaN   \n",
       "9        OkartoIceCream                                                NaN   \n",
       "10        ShalomHasaeyo                                                NaN   \n",
       "11         SteadfastEnd                                                NaN   \n",
       "12           MsNewKicks  First Of Her Name, Queen ABG, 나쁜 기집애, Blocker ...   \n",
       "14              pal2002                                                NaN   \n",
       "15         Ok-Value5827                                                NaN   \n",
       "16              j3ychen                                          Taiwanese   \n",
       "17     HappyPineapple11                                                NaN   \n",
       "18           alandizzle                                     I'm Asian. Hi.   \n",
       "19          gamesrgreat                                                NaN   \n",
       "20            jademing4                                         Deaf Asian   \n",
       "\n",
       "                                                 body  \\\n",
       "0   Thanks to everyone who engaged in insightful a...   \n",
       "1   I would prefer using a process that takes into...   \n",
       "2   u/Tungsten_, Thanks for creating a section jus...   \n",
       "3   As with anything related to Asians in politics...   \n",
       "4   Yet colleges will allow alumni and doners in e...   \n",
       "5   I just hated Affirmative Action as a distracti...   \n",
       "6   My own feeling is that I was never in love wit...   \n",
       "7   Anti Asian racism whether against East Asians ...   \n",
       "8   Can we overturn legacy and athlete admissions ...   \n",
       "9   I want to remind people that in California, on...   \n",
       "10  I think this is great news for our nation. Whi...   \n",
       "11  My main concern is that, since the Court did n...   \n",
       "12  There is obviously a lot to it and no real eas...   \n",
       "14  I’m sure selective schools like Harvard will f...   \n",
       "15  TBH, I don't know the depth of how affirmative...   \n",
       "16  People can continue to debate the legal implic...   \n",
       "17  The only issue with affirmative action was tha...   \n",
       "18  You know what I hate?\\n\\nThe main subreddits w...   \n",
       "19  I’m not a fan of affirmative action even tho i...   \n",
       "20  If we really want to be fair, let's strike dow...   \n",
       "\n",
       "                                           tokens_new  \\\n",
       "0   [Thanks, to, everyone, who, engaged, in, insig...   \n",
       "1   [I, would, prefer, using, a, process, that, ta...   \n",
       "2   [u/Tungsten_,, Thanks, for, creating, a, secti...   \n",
       "3   [As, with, anything, related, to, Asians, in, ...   \n",
       "4   [Yet, colleges, will, allow, alumni, and, done...   \n",
       "5   [I, just, hated, Affirmative, Action, as, a, d...   \n",
       "6   [My, own, feeling, is, that, I, was, never, in...   \n",
       "7   [Anti, Asian, racism, whether, against, East, ...   \n",
       "8   [Can, we, overturn, legacy, and, athlete, admi...   \n",
       "9   [I, want, to, remind, people, that, in, Califo...   \n",
       "10  [I, think, this, is, great, news, for, our, na...   \n",
       "11  [My, main, concern, is, that, since, the, Cour...   \n",
       "12  [There, is, obviously, a, lot, to, it, and, no...   \n",
       "14  [I, ’m, sure, selective, schools, like, Harvar...   \n",
       "15  [TBH, I, do, n't, know, the, depth, of, how, a...   \n",
       "16  [People, can, continue, to, debate, the, legal...   \n",
       "17  [The, only, issue, with, affirmative, action, ...   \n",
       "18  [You, know, what, I, hate, The, main, subreddi...   \n",
       "19  [I, ’m, not, a, fan, of, affirmative, action, ...   \n",
       "20  [If, we, really, want, to, be, fair, let, 's, ...   \n",
       "\n",
       "                                    normalized_tokens  normalized_tokens_count  \n",
       "0   [thank, engage, insightful, respectful, discou...                        9  \n",
       "1   [prefer, process, take, account, poverty, inst...                       52  \n",
       "2   [u/tungsten_,, thank, create, section, discuss...                      126  \n",
       "3   [relate, asians, politic, m, see, lot, non, as...                       25  \n",
       "4   [college, allow, alumnus, doner, easily, consi...                       19  \n",
       "5   [hate, affirmative, action, distraction, banda...                       78  \n",
       "6   [feeling, love, affirmative, action, possible,...                      102  \n",
       "7   [anti, asian, racism, east, asians, south, asi...                       21  \n",
       "8   [overturn, legacy, athlete, admission, point, ...                       15  \n",
       "9   [want, remind, people, california, progressive...                      104  \n",
       "10  [think, great, news, nation, abolish, legacy, ...                       58  \n",
       "11  [main, concern, court, strike, grutter, outrig...                       72  \n",
       "12  [obviously, lot, real, easy, answer, satisfy, ...                       46  \n",
       "14  [m, sure, selective, school, like, harvard, fi...                       20  \n",
       "15  [tbh, know, depth, affirmative, action, colleg...                       39  \n",
       "16  [people, continue, debate, legal, implication,...                       42  \n",
       "17  [issue, affirmative, action, explicit, quota, ...                       84  \n",
       "18  [know, hate, main, subreddit, certain, white, ...                      223  \n",
       "19  [m, fan, affirmative, action, tho, help, minor...                       40  \n",
       "20  [want, fair, let, strike, legacy, admission, a...                       38  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df['normalized_tokens'] = comments_df['tokens_new'].apply(lambda x: normalize_tokens(x, stop_words_freq))\n",
    "comments_df['normalized_tokens_count'] = comments_df['normalized_tokens'].apply(lambda x: len(x))\n",
    "\n",
    "comments_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add word count column\n",
    "comments_df['word_count'] = comments_df['tokens_new'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>tokens_new</th>\n",
       "      <th>normalized_tokens</th>\n",
       "      <th>normalized_tokens_count</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "      <td>[Thanks, to, everyone, who, engaged, in, insig...</td>\n",
       "      <td>[thank, engage, insightful, respectful, discou...</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "      <td>[I, would, prefer, using, a, process, that, ta...</td>\n",
       "      <td>[prefer, process, take, account, poverty, inst...</td>\n",
       "      <td>52</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "      <td>[u/Tungsten_,, Thanks, for, creating, a, secti...</td>\n",
       "      <td>[u/tungsten_,, thank, create, section, discuss...</td>\n",
       "      <td>126</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "      <td>[As, with, anything, related, to, Asians, in, ...</td>\n",
       "      <td>[relate, asians, politic, m, see, lot, non, as...</td>\n",
       "      <td>25</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pancake_muncher</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yet colleges will allow alumni and doners in e...</td>\n",
       "      <td>[Yet, colleges, will, allow, alumni, and, done...</td>\n",
       "      <td>[college, allow, alumnus, doner, easily, consi...</td>\n",
       "      <td>19</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3618</th>\n",
       "      <td>aduogetsatastegouda</td>\n",
       "      <td>NaN</td>\n",
       "      <td>But that's irrelevant. The right not to be dis...</td>\n",
       "      <td>[But, that, 's, irrelevant, The, right, not, t...</td>\n",
       "      <td>[irrelevant, right, discriminate, base, race, ...</td>\n",
       "      <td>38</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3619</th>\n",
       "      <td>rentonwong</td>\n",
       "      <td>Support Asian-American Media!</td>\n",
       "      <td>Despite my dislike of AA, at least 2/3rds of A...</td>\n",
       "      <td>[Despite, my, dislike, of, AA, at, least, 2/3r...</td>\n",
       "      <td>[despite, dislike, aa, 2/3rds, asian, american...</td>\n",
       "      <td>19</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3620</th>\n",
       "      <td>rentonwong</td>\n",
       "      <td>Support Asian-American Media!</td>\n",
       "      <td>&gt; If 1/3 of a racial minority's members say th...</td>\n",
       "      <td>[&gt;, If, 1/3, of, a, racial, minority, 's, memb...</td>\n",
       "      <td>[&gt;, racial, minority, member, want, discrimina...</td>\n",
       "      <td>27</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3621</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm just annoyed at how there's so much handwa...</td>\n",
       "      <td>[I, 'm, just, annoyed, at, how, there, 's, so,...</td>\n",
       "      <td>[m, annoyed, handwaving, consequence, pro, aa,...</td>\n",
       "      <td>48</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3622</th>\n",
       "      <td>rentonwong</td>\n",
       "      <td>Support Asian-American Media!</td>\n",
       "      <td>The current system as it stands preserves whil...</td>\n",
       "      <td>[The, current, system, as, it, stands, preserv...</td>\n",
       "      <td>[current, system, stand, preserve, privilege, ...</td>\n",
       "      <td>49</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3317 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 username                     flair_text  \\\n",
       "0               Tungsten_                            NaN   \n",
       "1          ProudBlackMatt               Chinese-American   \n",
       "2            TomatoCanned                            NaN   \n",
       "3            bad-fengshui                            NaN   \n",
       "4         Pancake_muncher                            NaN   \n",
       "...                   ...                            ...   \n",
       "3618  aduogetsatastegouda                            NaN   \n",
       "3619           rentonwong  Support Asian-American Media!   \n",
       "3620           rentonwong  Support Asian-American Media!   \n",
       "3621                  NaN                            NaN   \n",
       "3622           rentonwong  Support Asian-American Media!   \n",
       "\n",
       "                                                   body  \\\n",
       "0     Thanks to everyone who engaged in insightful a...   \n",
       "1     I would prefer using a process that takes into...   \n",
       "2     u/Tungsten_, Thanks for creating a section jus...   \n",
       "3     As with anything related to Asians in politics...   \n",
       "4     Yet colleges will allow alumni and doners in e...   \n",
       "...                                                 ...   \n",
       "3618  But that's irrelevant. The right not to be dis...   \n",
       "3619  Despite my dislike of AA, at least 2/3rds of A...   \n",
       "3620  > If 1/3 of a racial minority's members say th...   \n",
       "3621  I'm just annoyed at how there's so much handwa...   \n",
       "3622  The current system as it stands preserves whil...   \n",
       "\n",
       "                                             tokens_new  \\\n",
       "0     [Thanks, to, everyone, who, engaged, in, insig...   \n",
       "1     [I, would, prefer, using, a, process, that, ta...   \n",
       "2     [u/Tungsten_,, Thanks, for, creating, a, secti...   \n",
       "3     [As, with, anything, related, to, Asians, in, ...   \n",
       "4     [Yet, colleges, will, allow, alumni, and, done...   \n",
       "...                                                 ...   \n",
       "3618  [But, that, 's, irrelevant, The, right, not, t...   \n",
       "3619  [Despite, my, dislike, of, AA, at, least, 2/3r...   \n",
       "3620  [>, If, 1/3, of, a, racial, minority, 's, memb...   \n",
       "3621  [I, 'm, just, annoyed, at, how, there, 's, so,...   \n",
       "3622  [The, current, system, as, it, stands, preserv...   \n",
       "\n",
       "                                      normalized_tokens  \\\n",
       "0     [thank, engage, insightful, respectful, discou...   \n",
       "1     [prefer, process, take, account, poverty, inst...   \n",
       "2     [u/tungsten_,, thank, create, section, discuss...   \n",
       "3     [relate, asians, politic, m, see, lot, non, as...   \n",
       "4     [college, allow, alumnus, doner, easily, consi...   \n",
       "...                                                 ...   \n",
       "3618  [irrelevant, right, discriminate, base, race, ...   \n",
       "3619  [despite, dislike, aa, 2/3rds, asian, american...   \n",
       "3620  [>, racial, minority, member, want, discrimina...   \n",
       "3621  [m, annoyed, handwaving, consequence, pro, aa,...   \n",
       "3622  [current, system, stand, preserve, privilege, ...   \n",
       "\n",
       "      normalized_tokens_count  word_count  \n",
       "0                           9          20  \n",
       "1                          52         103  \n",
       "2                         126         269  \n",
       "3                          25          59  \n",
       "4                          19          40  \n",
       "...                       ...         ...  \n",
       "3618                       38          84  \n",
       "3619                       19          32  \n",
       "3620                       27          61  \n",
       "3621                       48         117  \n",
       "3622                       49         102  \n",
       "\n",
       "[3317 rows x 7 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# check if normalized_tokens column is list or string - Result: list\n",
    "print(type(comments_df['normalized_tokens'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save above df as csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df.to_csv('../data/comments_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comments_processed.csv\n"
     ]
    }
   ],
   "source": [
    "input_file = 'comments.csv'\n",
    "output_file = input_file[0:-4] + '_processed' + input_file[-4:]\n",
    "\n",
    "print(output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_fq_dist = nltk.ConditionalFreqDist(((len(w), w) for w in comments_df['normalized_tokens'].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130955\n"
     ]
    }
   ],
   "source": [
    "print(comments_fq_dist.N()) # number of total words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('asian', 2442),\n",
       " ('white', 1321),\n",
       " ('think', 956),\n",
       " ('black', 787),\n",
       " ('group', 573)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_fq_dist[5].most_common(5) #most common 5-letter words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d9defa72c2715dab9f7f172572cd30a1ab1a2083462d32ef96aadb7c6e0c73b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
