{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Flair Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "\n",
    "### Flair data\n",
    "1. DONE - Q: Does r/asianamerican have flair templates? A: No, it is only customizable.\n",
    "2. Decipher which ethnicity (Chinese, Japanese, Korean, Vietnamese, other) using flair text\n",
    "- How should we deal with multi-ethnic flairs (Chinese-Thai, Korean/Black, etc)\n",
    "- Looks like most of the comments with flair text are from a small subset of users with flairs who have commented numerous times\n",
    "3. DONE - Flairs can contain up to 10 emojis, so can we use emojis to decipher ethnicity? A: Don't think we need emoji data, doesn't seem to be used very much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flair data EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = pd.read_csv('../data/top_100_post_comments_user_flair.txt', header=None, names=['username', 'flair_text', 'body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3623, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pancake_muncher</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yet colleges will allow alumni and doners in e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>suberry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I just hated Affirmative Action as a distracti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Puzzled-Painter3301</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My own feeling is that I was never in love wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>e9967780</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anti Asian racism whether against East Asians ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Can we overturn legacy and athlete admissions ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OkartoIceCream</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I want to remind people that in California, on...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              username        flair_text  \\\n",
       "0            Tungsten_               NaN   \n",
       "1       ProudBlackMatt  Chinese-American   \n",
       "2         TomatoCanned               NaN   \n",
       "3         bad-fengshui               NaN   \n",
       "4      Pancake_muncher               NaN   \n",
       "5              suberry               NaN   \n",
       "6  Puzzled-Painter3301               NaN   \n",
       "7             e9967780               NaN   \n",
       "8                  NaN               NaN   \n",
       "9       OkartoIceCream               NaN   \n",
       "\n",
       "                                                body  \n",
       "0  Thanks to everyone who engaged in insightful a...  \n",
       "1  I would prefer using a process that takes into...  \n",
       "2  u/Tungsten_, Thanks for creating a section jus...  \n",
       "3  As with anything related to Asians in politics...  \n",
       "4  Yet colleges will allow alumni and doners in e...  \n",
       "5  I just hated Affirmative Action as a distracti...  \n",
       "6  My own feeling is that I was never in love wit...  \n",
       "7  Anti Asian racism whether against East Asians ...  \n",
       "8  Can we overturn legacy and athlete admissions ...  \n",
       "9  I want to remind people that in California, on...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(comments_df.shape)\n",
    "comments_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How many comments have flair text? A: Of 3623 rows, 3085 do NOT have flairs, 538 do have flairs\n",
    "- Seems like we could use more data... but I'm not sure if there is more to collect\n",
    "2. How many comments are by Chinese/Chinese-Americans flaired users?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "username       833\n",
      "flair_text    3085\n",
      "body             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(comments_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use find() to search the array of flair texts -- make the flair texts lowercase first\n",
    "# substrings to find:\n",
    "# Chinese: 'china', 'chines', 'abc'\n",
    "# Korean: 'korea', 'kor', 'abk', 'gyopo'\n",
    "# Japanese: 'jap', 'abj'\n",
    "# Filipino: 'filip', \"philppi\", 'pinoy', 'abf', 'abp'\n",
    "# Indian: 'indian', 'abi'\n",
    "# South Asian: 'desi', 'south asia'\n",
    "\n",
    "# Series of flair_text\n",
    "flair_text = comments_df['flair_text']\n",
    "\n",
    "# get rid of nan\n",
    "flair_text_nona = flair_text.fillna(0)\n",
    "flair_text_clean = flair_text_nona.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chinese flairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty matrix to hold indices of substring\n",
    "chine_matrix = np.empty((flair_text_clean.shape[0],3))\n",
    "\n",
    "# each column is for a different type of identifying substring\n",
    "chine_matrix[:,0] = flair_text_clean.str.find('china')\n",
    "chine_matrix[:,1] = flair_text_clean.str.find('chines')\n",
    "chine_matrix[:,2] = flair_text_clean.str.find('abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan nan nan]\n",
      " [-1.  0. -1.]\n",
      " [nan nan nan]\n",
      " ...\n",
      " [-1. -1. -1.]\n",
      " [nan nan nan]\n",
      " [-1. -1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "print(chine_matrix)\n",
    "# row of nan is comment w/o flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1. -1. -1.]\n",
      " [-1.  0. -1.]\n",
      " [-1. -1. -1.]\n",
      " ...\n",
      " [-1. -1. -1.]\n",
      " [-1. -1. -1.]\n",
      " [-1. -1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "# change nan to -1 (no substring found)\n",
    "chine_matrix_clean = np.nan_to_num(chine_matrix, nan=-1)\n",
    "print(chine_matrix_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False]\n",
      " [False  True False]\n",
      " [False False False]\n",
      " ...\n",
      " [False False False]\n",
      " [False False False]\n",
      " [False False False]]\n"
     ]
    }
   ],
   "source": [
    "# identify rows with one of the keywords (has value other than -1)\n",
    "print(chine_matrix_clean != -1)\n",
    "chine_rows = (chine_matrix_clean != -1).any(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3623,)\n",
      "97\n"
     ]
    }
   ],
   "source": [
    "print(chine_rows.shape)\n",
    "print(chine_rows.sum()) #97 comments of 3623 have Chinese flair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of unique users with Chinese flair: 16\n"
     ]
    }
   ],
   "source": [
    "chi_comments_df = comments_df[chine_rows]\n",
    "num_unique_users = len(pd.unique(chi_comments_df['username']))\n",
    "\n",
    "print(f'Num of unique users with Chinese flair: {num_unique_users}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chinese flair summary:\n",
    "- 97 comments with Chinese flair\n",
    "- 16 unique users with Chinese flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Korean flairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Korean substrings: 'kor', 'abk', 'gyopo', 'hanguk'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty matrix to hold korean substring indices\n",
    "kor_matrix = np.empty((flair_text_clean.shape[0],4))\n",
    "\n",
    "kor_matrix[:,0] = flair_text_clean.str.find('kor')\n",
    "kor_matrix[:,1] = flair_text_clean.str.find('abk')\n",
    "kor_matrix[:,2] = flair_text_clean.str.find('gyopo')\n",
    "kor_matrix[:,3] = flair_text_clean.str.find('hanguk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " ...\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "# change nan to -1 (no flair to no substring found)\n",
    "kor_matrix_clean = np.nan_to_num(kor_matrix, nan=-1)\n",
    "print(kor_matrix_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False False]\n",
      " [False False False False]\n",
      " [False False False False]\n",
      " ...\n",
      " [False False False False]\n",
      " [False False False False]\n",
      " [False False False False]]\n",
      "[False False False ... False False False]\n"
     ]
    }
   ],
   "source": [
    "# identify rows with one of the keywords (has value other than -1)\n",
    "print(kor_matrix_clean != -1)\n",
    "kor_rows = (kor_matrix_clean != -1).any(axis=1)\n",
    "print(kor_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3623,)\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(kor_rows.shape)\n",
    "print(kor_rows.sum()) # 12 comments of 3623 have Korean flair\n",
    "\n",
    "# get indexes of Korean flair comments\n",
    "#kor_idx = np.where(kor_rows==1)[0]\n",
    "kor_comments_df = comments_df[kor_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of unique users with Korean flair: 3\n"
     ]
    }
   ],
   "source": [
    "kor_comments_df = comments_df[kor_rows]\n",
    "num_unique_kor_users = len(pd.unique(kor_comments_df['username']))\n",
    "print(f'Num of unique users with Korean flair: {num_unique_kor_users}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Korean flairs summary:\n",
    "- 12 comments with Korean flair\n",
    "- 3 unique users with Korean flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d9defa72c2715dab9f7f172572cd30a1ab1a2083462d32ef96aadb7c6e0c73b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
