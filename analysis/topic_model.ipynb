{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Flair Analysis\n",
    "\n",
    "(no longer valid due to lack of data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "\n",
    "### Flair data\n",
    "1. DONE - Q: Does r/asianamerican have flair templates? A: No, it is only customizable.\n",
    "2. Decipher which ethnicity (Chinese, Japanese, Korean, Vietnamese, other) using flair text\n",
    "- How should we deal with multi-ethnic flairs (Chinese-Thai, Korean/Black, etc)\n",
    "- Looks like most of the comments with flair text are from a small subset of users with flairs who have commented numerous times\n",
    "3. DONE - Flairs can contain up to 10 emojis, so can we use emojis to decipher ethnicity? A: Don't think we need emoji data, doesn't seem to be used very much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flair data EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = pd.read_csv('../data/top_100_post_comments_user_flair.txt', header=None, names=['username', 'flair_text', 'body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3623, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pancake_muncher</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yet colleges will allow alumni and doners in e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>suberry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I just hated Affirmative Action as a distracti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Puzzled-Painter3301</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My own feeling is that I was never in love wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>e9967780</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anti Asian racism whether against East Asians ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Can we overturn legacy and athlete admissions ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OkartoIceCream</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I want to remind people that in California, on...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              username        flair_text  \\\n",
       "0            Tungsten_               NaN   \n",
       "1       ProudBlackMatt  Chinese-American   \n",
       "2         TomatoCanned               NaN   \n",
       "3         bad-fengshui               NaN   \n",
       "4      Pancake_muncher               NaN   \n",
       "5              suberry               NaN   \n",
       "6  Puzzled-Painter3301               NaN   \n",
       "7             e9967780               NaN   \n",
       "8                  NaN               NaN   \n",
       "9       OkartoIceCream               NaN   \n",
       "\n",
       "                                                body  \n",
       "0  Thanks to everyone who engaged in insightful a...  \n",
       "1  I would prefer using a process that takes into...  \n",
       "2  u/Tungsten_, Thanks for creating a section jus...  \n",
       "3  As with anything related to Asians in politics...  \n",
       "4  Yet colleges will allow alumni and doners in e...  \n",
       "5  I just hated Affirmative Action as a distracti...  \n",
       "6  My own feeling is that I was never in love wit...  \n",
       "7  Anti Asian racism whether against East Asians ...  \n",
       "8  Can we overturn legacy and athlete admissions ...  \n",
       "9  I want to remind people that in California, on...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(comments_df.shape)\n",
    "comments_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How many comments have flair text? A: Of 3623 rows, 3085 do NOT have flairs, 538 do have flairs\n",
    "- Seems like we could use more data... but I'm not sure if there is more to collect\n",
    "2. How many comments are by Chinese/Chinese-Americans flaired users?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "username       833\n",
      "flair_text    3085\n",
      "body             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(comments_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use find() to search the array of flair texts -- make the flair texts lowercase first\n",
    "# substrings to find:\n",
    "# Chinese: 'china', 'chines', 'abc'\n",
    "# Korean: 'korea', 'kor', 'abk', 'gyopo'\n",
    "# Japanese: 'jap', 'abj'\n",
    "# Filipino: 'filip', \"philppi\", 'pinoy', 'abf', 'abp'\n",
    "# Indian: 'indian', 'abi'\n",
    "# South Asian: 'desi', 'south asia'\n",
    "\n",
    "# Series of flair_text\n",
    "flair_text = comments_df['flair_text']\n",
    "\n",
    "# get rid of nan\n",
    "flair_text_nona = flair_text.fillna(0)\n",
    "flair_text_clean = flair_text_nona.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chinese flairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty matrix to hold indices of substring\n",
    "chine_matrix = np.empty((flair_text_clean.shape[0],3))\n",
    "\n",
    "# each column is for a different type of identifying substring\n",
    "chine_matrix[:,0] = flair_text_clean.str.find('china')\n",
    "chine_matrix[:,1] = flair_text_clean.str.find('chines')\n",
    "chine_matrix[:,2] = flair_text_clean.str.find('abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan nan nan]\n",
      " [-1.  0. -1.]\n",
      " [nan nan nan]\n",
      " ...\n",
      " [-1. -1. -1.]\n",
      " [nan nan nan]\n",
      " [-1. -1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "print(chine_matrix)\n",
    "# row of nan is comment w/o flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1. -1. -1.]\n",
      " [-1.  0. -1.]\n",
      " [-1. -1. -1.]\n",
      " ...\n",
      " [-1. -1. -1.]\n",
      " [-1. -1. -1.]\n",
      " [-1. -1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "# change nan to -1 (no substring found)\n",
    "chine_matrix_clean = np.nan_to_num(chine_matrix, nan=-1)\n",
    "print(chine_matrix_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False]\n",
      " [False  True False]\n",
      " [False False False]\n",
      " ...\n",
      " [False False False]\n",
      " [False False False]\n",
      " [False False False]]\n"
     ]
    }
   ],
   "source": [
    "# identify rows with one of the keywords (has value other than -1)\n",
    "print(chine_matrix_clean != -1)\n",
    "chine_rows = (chine_matrix_clean != -1).any(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3623,)\n",
      "97\n"
     ]
    }
   ],
   "source": [
    "print(chine_rows.shape)\n",
    "print(chine_rows.sum()) #97 comments of 3623 have Chinese flair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of unique users with Chinese flair: 16\n"
     ]
    }
   ],
   "source": [
    "chi_comments_df = comments_df[chine_rows]\n",
    "num_unique_users = len(pd.unique(chi_comments_df['username']))\n",
    "\n",
    "print(f'Num of unique users with Chinese flair: {num_unique_users}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chinese flair summary:\n",
    "- 97 comments with Chinese flair\n",
    "- 16 unique users with Chinese flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Korean flairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Korean substrings: 'kor', 'abk', 'gyopo', 'hanguk'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty matrix to hold korean substring indices\n",
    "kor_matrix = np.empty((flair_text_clean.shape[0],4))\n",
    "\n",
    "kor_matrix[:,0] = flair_text_clean.str.find('kor')\n",
    "kor_matrix[:,1] = flair_text_clean.str.find('abk')\n",
    "kor_matrix[:,2] = flair_text_clean.str.find('gyopo')\n",
    "kor_matrix[:,3] = flair_text_clean.str.find('hanguk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " ...\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "# change nan to -1 (no flair to no substring found)\n",
    "kor_matrix_clean = np.nan_to_num(kor_matrix, nan=-1)\n",
    "print(kor_matrix_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False False]\n",
      " [False False False False]\n",
      " [False False False False]\n",
      " ...\n",
      " [False False False False]\n",
      " [False False False False]\n",
      " [False False False False]]\n",
      "[False False False ... False False False]\n"
     ]
    }
   ],
   "source": [
    "# identify rows with one of the keywords (has value other than -1)\n",
    "print(kor_matrix_clean != -1)\n",
    "kor_rows = (kor_matrix_clean != -1).any(axis=1)\n",
    "print(kor_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3623,)\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "print(kor_rows.shape)\n",
    "print(kor_rows.sum()) # 12 comments of 3623 have Korean flair\n",
    "\n",
    "# get indexes of Korean flair comments\n",
    "#kor_idx = np.where(kor_rows==1)[0]\n",
    "kor_comments_df = comments_df[kor_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of unique users with Korean flair: 3\n"
     ]
    }
   ],
   "source": [
    "kor_comments_df = comments_df[kor_rows]\n",
    "num_unique_kor_users = len(pd.unique(kor_comments_df['username']))\n",
    "print(f'Num of unique users with Korean flair: {num_unique_kor_users}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Korean flairs summary:\n",
    "- 12 comments with Korean flair\n",
    "- 3 unique users with Korean flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# cluster detection\n",
    "import sklearn\n",
    "import sklearn.feature_extraction.text\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "import sklearn.datasets\n",
    "import sklearn.cluster\n",
    "import sklearn.decomposition\n",
    "import sklearn.metrics\n",
    "\n",
    "import nltk # for co-locations\n",
    "import scipy #For hierarchical clustering and some visuals\n",
    "#import scipy.cluster.hierarchy\n",
    "import gensim#For topic modeling\n",
    "import requests #For downloading our datasets\n",
    "import matplotlib.pyplot as plt #For graphics\n",
    "import matplotlib.cm #Still for graphics\n",
    "import seaborn as sns #Makes the graphics look nicer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = pd.read_csv('../data/comments_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: normalized_tokens column is string type, not list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "normalized_tokens = comments_df['normalized_tokens'][0]\n",
    "print(type(normalized_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "def converter(x):\n",
    "    return literal_eval(x)\n",
    "\n",
    "comments_df = pd.read_csv('../data/comments_df.csv', converters={'tokens_new':converter, 'normalized_tokens':converter})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "normalized_tokens = comments_df['normalized_tokens'][0]\n",
    "tokens_new = comments_df['tokens_new'][0]\n",
    "print(type(normalized_tokens))\n",
    "print(type(tokens_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token columns are now lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>username</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>tokens_new</th>\n",
       "      <th>normalized_tokens</th>\n",
       "      <th>normalized_tokens_count</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "      <td>[Thanks, to, everyone, who, engaged, in, insig...</td>\n",
       "      <td>[thank, engage, insightful, respectful, discou...</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "      <td>[I, would, prefer, using, a, process, that, ta...</td>\n",
       "      <td>[prefer, process, take, account, poverty, inst...</td>\n",
       "      <td>52</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "      <td>[u/Tungsten_,, Thanks, for, creating, a, secti...</td>\n",
       "      <td>[u/tungsten_,, thank, create, section, discuss...</td>\n",
       "      <td>126</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "      <td>[As, with, anything, related, to, Asians, in, ...</td>\n",
       "      <td>[relate, asians, politic, m, see, lot, non, as...</td>\n",
       "      <td>25</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Pancake_muncher</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yet colleges will allow alumni and doners in e...</td>\n",
       "      <td>[Yet, colleges, will, allow, alumni, and, done...</td>\n",
       "      <td>[college, allow, alumnus, doner, easily, consi...</td>\n",
       "      <td>19</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>suberry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I just hated Affirmative Action as a distracti...</td>\n",
       "      <td>[I, just, hated, Affirmative, Action, as, a, d...</td>\n",
       "      <td>[hate, affirmative, action, distraction, banda...</td>\n",
       "      <td>78</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Puzzled-Painter3301</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My own feeling is that I was never in love wit...</td>\n",
       "      <td>[My, own, feeling, is, that, I, was, never, in...</td>\n",
       "      <td>[feeling, love, affirmative, action, possible,...</td>\n",
       "      <td>102</td>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>e9967780</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Anti Asian racism whether against East Asians ...</td>\n",
       "      <td>[Anti, Asian, racism, whether, against, East, ...</td>\n",
       "      <td>[anti, asian, racism, east, asians, south, asi...</td>\n",
       "      <td>21</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Can we overturn legacy and athlete admissions ...</td>\n",
       "      <td>[Can, we, overturn, legacy, and, athlete, admi...</td>\n",
       "      <td>[overturn, legacy, athlete, admission, point, ...</td>\n",
       "      <td>15</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>OkartoIceCream</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I want to remind people that in California, on...</td>\n",
       "      <td>[I, want, to, remind, people, that, in, Califo...</td>\n",
       "      <td>[want, remind, people, california, progressive...</td>\n",
       "      <td>104</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0             username        flair_text  \\\n",
       "0           0            Tungsten_               NaN   \n",
       "1           1       ProudBlackMatt  Chinese-American   \n",
       "2           2         TomatoCanned               NaN   \n",
       "3           3         bad-fengshui               NaN   \n",
       "4           4      Pancake_muncher               NaN   \n",
       "5           5              suberry               NaN   \n",
       "6           6  Puzzled-Painter3301               NaN   \n",
       "7           7             e9967780               NaN   \n",
       "8           8                  NaN               NaN   \n",
       "9           9       OkartoIceCream               NaN   \n",
       "\n",
       "                                                body  \\\n",
       "0  Thanks to everyone who engaged in insightful a...   \n",
       "1  I would prefer using a process that takes into...   \n",
       "2  u/Tungsten_, Thanks for creating a section jus...   \n",
       "3  As with anything related to Asians in politics...   \n",
       "4  Yet colleges will allow alumni and doners in e...   \n",
       "5  I just hated Affirmative Action as a distracti...   \n",
       "6  My own feeling is that I was never in love wit...   \n",
       "7  Anti Asian racism whether against East Asians ...   \n",
       "8  Can we overturn legacy and athlete admissions ...   \n",
       "9  I want to remind people that in California, on...   \n",
       "\n",
       "                                          tokens_new  \\\n",
       "0  [Thanks, to, everyone, who, engaged, in, insig...   \n",
       "1  [I, would, prefer, using, a, process, that, ta...   \n",
       "2  [u/Tungsten_,, Thanks, for, creating, a, secti...   \n",
       "3  [As, with, anything, related, to, Asians, in, ...   \n",
       "4  [Yet, colleges, will, allow, alumni, and, done...   \n",
       "5  [I, just, hated, Affirmative, Action, as, a, d...   \n",
       "6  [My, own, feeling, is, that, I, was, never, in...   \n",
       "7  [Anti, Asian, racism, whether, against, East, ...   \n",
       "8  [Can, we, overturn, legacy, and, athlete, admi...   \n",
       "9  [I, want, to, remind, people, that, in, Califo...   \n",
       "\n",
       "                                   normalized_tokens  normalized_tokens_count  \\\n",
       "0  [thank, engage, insightful, respectful, discou...                        9   \n",
       "1  [prefer, process, take, account, poverty, inst...                       52   \n",
       "2  [u/tungsten_,, thank, create, section, discuss...                      126   \n",
       "3  [relate, asians, politic, m, see, lot, non, as...                       25   \n",
       "4  [college, allow, alumnus, doner, easily, consi...                       19   \n",
       "5  [hate, affirmative, action, distraction, banda...                       78   \n",
       "6  [feeling, love, affirmative, action, possible,...                      102   \n",
       "7  [anti, asian, racism, east, asians, south, asi...                       21   \n",
       "8  [overturn, legacy, athlete, admission, point, ...                       15   \n",
       "9  [want, remind, people, california, progressive...                      104   \n",
       "\n",
       "   word_count  \n",
       "0          20  \n",
       "1         103  \n",
       "2         269  \n",
       "3          59  \n",
       "4          40  \n",
       "5         171  \n",
       "6         231  \n",
       "7          46  \n",
       "8          29  \n",
       "9         200  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text has already been tokenized, lemmatized, normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>username</th>\n",
       "      <th>flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>tokens_new</th>\n",
       "      <th>normalized_tokens</th>\n",
       "      <th>normalized_tokens_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>normalized_tokens_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3618</th>\n",
       "      <td>3618</td>\n",
       "      <td>aduogetsatastegouda</td>\n",
       "      <td>NaN</td>\n",
       "      <td>But that's irrelevant. The right not to be dis...</td>\n",
       "      <td>[But, that, 's, irrelevant, The, right, not, t...</td>\n",
       "      <td>[irrelevant, right, discriminate, base, race, ...</td>\n",
       "      <td>38</td>\n",
       "      <td>84</td>\n",
       "      <td>irrelevant right discriminate base race subjec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3619</th>\n",
       "      <td>3619</td>\n",
       "      <td>rentonwong</td>\n",
       "      <td>Support Asian-American Media!</td>\n",
       "      <td>Despite my dislike of AA, at least 2/3rds of A...</td>\n",
       "      <td>[Despite, my, dislike, of, AA, at, least, 2/3r...</td>\n",
       "      <td>[despite, dislike, aa, 2/3rds, asian, american...</td>\n",
       "      <td>19</td>\n",
       "      <td>32</td>\n",
       "      <td>despite dislike aa 2/3rds asian americans base...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3620</th>\n",
       "      <td>3620</td>\n",
       "      <td>rentonwong</td>\n",
       "      <td>Support Asian-American Media!</td>\n",
       "      <td>&gt; If 1/3 of a racial minority's members say th...</td>\n",
       "      <td>[&gt;, If, 1/3, of, a, racial, minority, 's, memb...</td>\n",
       "      <td>[&gt;, racial, minority, member, want, discrimina...</td>\n",
       "      <td>27</td>\n",
       "      <td>61</td>\n",
       "      <td>&gt; racial minority member want discriminate ove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3621</th>\n",
       "      <td>3621</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm just annoyed at how there's so much handwa...</td>\n",
       "      <td>[I, 'm, just, annoyed, at, how, there, 's, so,...</td>\n",
       "      <td>[m, annoyed, handwaving, consequence, pro, aa,...</td>\n",
       "      <td>48</td>\n",
       "      <td>117</td>\n",
       "      <td>m annoyed handwaving consequence pro aa anti a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3622</th>\n",
       "      <td>3622</td>\n",
       "      <td>rentonwong</td>\n",
       "      <td>Support Asian-American Media!</td>\n",
       "      <td>The current system as it stands preserves whil...</td>\n",
       "      <td>[The, current, system, as, it, stands, preserv...</td>\n",
       "      <td>[current, system, stand, preserve, privilege, ...</td>\n",
       "      <td>49</td>\n",
       "      <td>102</td>\n",
       "      <td>current system stand preserve privilege play m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0             username                     flair_text  \\\n",
       "3618        3618  aduogetsatastegouda                            NaN   \n",
       "3619        3619           rentonwong  Support Asian-American Media!   \n",
       "3620        3620           rentonwong  Support Asian-American Media!   \n",
       "3621        3621                  NaN                            NaN   \n",
       "3622        3622           rentonwong  Support Asian-American Media!   \n",
       "\n",
       "                                                   body  \\\n",
       "3618  But that's irrelevant. The right not to be dis...   \n",
       "3619  Despite my dislike of AA, at least 2/3rds of A...   \n",
       "3620  > If 1/3 of a racial minority's members say th...   \n",
       "3621  I'm just annoyed at how there's so much handwa...   \n",
       "3622  The current system as it stands preserves whil...   \n",
       "\n",
       "                                             tokens_new  \\\n",
       "3618  [But, that, 's, irrelevant, The, right, not, t...   \n",
       "3619  [Despite, my, dislike, of, AA, at, least, 2/3r...   \n",
       "3620  [>, If, 1/3, of, a, racial, minority, 's, memb...   \n",
       "3621  [I, 'm, just, annoyed, at, how, there, 's, so,...   \n",
       "3622  [The, current, system, as, it, stands, preserv...   \n",
       "\n",
       "                                      normalized_tokens  \\\n",
       "3618  [irrelevant, right, discriminate, base, race, ...   \n",
       "3619  [despite, dislike, aa, 2/3rds, asian, american...   \n",
       "3620  [>, racial, minority, member, want, discrimina...   \n",
       "3621  [m, annoyed, handwaving, consequence, pro, aa,...   \n",
       "3622  [current, system, stand, preserve, privilege, ...   \n",
       "\n",
       "      normalized_tokens_count  word_count  \\\n",
       "3618                       38          84   \n",
       "3619                       19          32   \n",
       "3620                       27          61   \n",
       "3621                       48         117   \n",
       "3622                       49         102   \n",
       "\n",
       "                                  normalized_tokens_str  \n",
       "3618  irrelevant right discriminate base race subjec...  \n",
       "3619  despite dislike aa 2/3rds asian americans base...  \n",
       "3620  > racial minority member want discriminate ove...  \n",
       "3621  m annoyed handwaving consequence pro aa anti a...  \n",
       "3622  current system stand preserve privilege play m...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 130955 bigrams in the finder.\n"
     ]
    }
   ],
   "source": [
    "bigrams = nltk.collocations.BigramCollocationFinder.from_words(comments_df['normalized_tokens'].sum())\n",
    "print(f'There are {bigrams.N} bigrams in the finder.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('affirmative', 'action'),\n",
       " ('asian', 'americans'),\n",
       " ('asian', 'american'),\n",
       " ('white', 'people'),\n",
       " ('high', 'school'),\n",
       " ('college', 'admission'),\n",
       " ('race', 'base'),\n",
       " ('asian', 'student'),\n",
       " ('legacy', 'admission'),\n",
       " ('test', 'score'),\n",
       " ('ivy', 'league'),\n",
       " ('white', 'student'),\n",
       " ('high', 'education'),\n",
       " ('black', 'hispanic'),\n",
       " ('support', 'affirmative'),\n",
       " ('black', 'people'),\n",
       " ('student', 'body'),\n",
       " ('asian', 'applicant'),\n",
       " ('model', 'minority'),\n",
       " ('black', 'latino'),\n",
       " ('chinese', 'americans'),\n",
       " ('middle', 'class'),\n",
       " ('supreme', 'court'),\n",
       " ('black', 'student'),\n",
       " ('asian', 'kid'),\n",
       " ('sit', 'score'),\n",
       " ('feel', 'like'),\n",
       " ('african', 'american'),\n",
       " ('admission', 'officer'),\n",
       " ('east', 'asians'),\n",
       " ('m', 'sure'),\n",
       " ('admission', 'process'),\n",
       " ('asian', 'people'),\n",
       " ('minority', 'group'),\n",
       " ('holistic', 'admission'),\n",
       " ('white', 'supremacy'),\n",
       " ('african', 'americans'),\n",
       " ('base', 'affirmative'),\n",
       " ('personality', 'score'),\n",
       " ('american', 'student'),\n",
       " ('elite', 'school'),\n",
       " ('low', 'income'),\n",
       " ('united', 'states'),\n",
       " ('white', 'applicant'),\n",
       " ('american', 'applicant'),\n",
       " ('anti', 'asian'),\n",
       " ('racial', 'discrimination'),\n",
       " ('admission', 'office'),\n",
       " ('base', 'aa'),\n",
       " ('base', 'race')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bigramScoring(count, wordsTuple, total):\n",
    "    return count\n",
    "bigrams.nbest(bigramScoring, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "whBigrams.score_ngrams(bigram_measures.likelihood_ratio)[:40]\n",
    "# other options include student_t, chi_sq, likelihood_ratio, pmi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = sklearn.feature_extraction.text.CountVectorizer()\n",
    "count_vector = count_vectorizer.fit_transform(comments_df['normalized_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3623, 10425)\n"
     ]
    }
   ],
   "source": [
    "print(count_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3623 rows, 10425 columns/unique tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. TD-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdidf_transformer = sklearn.feature_extraction.text.TfidfTransformer()\n",
    "tdidf_vector = tdidf_transformer.fit_transform(count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('thank', 0.2790647239686926),\n",
       " ('engage', 0.2624916204470818),\n",
       " ('insightful', 0.4308677651786394),\n",
       " ('respectful', 0.2588701492057808),\n",
       " ('discourse', 0.3724050122335431),\n",
       " ('news', 0.4155587375062185),\n",
       " ('thread', 0.3355191219247845),\n",
       " ('lock', 0.3528365045862098),\n",
       " ('comment', 0.2282958742756067),\n",
       " ('prefer', 0.08961425241998294),\n",
       " ('process', 0.16812507837038776),\n",
       " ('take', 0.10333828523481878),\n",
       " ('account', 0.09791761829024669),\n",
       " ('poverty', 0.12307005687760202),\n",
       " ('instead', 0.11641353787807514),\n",
       " ('generation', 0.11641353787807514),\n",
       " ('family', 0.10922874743933599),\n",
       " ('come', 0.14501625345093178),\n",
       " ('america', 0.13812266943966214),\n",
       " ('painfully', 0.11936149821893847)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(count_vectorizer.vocabulary_.keys(), tdidf_vector.data))[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Prune Matrix of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize\n",
    "prune_vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(min_df=3, max_features=1000, stop_words='english', norm='l2') # why norm=l2?\n",
    "#train\n",
    "pruned_vec = prune_vectorizer.fit_transform(comments_df['normalized_tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- min document freq=3 because low document frequency inflates td-idf\n",
    "- An idea: visualize document freq of each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3623x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 73451 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, matrix is only 1000 terms/columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"vector\" is missing\n",
      "The available words are: ['thank', 'news', 'thread', 'comment', 'prefer', 'process', 'account', 'poverty', 'instead', 'generation'] ...\n"
     ]
    }
   ],
   "source": [
    "# try to find term in matrix\n",
    "termtofind = 'vector'\n",
    "try:\n",
    "    print(prune_vectorizer.vocabulary_[termtofind])\n",
    "except KeyError:\n",
    "    print(f'\"{termtofind}\" is missing')\n",
    "    print('The available words are: {} ...'.format(list(prune_vectorizer.vocabulary_.keys())[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d9defa72c2715dab9f7f172572cd30a1ab1a2083462d32ef96aadb7c6e0c73b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
