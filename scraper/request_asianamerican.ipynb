{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Progress Report:\n",
    "- DONE: Feb 14: can output a file written with comments but error in delimiters, every character delimited by ','\n",
    "    Also, Python error about being unable to transcribe unicode for a certain string about 3 secs into loop in get_comments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import datetime\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_reddit(): returns .json object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reddit(subreddit,listing,limit,timeframe):\n",
    "    try:\n",
    "        base_url = f'https://www.reddit.com/r/{subreddit}/{listing}.json?limit={limit}&t={timeframe}'\n",
    "        request = requests.get(base_url, headers = {'User-agent': 'yourbot'})\n",
    "    except:\n",
    "        print('An Error Occured')\n",
    "    return request.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for request\n",
    "\n",
    "subreddit = 'asianamerican'\n",
    "limit = 100\n",
    "timeframe = 'all' #hour, day, week, month, year, all\n",
    "listing = 'top' # controversial, best, hot, new, random, rising, top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = get_reddit(subreddit, listing, limit, timeframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_titles(r):\n",
    "    '''\n",
    "    Get a List of post titles\n",
    "    '''\n",
    "    posts = []\n",
    "    for post in r['data']['children']:\n",
    "        x = post['data']['title']\n",
    "        posts.append(x)\n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = get_post_titles(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NBA All-Star Damian Lillard wears \"\"Stop Asian Hate\" shirt while sitting out tonight\\'s game', '“Fuck you! We will stop the Hate!” NBA star Baron Davis stands against anti-Asian racism.', 'Accurate', \"My friend's mother was one of the victims of Anti-Asian shooting that recently occurred. The gofundme already lays out their situation but she was a single mother working hard to support her kids through school and the pandemic. Any contributions would mean the world\", 'Megan Thee Stallion donates $50K to AAPI legal advocacy group after Atlanta spa shootings']\n"
     ]
    }
   ],
   "source": [
    "print(titles[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-88943241ff1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# print keys/elements of a Reddit object (in this case, submission (t3))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mlist_red_objs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'children'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# important keys: (1) id (Reddit thing ID), (2) title, (3) ups (upvotes),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# (4) downs (downvotes), (5) score (not sure), (6) likes (not sure), (7) num_comments)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'search' is not defined"
     ]
    }
   ],
   "source": [
    "# print keys/elements of a Reddit object (in this case, submission (t3))\n",
    "list_red_objs = list(search['data']['children'][0]['data'].keys())\n",
    "\n",
    "# important keys: (1) id (Reddit thing ID), (2) title, (3) ups (upvotes), \n",
    "# (4) downs (downvotes), (5) score (not sure), (6) likes (not sure), (7) num_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'r' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-3ce96acc31a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#print(r['data']['children'])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdict_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'children'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mdict_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'r' is not defined"
     ]
    }
   ],
   "source": [
    "# What categories do I need from each post?\n",
    "## Comments\n",
    "## \n",
    "\n",
    "#print(r['data']['children'])\n",
    "dict_ = pd.DataFrame.from_dict(r['data']['children'][0])\n",
    "dict_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_search()\n",
    "- gets the .json file of a Reddit search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrected request url\n",
    "#https://www.reddit.com/r/asianamerican/search/?q=affirmative%20action&restrict_sr=1&sort=top\n",
    "\n",
    "def get_search(subreddit,query,limit,timeframe,sort,restrict_sr):\n",
    "    try:\n",
    "        base_url = f'https://www.reddit.com/r/{subreddit}/search.json?q={query}&restrict_sr={restrict_sr}&limit={limit}&t={timeframe}&sort={sort}'\n",
    "        request = requests.get(base_url, headers = {'User-agent': 'yourbot'})\n",
    "    except:\n",
    "        print('An Error Occured')\n",
    "    return request.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get top 10 post (submission) IDs from Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_submission_info(r, n):\n",
    "    '''\n",
    "    Get a List of post titles\n",
    "    r: .json object\n",
    "    n: num of top posts\n",
    "    # important keys: (1) id (Reddit thing ID), (2) title, (3) ups (upvotes), \n",
    "    # (4) downs (downvotes), (5) score (not sure), (6) likes (not sure), (7) num_comments)\n",
    "\n",
    "    Returns: pandas Dataframe object\n",
    "    '''\n",
    "    # create 3 empty arrays of length n\n",
    "    id_arr = np.empty(n, dtype=\"S10\")\n",
    "    up_arr = np.empty(n)\n",
    "    down_arr = np.empty(n)\n",
    "    score_arr = np.empty(n)\n",
    "    likes_arr = np.empty(n)\n",
    "    num_com_arr = np.empty(n)\n",
    "    \n",
    "    for i, post in enumerate(r['data']['children']):\n",
    "        id_arr[i] = post['data']['id']\n",
    "        up_arr[i] = post['data']['ups']\n",
    "        down_arr[i] = post['data']['downs']\n",
    "        score_arr[i] = post['data']['score']\n",
    "        likes_arr[i] = post['data']['likes']\n",
    "        num_com_arr[i] = post['data']['num_comments']\n",
    "\n",
    "    # create Dataframe\n",
    "    df = pd.DataFrame({'id': id_arr, 'upvotes': up_arr, 'downvotes': down_arr, 'score': score_arr, 'likes': likes_arr, 'num_comments': num_com_arr})\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Driver file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_search() params\n",
    "sr = 'asianamerican'\n",
    "q = 'affirmative action'\n",
    "limit = 100\n",
    "timeframe = 'all'\n",
    "sort = 'top'\n",
    "restrict_sr = 1\n",
    "\n",
    "# variable for array of titles\n",
    "titles = None\n",
    "\n",
    "search = get_search(subreddit=sr,query=q, limit=limit, timeframe=timeframe, sort=sort, restrict_sr=restrict_sr)\n",
    "df = get_top_n_submission_info(search, limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>downvotes</th>\n",
       "      <th>score</th>\n",
       "      <th>likes</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'14m8mf4'</td>\n",
       "      <td>241.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>357.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'hqic5b'</td>\n",
       "      <td>223.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>223.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'9nk0do'</td>\n",
       "      <td>212.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'sejv5m'</td>\n",
       "      <td>152.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'h9tuc2'</td>\n",
       "      <td>142.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b'sbrca3'</td>\n",
       "      <td>130.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>b'cwk0vc'</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>134.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b'8ajkwc'</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>b'8usex7'</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>b'rjy8ut'</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>b'7481yg'</td>\n",
       "      <td>101.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>116.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>b'5btr2j'</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>b'87ru7q'</td>\n",
       "      <td>84.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>b'pvb846'</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>b'jny9p4'</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>139.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>b'3oymmf'</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>b'6rolbg'</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>b'8uonyt'</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>b'sz1qyo'</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>b'9meo0b'</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>b'8vt7kt'</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>b'8rlr3y'</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>b'j7ybkv'</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>b'9mzffm'</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>b'jm4x1b'</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>b'b9rf6b'</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>b'ti11r7'</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>b'3w7268'</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>b'6ruvgx'</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>b'9wwgwt'</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>b'9ozm49'</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>b'967i7r'</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>b'6t02ts'</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>b'6r0tss'</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>b'9z3ssz'</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>b'5cudq1'</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>b'sbvysg'</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>b'6rjw6f'</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>b'3dmbpz'</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>b'62q795'</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>b'9s7sz1'</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>b'3e4d2v'</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>b'36jedc'</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>b'4kq3uy'</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>b'5i6idf'</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>b'dezvi6'</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>b'9kgf7i'</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>b'21a4zz'</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>b'9oq200'</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>b'23wexw'</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  upvotes  downvotes  score  likes  num_comments\n",
       "0   b'14m8mf4'    241.0        0.0  241.0    NaN         357.0\n",
       "1    b'hqic5b'    223.0        0.0  223.0    NaN         114.0\n",
       "2    b'9nk0do'    212.0        0.0  212.0    NaN          44.0\n",
       "3    b'sejv5m'    152.0        0.0  152.0    NaN          85.0\n",
       "4    b'h9tuc2'    142.0        0.0  142.0    NaN         108.0\n",
       "5    b'sbrca3'    130.0        0.0  130.0    NaN         224.0\n",
       "6    b'cwk0vc'    133.0        0.0  133.0    NaN         134.0\n",
       "7    b'8ajkwc'    122.0        0.0  122.0    NaN          32.0\n",
       "8    b'8usex7'    120.0        0.0  120.0    NaN          42.0\n",
       "9    b'rjy8ut'    108.0        0.0  108.0    NaN          39.0\n",
       "10   b'7481yg'    101.0        0.0  101.0    NaN         116.0\n",
       "11   b'5btr2j'     92.0        0.0   92.0    NaN          23.0\n",
       "12   b'87ru7q'     84.0        0.0   84.0    NaN          81.0\n",
       "13   b'pvb846'     79.0        0.0   79.0    NaN          30.0\n",
       "14   b'jny9p4'     71.0        0.0   71.0    NaN         139.0\n",
       "15   b'3oymmf'     64.0        0.0   64.0    NaN          65.0\n",
       "16   b'6rolbg'     59.0        0.0   59.0    NaN          42.0\n",
       "17   b'8uonyt'     52.0        0.0   52.0    NaN          36.0\n",
       "18   b'sz1qyo'     50.0        0.0   50.0    NaN          19.0\n",
       "19   b'9meo0b'     46.0        0.0   46.0    NaN          65.0\n",
       "20   b'8vt7kt'     44.0        0.0   44.0    NaN          30.0\n",
       "21   b'8rlr3y'     41.0        0.0   41.0    NaN          42.0\n",
       "22   b'j7ybkv'     41.0        0.0   41.0    NaN           3.0\n",
       "23   b'9mzffm'     38.0        0.0   38.0    NaN          75.0\n",
       "24   b'jm4x1b'     33.0        0.0   33.0    NaN          27.0\n",
       "25   b'b9rf6b'     33.0        0.0   33.0    NaN           6.0\n",
       "26   b'ti11r7'     34.0        0.0   34.0    NaN          66.0\n",
       "27   b'3w7268'     30.0        0.0   30.0    NaN           6.0\n",
       "28   b'6ruvgx'     30.0        0.0   30.0    NaN          51.0\n",
       "29   b'9wwgwt'     25.0        0.0   25.0    NaN           9.0\n",
       "30   b'9ozm49'     28.0        0.0   28.0    NaN         219.0\n",
       "31   b'967i7r'     28.0        0.0   28.0    NaN          58.0\n",
       "32   b'6t02ts'     29.0        0.0   29.0    NaN           4.0\n",
       "33   b'6r0tss'     27.0        0.0   27.0    NaN          59.0\n",
       "34   b'9z3ssz'     29.0        0.0   29.0    NaN          24.0\n",
       "35   b'5cudq1'     29.0        0.0   29.0    NaN           6.0\n",
       "36   b'sbvysg'     26.0        0.0   26.0    NaN           4.0\n",
       "37   b'6rjw6f'     27.0        0.0   27.0    NaN          11.0\n",
       "38   b'3dmbpz'     27.0        0.0   27.0    NaN           8.0\n",
       "39   b'62q795'     25.0        0.0   25.0    NaN           1.0\n",
       "40   b'9s7sz1'     23.0        0.0   23.0    NaN          14.0\n",
       "41   b'3e4d2v'     23.0        0.0   23.0    NaN           2.0\n",
       "42   b'36jedc'     24.0        0.0   24.0    NaN          60.0\n",
       "43   b'4kq3uy'     22.0        0.0   22.0    NaN           9.0\n",
       "44   b'5i6idf'     22.0        0.0   22.0    NaN          58.0\n",
       "45   b'dezvi6'     23.0        0.0   23.0    NaN          25.0\n",
       "46   b'9kgf7i'     21.0        0.0   21.0    NaN          40.0\n",
       "47   b'21a4zz'     16.0        0.0   16.0    NaN          38.0\n",
       "48   b'9oq200'     17.0        0.0   17.0    NaN           0.0\n",
       "49   b'23wexw'     17.0        0.0   17.0    NaN          28.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['14m8mf4' '9nk0do' 'sejv5m' 'h9tuc2' 'sbrca3' 'cwk0vc' '8ajkwc' '8usex7'\n",
      " 'rjy8ut' '7481yg' '5btr2j' '87ru7q' 'pvb846' 'jny9p4' '3oymmf' '6rolbg'\n",
      " '8uonyt' 'sz1qyo' '9meo0b' '8vt7kt' '8rlr3y' 'j7ybkv' '9mzffm' 'jm4x1b'\n",
      " 'b9rf6b' '3w7268' '6ruvgx' '9wwgwt' '9ozm49' '967i7r' '9z3ssz' '6t02ts'\n",
      " '6r0tss' '5cudq1' '3dmbpz' '6rjw6f' 'sbvysg' '62q795' '9s7sz1' '36jedc'\n",
      " '3e4d2v' '4kq3uy' '5i6idf' '9kgf7i' '21a4zz' '9oq200' '6riosv' '3euiwo'\n",
      " '23wexw' '31whah' '20ewl2' '9rd4tx' '23ouhl' 'anfd61' 'dskb09' '6ra6ey'\n",
      " 'jka8ii' '105fx43' '9oovpa' '2n01k4' '1v63z3' '8a1j3p' 'h107c4' '9ap203'\n",
      " '2rdduz' '1juzmv' '6sqhhh' '3bkavj' '3rgzi3' '12e8uno' 'sujd6e' 'drhgp6'\n",
      " '43eu4h' 'mauapi' 'tuqh5o' '88rs2g' '266qsv' '14v43k' '3ija6r' '4eb7qj'\n",
      " '2c8iu1' '3rkhqd' '4lqzqq' '4q20ib' '24hmx8' 'esbwk7' 'd6767e' '6ek03l'\n",
      " '5cw7uu' '789gct' '1belws6' 'bgj9bw' '4ziaf0']\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "# get column of t3 object submission IDs\n",
    "id_arr = df_no_chinese['id'].to_numpy()\n",
    "\n",
    "# decode strings from byte \n",
    "def decode_arr(x):\n",
    "    return x.decode('utf-8')\n",
    "\n",
    "vfunc = np.vectorize(decode_arr)\n",
    "id_arr_decoded = vfunc(id_arr)\n",
    "\n",
    "print(id_arr_decoded)\n",
    "print(len(id_arr_decoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use id_arr to get comments of these submission id's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Don't use below code - use PRAW instead\n",
    "def get_comments(sr, post_id, context, limit, showedits, showmedia, showmore, showtitle, theme, threaded, truncate, sort):\n",
    "    try:\n",
    "        base_url = f'https://www.reddit.com/r/{sr}/comments/{post_id}?context={context}&limit={limit}&showedits={showedits}&showmore={showmore}&showmedia={showmedia}&showtitle={showtitle}&sort={sort}&theme={theme}&threaded={threaded}&truncate={truncate}'\n",
    "        request = requests.get(base_url, headers = {'User_agent': 'yourbot'})\n",
    "        return request.json()\n",
    "    except:\n",
    "        print('An Error Occured')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "sr = 'asianamerican'\n",
    "context = 5 # int: 0-8\n",
    "limit = 100\n",
    "showedits = False\n",
    "showmore = False\n",
    "showmedia = False\n",
    "showtitle = False\n",
    "sort = 'confidence' # one of (confidence, top, new, controversial, old, random, qa, live)\n",
    "theme = 'default'\n",
    "threaded = False\n",
    "truncate = 25\n",
    "post_id = 'sejv5m'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An Error Occured\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''comments = get_comments(sr=sr,\n",
    "                        post_id=post_id,\n",
    "                        context=context,\n",
    "                        limit=limit,\n",
    "                        showedits=showedits,\n",
    "                        showmore=showmore,\n",
    "                        showtitle=showtitle,\n",
    "                        showmedia=showmedia,\n",
    "                        sort=sort,\n",
    "                        theme=theme,\n",
    "                        threaded=threaded,\n",
    "                        truncate=truncate)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using PRAW Instead of Reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "\n",
    "# need to take secret and password off of Github in the future\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=\"iaKUGuo77BwJYHElyAo1Jg\",\n",
    "    client_secret=\"19bSH4KJpPJB2WFeYhUe--jk4IKeHQ\",\n",
    "    password='Ehdwn1928K!',\n",
    "    user_agent=\"Comment Extraction u/dlehdwn\",\n",
    "    username='dlehdwn',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dlehdwn\n"
     ]
    }
   ],
   "source": [
    "print(reddit.user.me())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#enable read-only mode\n",
    "#reddit.read_only = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(praw_obj, id_arr, filename):\n",
    "    '''\n",
    "    praw_obj: PRAW Reddit object\n",
    "    id_arr: array of Reddit t3 IDs\n",
    "    '''\n",
    "    #open a file to write to\n",
    "    with open(filename, 'w', newline='', encoding=\"utf-8\") as f:\n",
    "\n",
    "        # writer\n",
    "        writer = csv.writer(f, delimiter=',', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "        # for each submission/post\n",
    "        for id in id_arr:\n",
    "            sub = praw_obj.submission(id)\n",
    "            \n",
    "            # get post data\n",
    "            time = datetime.datetime.fromtimestamp(sub.created)\n",
    "            post_text = sub.title + ' ' + sub.selftext\n",
    "            parent_id = None\n",
    "\n",
    "            # submission author can be None\n",
    "            if sub.author is not None:\n",
    "                username = sub.author.name\n",
    "                flair = sub.author_flair_text\n",
    "                writer.writerow([id, parent_id, username, time, flair, post_text])\n",
    "\n",
    "            else:\n",
    "                #print('comment.author is None')\n",
    "                #print(f'body: {body}\\n')\n",
    "                writer.writerow([id, parent_id, None, time, None, post_text])\n",
    "\n",
    "            # get all comments\n",
    "            sub.comments.replace_more(limit=None, threshold=1) #threshold - min num of comments needed to traverse into \"More comments\" object\n",
    "            for comment in sub.comments.list():\n",
    "\n",
    "                # get comment features\n",
    "                comment_time = datetime.datetime.fromtimestamp(comment.created)\n",
    "                comment_id = comment.id\n",
    "                comment_parent_id = comment.parent_id\n",
    "                body = comment.body\n",
    "\n",
    "                # get username of comment author\n",
    "                if comment.author is not None:\n",
    "                    comment_username = comment.author.name\n",
    "                    comment_flair = comment.author_flair_text\n",
    "                    writer.writerow([comment_id, comment_parent_id, comment_username, comment_time, comment_flair, body])\n",
    "                \n",
    "                else:\n",
    "                    #print('comment.author is None')\n",
    "                    #print(f'body: {body}\\n')\n",
    "                    writer.writerow([comment_id, comment_parent_id, None, comment_time, None, body])\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_comments_collected = get_comments(reddit, id_arr_decoded, 'top_100_post_comments_no_chinese.txt') #takes a min to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3707.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# num of actual total comments\n",
    "print(df['num_comments'].sum())\n",
    "\n",
    "# num of collected comments\n",
    "print(num_comments_collected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Num of comments collected\n",
    "- Num of collected posts and comments in top 100 posts: 3633\n",
    "- Num of actual total posts and comments in top 100 posts: 3709"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with one submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = reddit.submission(\"14m8mf4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-29 10:54:44\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "# get datetime of post\n",
    "created = submission.created\n",
    "time = datetime.datetime.fromtimestamp(created)\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14m8mf4\n",
      "[Megathread] Supreme Court Ruling on Affirmative Action\n",
      "Tungsten_\n",
      "None\n",
      "2023-06-29 10:54:44\n",
      "2023-06-29 10:54:44\n",
      "This is a consolidated thread for users to discuss today's supreme court decision on affirmative action at Harvard and UNC. Please, even in disagreement, be civil and kind.\n",
      "\n",
      "[NBC](https://www.nbcnews.com/politics/supreme-court/supreme-court-strikes-affirmative-action-programs-harvard-unc-rcna66770)\n",
      "\n",
      "[CNN](https://www.cnn.com/politics/live-news/supreme-court-decisions/index.html)\n",
      "\n",
      "[NYT](https://www.nytimes.com/live/2023/06/29/us/affirmative-action-supreme-court)\n",
      "\n",
      "[WaPo](https://www.washingtonpost.com/education/2023/06/29/supreme-court-student-loan-forgiveness-affirmative-action/)\n",
      "\n",
      "[Supreme Court Opinion](https://www.supremecourt.gov/opinions/22pdf/20-1199_hgdj.pdf)\n"
     ]
    }
   ],
   "source": [
    "if submission.author is not None:\n",
    "    username = submission.author.name\n",
    "    flair = submission.author_flair_text\n",
    "    time = datetime.datetime.fromtimestamp(submission.created)\n",
    "    time_utc = datetime.datetime.fromtimestamp(submission.created_utc) # created and created_utc return the same time\n",
    "    self_text = submission.selftext\n",
    "    title = submission.title\n",
    "    id = submission.id\n",
    "\n",
    "print(id)\n",
    "print(title)\n",
    "print(username)\n",
    "print(flair)\n",
    "print(time)\n",
    "print(time_utc)\n",
    "print(self_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prawcore.exceptions import Forbidden, NotFound\n",
    "\n",
    "def user_comments_posts_exclude_sr(usernames_input, regex, sr_='t5_2rfyw'):\n",
    "    '''\n",
    "    Function to get the comments and posts (not in r/asianamerican) by the users collected from r/asianamerican relating to affirmative action\n",
    "    Returns: list of usernames which had more than 100 posts or comments\n",
    "    \n",
    "    username: Redditor username\n",
    "    regex: regular expressions used to search relevant comments and posts\n",
    "    df_: dataframe to store returned comments and posts\n",
    "    sr: subreddit id to not retrieve from (in our case: t5_2rfyw (r/asianamerican))\n",
    "    '''\n",
    "\n",
    "    # count of usernames iterated\n",
    "    count = 0\n",
    "\n",
    "    # arrays to hold attributes of relevant comments&posts\n",
    "    id_arr = []\n",
    "    parent_id_arr = []\n",
    "    username_arr = []\n",
    "    time_created_arr = []\n",
    "    flair_arr = []\n",
    "    body_arr = []\n",
    "    subreddit_arr = []\n",
    "\n",
    "    for username in usernames_input:\n",
    "\n",
    "        try:\n",
    "            submissions_sublist = reddit.redditor(username).submissions.new(limit=None)\n",
    "\n",
    "            # get submissions of this user\n",
    "            for subm in submissions_sublist:\n",
    "                # check subreddit id\n",
    "                if subm.subreddit_id != sr_:\n",
    "                    # title or selftext matches regex, append row\n",
    "                    if regex.search(subm.title.lower()) or regex.search(subm.selftext.lower()):\n",
    "                        id_arr.append(subm.id)\n",
    "                        parent_id_arr.append(None) # submissions don't have parents\n",
    "                        username_arr.append(username)\n",
    "                        time_created_arr.append(subm.created_utc)\n",
    "                        flair_arr.append(subm.author_flair_text)\n",
    "                        body_arr.append(f'{subm.title} {subm.selftext}')\n",
    "                        subreddit_arr.append(subm.subreddit_id)\n",
    "\n",
    "        except Forbidden:\n",
    "            print(f\"We are forbidden from {username}'s submissions!\")\n",
    "            pass\n",
    "\n",
    "        except NotFound:\n",
    "            print(f'404 NotFound Error')\n",
    "            pass\n",
    "\n",
    "\n",
    "        try:\n",
    "\n",
    "            # get comments of this user from other subreddits\n",
    "            comments_sublist = reddit.redditor(username).comments.new(limit=None)\n",
    "\n",
    "            for comment in comments_sublist:\n",
    "                if comment.subreddit_id != sr_:\n",
    "                    if regex.search(comment.body.lower()):\n",
    "                        id_arr.append(comment.id)\n",
    "                        parent_id_arr.append(comment.parent_id) # submissions don't have parents\n",
    "                        username_arr.append(username)\n",
    "                        time_created_arr.append(comment.created_utc)\n",
    "                        flair_arr.append(comment.author_flair_text)\n",
    "                        body_arr.append(comment.body)\n",
    "                        subreddit_arr.append(comment.subreddit_id)\n",
    "\n",
    "        except Forbidden:\n",
    "            print(f\"We are forbidden from {username}'s comments!\")\n",
    "            pass\n",
    "\n",
    "        except NotFound:\n",
    "            print(f'404 NotFound Error')\n",
    "            pass\n",
    "\n",
    "    if count % int(len(usernames_input)/3) == 0:\n",
    "        print(f'Count: {count} - a third completed')\n",
    "        \n",
    "\n",
    "    # create dataframe to append\n",
    "    dict_append = {'id': id_arr, 'parent_id': parent_id_arr, 'username': username_arr, 'time_created_arr': time_created_arr, 'flair': flair_arr, 'body': body_arr, 'subreddit': subreddit_arr}\n",
    "    df_append.to_csv('df_append_1.csv', index=False)\n",
    "\n",
    "    df_append = pd.DataFrame.from_dict(dict_append)\n",
    "    \n",
    "    return df_append"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_df = pd.read_csv('C:/Users/danie/Study/MACSS/macs30200/scraper/top_100_post_comments_user_time_text.txt', header=None, names=['id', 'parent_id','username', 'time_created', 'flair', 'body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>username</th>\n",
       "      <th>time_created</th>\n",
       "      <th>flair</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14m8mf4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-29 10:54:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Megathread] Supreme Court Ruling on Affirmati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jq5du0z</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-30 11:33:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jq0dgzx</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>2023-06-29 11:16:15</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jq0cg7k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>2023-06-29 11:09:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jq0f52k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>2023-06-29 11:26:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3729</th>\n",
       "      <td>epcqlfa</td>\n",
       "      <td>t1_elt4c17</td>\n",
       "      <td>arientyse</td>\n",
       "      <td>2019-05-29 14:15:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Please don't refer to us as \"the blacks.\" It's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3730</th>\n",
       "      <td>4ziaf0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unkle</td>\n",
       "      <td>2016-08-25 07:44:54</td>\n",
       "      <td>senyorito</td>\n",
       "      <td>From Self-Interest to Collective Morality: How...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3731</th>\n",
       "      <td>d6witvi</td>\n",
       "      <td>t3_4ziaf0</td>\n",
       "      <td>Swordbow</td>\n",
       "      <td>2016-08-25 14:47:22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The writer takes good nuggets but integrates t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3732</th>\n",
       "      <td>d6z9lhb</td>\n",
       "      <td>t3_4ziaf0</td>\n",
       "      <td>IRVCath</td>\n",
       "      <td>2016-08-27 17:02:04</td>\n",
       "      <td>Fil-Am, 1.5 Generation</td>\n",
       "      <td>Interestingly enough, even though Filipinos ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3733</th>\n",
       "      <td>d6w9qin</td>\n",
       "      <td>t3_4ziaf0</td>\n",
       "      <td>BletchTheWalrus</td>\n",
       "      <td>2016-08-25 11:40:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nope</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3734 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id   parent_id         username         time_created  \\\n",
       "0     14m8mf4         NaN        Tungsten_  2023-06-29 10:54:44   \n",
       "1     jq5du0z  t3_14m8mf4        Tungsten_  2023-06-30 11:33:11   \n",
       "2     jq0dgzx  t3_14m8mf4   ProudBlackMatt  2023-06-29 11:16:15   \n",
       "3     jq0cg7k  t3_14m8mf4     TomatoCanned  2023-06-29 11:09:47   \n",
       "4     jq0f52k  t3_14m8mf4     bad-fengshui  2023-06-29 11:26:41   \n",
       "...       ...         ...              ...                  ...   \n",
       "3729  epcqlfa  t1_elt4c17        arientyse  2019-05-29 14:15:14   \n",
       "3730   4ziaf0         NaN            unkle  2016-08-25 07:44:54   \n",
       "3731  d6witvi   t3_4ziaf0         Swordbow  2016-08-25 14:47:22   \n",
       "3732  d6z9lhb   t3_4ziaf0          IRVCath  2016-08-27 17:02:04   \n",
       "3733  d6w9qin   t3_4ziaf0  BletchTheWalrus  2016-08-25 11:40:47   \n",
       "\n",
       "                       flair  \\\n",
       "0                        NaN   \n",
       "1                        NaN   \n",
       "2           Chinese-American   \n",
       "3                        NaN   \n",
       "4                        NaN   \n",
       "...                      ...   \n",
       "3729                     NaN   \n",
       "3730               senyorito   \n",
       "3731                     NaN   \n",
       "3732  Fil-Am, 1.5 Generation   \n",
       "3733                     NaN   \n",
       "\n",
       "                                                   body  \n",
       "0     [Megathread] Supreme Court Ruling on Affirmati...  \n",
       "1     Thanks to everyone who engaged in insightful a...  \n",
       "2     I would prefer using a process that takes into...  \n",
       "3     u/Tungsten_, Thanks for creating a section jus...  \n",
       "4     As with anything related to Asians in politics...  \n",
       "...                                                 ...  \n",
       "3729  Please don't refer to us as \"the blacks.\" It's...  \n",
       "3730  From Self-Interest to Collective Morality: How...  \n",
       "3731  The writer takes good nuggets but integrates t...  \n",
       "3732  Interestingly enough, even though Filipinos ar...  \n",
       "3733                                               Nope  \n",
       "\n",
       "[3734 rows x 6 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add subreddit column to dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit_col = ['t5_2rfyw']*comments_df.shape[0]\n",
    "comments_df['subreddit'] = subreddit_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>username</th>\n",
       "      <th>time_created</th>\n",
       "      <th>flair</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14m8mf4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-29 10:54:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Megathread] Supreme Court Ruling on Affirmati...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jq5du0z</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>Tungsten_</td>\n",
       "      <td>2023-06-30 11:33:11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks to everyone who engaged in insightful a...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>jq0dgzx</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>ProudBlackMatt</td>\n",
       "      <td>2023-06-29 11:16:15</td>\n",
       "      <td>Chinese-American</td>\n",
       "      <td>I would prefer using a process that takes into...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jq0cg7k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>TomatoCanned</td>\n",
       "      <td>2023-06-29 11:09:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u/Tungsten_, Thanks for creating a section jus...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jq0f52k</td>\n",
       "      <td>t3_14m8mf4</td>\n",
       "      <td>bad-fengshui</td>\n",
       "      <td>2023-06-29 11:26:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As with anything related to Asians in politics...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3729</th>\n",
       "      <td>epcqlfa</td>\n",
       "      <td>t1_elt4c17</td>\n",
       "      <td>arientyse</td>\n",
       "      <td>2019-05-29 14:15:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Please don't refer to us as \"the blacks.\" It's...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3730</th>\n",
       "      <td>4ziaf0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unkle</td>\n",
       "      <td>2016-08-25 07:44:54</td>\n",
       "      <td>senyorito</td>\n",
       "      <td>From Self-Interest to Collective Morality: How...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3731</th>\n",
       "      <td>d6witvi</td>\n",
       "      <td>t3_4ziaf0</td>\n",
       "      <td>Swordbow</td>\n",
       "      <td>2016-08-25 14:47:22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The writer takes good nuggets but integrates t...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3732</th>\n",
       "      <td>d6z9lhb</td>\n",
       "      <td>t3_4ziaf0</td>\n",
       "      <td>IRVCath</td>\n",
       "      <td>2016-08-27 17:02:04</td>\n",
       "      <td>Fil-Am, 1.5 Generation</td>\n",
       "      <td>Interestingly enough, even though Filipinos ar...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3733</th>\n",
       "      <td>d6w9qin</td>\n",
       "      <td>t3_4ziaf0</td>\n",
       "      <td>BletchTheWalrus</td>\n",
       "      <td>2016-08-25 11:40:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nope</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3734 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id   parent_id         username         time_created  \\\n",
       "0     14m8mf4         NaN        Tungsten_  2023-06-29 10:54:44   \n",
       "1     jq5du0z  t3_14m8mf4        Tungsten_  2023-06-30 11:33:11   \n",
       "2     jq0dgzx  t3_14m8mf4   ProudBlackMatt  2023-06-29 11:16:15   \n",
       "3     jq0cg7k  t3_14m8mf4     TomatoCanned  2023-06-29 11:09:47   \n",
       "4     jq0f52k  t3_14m8mf4     bad-fengshui  2023-06-29 11:26:41   \n",
       "...       ...         ...              ...                  ...   \n",
       "3729  epcqlfa  t1_elt4c17        arientyse  2019-05-29 14:15:14   \n",
       "3730   4ziaf0         NaN            unkle  2016-08-25 07:44:54   \n",
       "3731  d6witvi   t3_4ziaf0         Swordbow  2016-08-25 14:47:22   \n",
       "3732  d6z9lhb   t3_4ziaf0          IRVCath  2016-08-27 17:02:04   \n",
       "3733  d6w9qin   t3_4ziaf0  BletchTheWalrus  2016-08-25 11:40:47   \n",
       "\n",
       "                       flair  \\\n",
       "0                        NaN   \n",
       "1                        NaN   \n",
       "2           Chinese-American   \n",
       "3                        NaN   \n",
       "4                        NaN   \n",
       "...                      ...   \n",
       "3729                     NaN   \n",
       "3730               senyorito   \n",
       "3731                     NaN   \n",
       "3732  Fil-Am, 1.5 Generation   \n",
       "3733                     NaN   \n",
       "\n",
       "                                                   body subreddit  \n",
       "0     [Megathread] Supreme Court Ruling on Affirmati...  t5_2rfyw  \n",
       "1     Thanks to everyone who engaged in insightful a...  t5_2rfyw  \n",
       "2     I would prefer using a process that takes into...  t5_2rfyw  \n",
       "3     u/Tungsten_, Thanks for creating a section jus...  t5_2rfyw  \n",
       "4     As with anything related to Asians in politics...  t5_2rfyw  \n",
       "...                                                 ...       ...  \n",
       "3729  Please don't refer to us as \"the blacks.\" It's...  t5_2rfyw  \n",
       "3730  From Self-Interest to Collective Morality: How...  t5_2rfyw  \n",
       "3731  The writer takes good nuggets but integrates t...  t5_2rfyw  \n",
       "3732  Interestingly enough, even though Filipinos ar...  t5_2rfyw  \n",
       "3733                                               Nope  t5_2rfyw  \n",
       "\n",
       "[3734 rows x 7 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252.33333333333334\n"
     ]
    }
   ],
   "source": [
    "regex = re.compile(r'affirma?t?i?v?e?\\saction')\n",
    "usernames = comments_df['username'].unique() \n",
    "#print(type(usernames))\n",
    "usernames = usernames[~pd.isnull(usernames)] # get rid of nan\n",
    "\n",
    "print(len(usernames)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are forbidden from ShalomHasaeyo's submissions!\n",
      "We are forbidden from ShalomHasaeyo's comments!\n",
      "We are forbidden from Substantial_Bath_887's submissions!\n",
      "We are forbidden from Substantial_Bath_887's comments!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-bdb5c0b06df7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfirst_third_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser_comments_posts_exclude_sr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0musernames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m253\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mregex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-35-2b4d83c0464b>\u001b[0m in \u001b[0;36muser_comments_posts_exclude_sr\u001b[1;34m(usernames_input, regex, sr_)\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0mcomments_sublist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreddit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mredditor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0musername\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mcomment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcomments_sublist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcomment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubreddit_id\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0msr_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mregex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\praw\\models\\listing\\generator.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_listing\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_list_index\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_listing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_list_index\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\praw\\models\\listing\\generator.py\u001b[0m in \u001b[0;36m_next_batch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     87\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_listing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reddit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_listing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extract_sublist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_listing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_list_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\praw\\util\\deprecate_args.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                 )\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_old_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\praw\\reddit.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, path, params)\u001b[0m\n\u001b[0;32m    710\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m         \"\"\"\n\u001b[1;32m--> 712\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_objectify_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"GET\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    713\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0m_deprecate_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"fullnames\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"url\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"subreddits\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\praw\\reddit.py\u001b[0m in \u001b[0;36m_objectify_request\u001b[1;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[0;32m    521\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 523\u001b[1;33m                 \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    524\u001b[0m             )\n\u001b[0;32m    525\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\praw\\util\\deprecate_args.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                 )\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_old_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\praw\\reddit.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, data, files, json, method, params, path)\u001b[0m\n\u001b[0;32m    945\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m                 \u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    948\u001b[0m             )\n\u001b[0;32m    949\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mBadRequest\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\prawcore\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, path, data, files, json, params, timeout)\u001b[0m\n\u001b[0;32m    335\u001b[0m             \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m             \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m             \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         )\n\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\prawcore\\sessions.py\u001b[0m in \u001b[0;36m_request_with_retries\u001b[1;34m(self, data, files, json, method, params, timeout, url, retry_strategy_state)\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[0mretry_strategy_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m             \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m         )\n\u001b[0;32m    238\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\prawcore\\sessions.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, data, files, json, method, params, retry_strategy_state, timeout, url)\u001b[0m\n\u001b[0;32m    193\u001b[0m                 \u001b[0mjson\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m                 \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m             )\n\u001b[0;32m    197\u001b[0m             log.debug(\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\prawcore\\rate_limit.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, request_function, set_header_callback, *args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"headers\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset_header_callback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\prawcore\\requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, timeout, *args, **kwargs)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             return self._http.request(\n\u001b[1;32m---> 59\u001b[1;33m                 \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m             )\n\u001b[0;32m     61\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    531\u001b[0m         }\n\u001b[0;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    645\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 646\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    647\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    648\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m                 )\n\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    670\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 672\u001b[1;33m                 \u001b[0mchunked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    673\u001b[0m             )\n\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    419\u001b[0m                     \u001b[1;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m                     \u001b[1;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m                     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\urllib3\\packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    414\u001b[0m                 \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 416\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    417\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1342\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1344\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1345\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\urllib3\\contrib\\pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWantReadError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait_for_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgettimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The read operation timed out\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\urllib3\\util\\wait.py\u001b[0m in \u001b[0;36mwait_for_read\u001b[1;34m(sock, timeout)\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[0mReturns\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msocket\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mreadable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mFalse\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0mexpired\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m     \"\"\"\n\u001b[1;32m--> 146\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mwait_for_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\urllib3\\util\\wait.py\u001b[0m in \u001b[0;36mselect_wait_for_socket\u001b[1;34m(sock, read, write, timeout)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# thing.)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0mfn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrcheck\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwcheck\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwcheck\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m     \u001b[0mrready\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwready\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxready\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_retry_on_intr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrready\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mwready\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mxready\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\danie\\anaconda3\\lib\\site-packages\\urllib3\\util\\wait.py\u001b[0m in \u001b[0;36m_retry_on_intr\u001b[1;34m(fn, timeout)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;31m# Modern Python, that retries syscalls by default\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_retry_on_intr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "first_third_df = user_comments_posts_exclude_sr(usernames[:253],regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***No username reached 100 comments or submission***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([], [])\n"
     ]
    }
   ],
   "source": [
    "print(reached_100_tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>username</th>\n",
       "      <th>time_created</th>\n",
       "      <th>flair</th>\n",
       "      <th>body</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3724</th>\n",
       "      <td>elsys2l</td>\n",
       "      <td>t1_elspvh4</td>\n",
       "      <td>Marisa_Nya</td>\n",
       "      <td>2019-04-25 22:14:43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yeah. I definitely feel this. I'm Pakistani an...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3725</th>\n",
       "      <td>elmt99c</td>\n",
       "      <td>t1_ellf35p</td>\n",
       "      <td>lukarioDC</td>\n",
       "      <td>2019-04-23 23:43:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>For all those out there out-of-the-loop, here ...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3726</th>\n",
       "      <td>ellffyn</td>\n",
       "      <td>t1_elldtlt</td>\n",
       "      <td>Marisa_Nya</td>\n",
       "      <td>2019-04-23 13:55:49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm not sure why this reply is at -1. I'm tryi...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3727</th>\n",
       "      <td>elpcbu2</td>\n",
       "      <td>t1_elow87u</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-24 19:22:20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Here's a couple of online articles about it: \\...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3728</th>\n",
       "      <td>elt4c17</td>\n",
       "      <td>t1_elsys2l</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-25 23:34:23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TBH, I find this macro-lumping rather lazy on ...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3729</th>\n",
       "      <td>epcqlfa</td>\n",
       "      <td>t1_elt4c17</td>\n",
       "      <td>arientyse</td>\n",
       "      <td>2019-05-29 14:15:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Please don't refer to us as \"the blacks.\" It's...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3730</th>\n",
       "      <td>4ziaf0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unkle</td>\n",
       "      <td>2016-08-25 07:44:54</td>\n",
       "      <td>senyorito</td>\n",
       "      <td>From Self-Interest to Collective Morality: How...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3731</th>\n",
       "      <td>d6witvi</td>\n",
       "      <td>t3_4ziaf0</td>\n",
       "      <td>Swordbow</td>\n",
       "      <td>2016-08-25 14:47:22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The writer takes good nuggets but integrates t...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3732</th>\n",
       "      <td>d6z9lhb</td>\n",
       "      <td>t3_4ziaf0</td>\n",
       "      <td>IRVCath</td>\n",
       "      <td>2016-08-27 17:02:04</td>\n",
       "      <td>Fil-Am, 1.5 Generation</td>\n",
       "      <td>Interestingly enough, even though Filipinos ar...</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3733</th>\n",
       "      <td>d6w9qin</td>\n",
       "      <td>t3_4ziaf0</td>\n",
       "      <td>BletchTheWalrus</td>\n",
       "      <td>2016-08-25 11:40:47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nope</td>\n",
       "      <td>t5_2rfyw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id   parent_id         username         time_created  \\\n",
       "3724  elsys2l  t1_elspvh4       Marisa_Nya  2019-04-25 22:14:43   \n",
       "3725  elmt99c  t1_ellf35p        lukarioDC  2019-04-23 23:43:15   \n",
       "3726  ellffyn  t1_elldtlt       Marisa_Nya  2019-04-23 13:55:49   \n",
       "3727  elpcbu2  t1_elow87u              NaN  2019-04-24 19:22:20   \n",
       "3728  elt4c17  t1_elsys2l              NaN  2019-04-25 23:34:23   \n",
       "3729  epcqlfa  t1_elt4c17        arientyse  2019-05-29 14:15:14   \n",
       "3730   4ziaf0         NaN            unkle  2016-08-25 07:44:54   \n",
       "3731  d6witvi   t3_4ziaf0         Swordbow  2016-08-25 14:47:22   \n",
       "3732  d6z9lhb   t3_4ziaf0          IRVCath  2016-08-27 17:02:04   \n",
       "3733  d6w9qin   t3_4ziaf0  BletchTheWalrus  2016-08-25 11:40:47   \n",
       "\n",
       "                       flair  \\\n",
       "3724                     NaN   \n",
       "3725                     NaN   \n",
       "3726                     NaN   \n",
       "3727                     NaN   \n",
       "3728                     NaN   \n",
       "3729                     NaN   \n",
       "3730               senyorito   \n",
       "3731                     NaN   \n",
       "3732  Fil-Am, 1.5 Generation   \n",
       "3733                     NaN   \n",
       "\n",
       "                                                   body subreddit  \n",
       "3724  Yeah. I definitely feel this. I'm Pakistani an...  t5_2rfyw  \n",
       "3725  For all those out there out-of-the-loop, here ...  t5_2rfyw  \n",
       "3726  I'm not sure why this reply is at -1. I'm tryi...  t5_2rfyw  \n",
       "3727  Here's a couple of online articles about it: \\...  t5_2rfyw  \n",
       "3728  TBH, I find this macro-lumping rather lazy on ...  t5_2rfyw  \n",
       "3729  Please don't refer to us as \"the blacks.\" It's...  t5_2rfyw  \n",
       "3730  From Self-Interest to Collective Morality: How...  t5_2rfyw  \n",
       "3731  The writer takes good nuggets but integrates t...  t5_2rfyw  \n",
       "3732  Interestingly enough, even though Filipinos ar...  t5_2rfyw  \n",
       "3733                                               Nope  t5_2rfyw  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks to everyone who engaged in insightful and respectful discourse about the news. \n",
      "\n",
      "This thread is now locked for comments.\n",
      "I would prefer using a process that takes into account poverty instead. \n",
      "\n",
      "The first generation of my family that came to America was *painfully* poor and everyone showed up with neither money nor education. They worked in kitchens and laundromats. Notice a lot of people in bigger reddit boards talking shit about the \"Chinese billionaire\" boogeyman (fearmongering like this also erases the less visible Asian races who came to America as refugees and reduces all Asians to a monolithic \"rich Asian stereotype\") and how this will only help them. The Chinese people I know were not coming to America with bags of cash.\n",
      "u/Tungsten_, Thanks for creating a section just to discuss this. When I read the news I immediately went searching for a forum where folks might have civil discourse on this topic.\n",
      "\n",
      "Just had a few comments/questions:\n",
      "\n",
      "1. Has anyone come across seemingly legitimate data sets on asians & college admission with respect to Affirmative Action (AA for short going forward)\n",
      "2. As an Asian (not born in the US but pretty much assimilated here for 35+ years), I am conflicted. Research results like this one show: [https://www.pewresearch.org/race-ethnicity/2023/06/08/asian-americans-hold-mixed-views-around-affirmative-action/](https://www.pewresearch.org/race-ethnicity/2023/06/08/asian-americans-hold-mixed-views-around-affirmative-action/) that something like 53% Asians think AA is a good thing, and yet when you scroll down and look at the question of \"Should colleges consider race/ethnicity in college admissions,\" the percentage of Asians that say yes are at 21%, no at 76%.\n",
      "\n",
      "I am part of the 76%.... and I'm conflicted. I know especially for the underserved, AA makes a significant impact in giving folks better chances at life which in turn translates to diversity in every facet of work, society, life in general, which I view is a good thing.\n",
      "\n",
      "But specifically regarding college admissions.. say for my own kids? (not college aged yet)  I would like to see more data on whether year 2000 and beyond AA in college admissions was harmful to Asians in general. In my own experience (anecdotal, totally not data science driven), I feel like AA in college admissions has hurt friends and family, in a reverse sort of sense.\n",
      "\n",
      "But for the sake of the underserved, I didn't want AA to go away. So I am deeply conflicted.\n",
      "\n",
      "Your thoughts?\n",
      "As with anything related to Asians in politics, I'm seeing a lot of non-asian people telling us what we should do and how we should feel. I feel like this is the fundamental problem with popular politics, even when issues affect us directly, it is never centered on our experience, our perspective, and our own self-interests.\n",
      "Yet colleges will allow alumni and doners in easily without considering merit, which make up about 40% of an ivy league school. The system will remain skewed for the rich and powerful while the rest are distracted fighting for scraps.\n",
      "I just hated Affirmative Action as a distraction and a bandaid from multiple failed systems.\n",
      "\n",
      "Why are we trying to fix a problem at the *end* of the line? The problem is that Black/Latino students aren't graduating high school with the skills to be competitive for college. And note I said skills, not grades, because GPA is subjective bullshit and I've known 3.0 GPA kids from competitive schools who could run circles around 4.0 kids at shitty schools.\n",
      "\n",
      "If colleges aren't getting competitive applicants, why should they have to change their methodology to make up for the broken K-12 system? Why are we pointing a finger at colleges when we should be pointing at K-12?\n",
      "\n",
      "And so now what? Affirmative Action is not legal, but colleges will continue to artificially create a diverse environment by twisting their acceptance process until they get the result people want. And so do we continue to ignore the giant elephant in the room of what the fuck is wrong with K-12?\n",
      "My own feeling is that I was never in love with affirmative action, because it's not possible to give a preference for one group without implicitly making it harder for another group, but I supported it because I support diversity in higher education. When I applied to college I avoided applying to schools that were 90% white. I also believe that I benefited from it, because I was a first-generation college student. Affirmative action isn't just about helping black students.\n",
      "\n",
      "I also think that in the grand scheme of things, affirmative action is only used in very selective colleges (where there are probably more valedictorians with perefct test scores than there are spots), and not where most people go to school which is community college and big state universities, which are not very selective and mainly pick based on grades, test scores, etc. Honestly, most community colleges/universities will select you if you can pay and I wish people would stop obsessing over Harvard.\n",
      "\n",
      "I think that admissions officers do have racial biases and that these won't go away no matter what happens with affirmative action. Anti-Asian racism won't go down as a result of the ruling.\n",
      "\n",
      "I also think that overall the general support for diversity initiatives in the workplace is a good thing (overall) and that's something this ruling won't affect.\n",
      "Anti Asian racism whether against East Asians or South Asians will not go away because of this ruling, we have to keep a very close watch over many upset educators/admission officers who will do everything under the Sun to hold back deserving children going forward.\n",
      "Can we overturn legacy and athlete admissions now? What's the point of overturning affirmative action but keeping legacy and athlete admissions, except stacking the game towards the privileged?\n",
      "I want to remind people that in California, one of the most progressive states in our country, a [proposition to reinstate affirmative action lost by 15 percentage points.](https://ballotpedia.org/California_Proposition_16,_Repeal_Proposition_209_Affirmative_Action_Amendment_(2020)) Race based affirmative action is broadly unpopular overall. You'll really only find far-left progressives try to paint this as as polarized issue. Just because the head of SFFA is a conservative litigant activist does not mean you're part of the GOP because you hold the same stance of being against affirmative action\n",
      "\n",
      "I know people will say \"but why don't anti-affirmative action types care about legacies??\" The truth is most of us want to see legacy preferences done away with, but there is no grounds within the Constitution to sue a private university for engaging in legacy preference.\n",
      "\n",
      "In fact, in the oral arguments for this case, [**Harvard defended their practice of legacy/donor preference when SFFA brought up that eliminating it would increase diversity**](https://www.thecrimson.com/article/2022/11/1/legacy-admissions-scotus/)**.**\n",
      "\n",
      "Now that race-based affirmative action is struck down, it's no longer tenable for higher institutions like Harvard to act like they meaningfully care about diversity while having an  inherently inequitable preference for legacy/donor applicants.\n",
      "I think this is great news for our nation. While we’re at it, we should also abolish legacy admission and admission of children of wealthy donors. \n",
      "\n",
      "I’m a firm believer on meritocracy and equality for all. \n",
      "\n",
      "The push we as a nation should be focusing on is for quality and accessible early childhood education for all, regardless of socioeconomic level, which has decades of data showing it pays dividends on a student’s academic life and beyond. \n",
      "\n",
      "By the end of HS or college, the attempt at a boost comes too late, often with underperforming students flunking and dropping out of schools they were unqualified for, while creating racial tensions for obvious reasons. Suppressing and oppressing one group to lift another isn’t the answer.\n",
      "My main concern is that, since the Court did not strike down *Grutter* outright, universities are simply going to find a dozen other, more indirect, insidious ways to continue discriminating against Asian applicants. They won't come right out and say it, but they'll do things like award 'admission points' to applicants of other races who write application essays about how they faced anti-black, anti-Hispanic, anti-native-American discrimination, but award no points or in fact even deduct points for students who write about how they faced anti-Asian discrimination.\n",
      "\n",
      "Even in its ruling, the Supreme Court clarified in its majority opinion that universities can still consider race in admissions when an applicant highlights his or her racial difficulties in their personal background life story. It's not hard to see how universities can and will manipulate this.\n",
      "There is obviously a lot to it and no real easy answer that will satisfy everyone but for me, I want students to be admitted based on individual merit. If there are 500 spots, the top 500 students should get in. If I'm student #501 and I didn't get in, I'd want to know it's because they had better grades/scores. I'd be OK with that. \n",
      "\n",
      "It's been a while since I've been in college and went through the application process. Like many others, I identified my reaches and safety choices. Do people really make such a big deal about not getting into their reaches?\n",
      "\n",
      "I have no problem with Asians doing what's best for Asians. Nobody else is going to have our best interests in mind. It shouldn't be on anyone to put their interests behind others.\n",
      "[deleted]\n",
      "I’m sure selective schools like Harvard will find new and innovative ways to discriminate against Asians - but AT LEAST they can’t do a “-20 points for being Asian” in their formula anymore. And perhaps many schools will not. And overall I do consider this a win.\n",
      "TBH, I don't know the depth of how affirmative actions or college admissions work. However, I don't think this would necessarily stop negative biases against Asians, if there are any. If the biases continue to exist, this would benefit White students more as they are deemed to have more leadership qualities, and middle-to-upper class White families tend to have connections that allow the young ones develop and showcase more of these leadership qualities or take them to a higher level.\n",
      "People can continue to debate the legal implications all day long. Plenty of pundits and politicians to look to for emotional responses.\n",
      "\n",
      "My prediction is, like some others have said, there will be minimal practical changes in admission results.\n",
      "\n",
      "But, when I read in the Opinion (1) an affirmation that race should not be used as “a stereotype or a negative” and (2) acknowledgement that racial categories are opaque and incoherent, I did feel seen. And it did feel good.\n",
      "\n",
      "Then that feeling went right away when the Obamas tweeted.\n",
      "The only issue with affirmative action was that it was not explicit enough. If there was a quota system that simply mandated x% of any class had to be an URM paired with a system that minimized the subjective influence of the admission officer and the rest of the chips fall where may, that would be far more tolerable. Instead, we have this bullshit about personality scores that give admission officers the freedom to discriminate and favor any group as they please. \n",
      "\n",
      "This ruling doesn't seem to have affected any of that so the actual material impact seems like it'll be quite limited. I haven't looked at the ruling in enough detail, but I am extremely curious if these rulings mentioned anything about the usage of \"personality scores\". The really critical moment in this entire case was Alito asking the Harvard lawyer Waxman why Asian Americans get worse personality scores than everyone else. Waxman basically gave a self contradictory answer of \"we do it to triage applicants (that is, quickly reject them) but it doesn't matter\". The critical issue of discrimination against Asian Americans as a matter of policy seems to have been entirely dodged.\n",
      "You know what I hate?\n",
      "\n",
      "The main subreddits where certain white people are now rally-crying for asians, clearly using us as a fucking political football to further their agenda of pushing brown and blacks further down.\n",
      "\n",
      "They don't give a fuck about asians, they just want to use us. And I hate that certain asian americans fail to see that.\n",
      "\n",
      "Now, I'm mixed about affirmative action. Because I do believe that elite universities should be more diverse, but at the same time, I also recognize that race is a murky factor. Affirmative action does NOT have POC quota's, it just takes into account race for the applicant. That's not the same as a quota. So for folks who say that this rids of a race quota, you're wrong. Full stop.\n",
      "\n",
      "Would a socio-economic factor be better? I've seen zip codes float around as a solution.\n",
      "\n",
      "There certainly would be bad actors who could game the system, but perhaps that's a better approach?\n",
      "\n",
      "Anyways, I'm just rambling and using this as an excuse to not work.\n",
      "\n",
      "edit: just wrapped up a work meeting, and wanted to get more thoughts out.\n",
      "\n",
      "https://www.ets.org/Media/Research/pdf/reardon_white_paper.pdf\n",
      "\n",
      "This academic paper suggests (emphasis mine, where SES = socio-economic status)\n",
      "\n",
      ">Second, while it has been argued that affirmative action can lead to academic mismatch for\n",
      "minority students, **we find no evidence that this is a systematic result of affirmative action\n",
      "policies.** Moderate levels of race- and/or SES-based affirmative action appear unlikely to\n",
      "result in high-achieving minority or low-SES students enrolling, on average, in colleges\n",
      "where their academic preparation was below the average level for the college they enrolled\n",
      "in. Similarly, we find that affirmative action has little effect on the average academic\n",
      "preparation of students in the colleges of the typical White and high-SES student\n",
      "\n",
      "Additionally: \n",
      "\n",
      ">Until racial disparities in educational preparation are eliminated, then, other strategies are\n",
      "needed. **Our analysis here suggests that affirmative action policies based on socioeconomic\n",
      "status are unlikely to achieve meaningful increases in racial diversity. That is not to say that\n",
      "socioeconomic affirmative action would not be valuable in its own right—it would increase\n",
      "socioeconomic diversity on university campuses and would benefit low-income college\n",
      "applicants—but only that it is not an effective or efficient means to achieving racial diversity.**\n",
      "Race-conscious affirmative action does, however, increase racial diversity effectively at the\n",
      "schools that use it. Although imperfect, it may be the best strategy we currently have. \n",
      "\n",
      "\n",
      "\n",
      "so to my earlier point. I DO FUNDAMENTALLY believe that educational institutes should be diverse. An educated society is a net benefit, I think we can all agree to that, right?\n",
      "I’m not a fan of affirmative action even tho it does help minorities in this country. I recognize this country is deeply unfair due to its history.  \n",
      "\n",
      "That being said, it was a little difficult psychologically being a great student and knowing top universities look at my race as undesirable. For that reason alone I am glad for Asian Americans to see this change. But realistically speaking I doubt it does anything significant to help the Asian American community or our acceptance rates to top universities\n",
      "If we really want to be fair, let's strike down legacy admissions as well as admissions for athletes and children of donors and faculty, since they make up 43% of white students at selective universities. These universities are not a meritocracy until we address legacy admissions. Getting rid of affirmative action (which I think is wrong to do) without even addressing these nepotistic admissions is ridiculous. Only then will I believe that these activists genuinely are acting in the interest of Asian Americans and other minorities.\n",
      "I always say, if an immigrant family of 3 or 4 can come to the United States with only a few hundred dollars (or less) in their pocket, working an underpaid position, and not really speaking English for the first couple years, but still have to do significantly better in academic and extracurricular activities than those who were born here, speaking English from day 1, and have access to public education, simply because of skin color, it's discrimination and racism. \n",
      "\n",
      "There are many socioeconomics factors that contribute to people's hardships, but let's not limit that to skin color.\n",
      "I don't care about where Edward Blum lies, I don't care about the optics of this. AA is racist against Asians, it doesn't deserve a more nuanced \"analysis\", and we don't need to go through mental gymnastics on broader effects and all that jazz. \n",
      "\n",
      "A racist policy against Asians is now gone. Simple as that.\n",
      "I am too dumb to read legalese, but can anyone tell us whether this will apply to selective high schools like Lowell, Stuyvesant, Thomas Jefferson, etc?\n",
      "We might need less affirmative action if we had socialism in healthcare and housing, so that life would be more equal.\n",
      "Lots of people saying this won't change much, that schools will just adjust their policies to meet their goals, but that's not the case.\n",
      "\n",
      "tl;dr more litigations are incoming ofc, any such attempt that violates intent of AA being overturned will be penalized by the court.\n",
      "\n",
      "\"After Affirmative Action Ends\"  -New Yorker\n",
      "\n",
      "https://archive.fo/gsEkW\n",
      "\n",
      "In 2013, the Court, in Fisher v. University of Texas, discussed a race-neutral admissions method that was enacted by the Texas legislature: the top ten per cent of students in every high school in the state were automatically guaranteed admission to any of the state’s public colleges or universities. Because de-facto residential segregation resulted in de-facto school segregation in much of the state, admitting the top ten per cent of each high school meant that a large number of Black and Latino students would be admitted to colleges and universities in Texas. That would produce significant racial diversity on campuses without admissions officers considering applicants’ race.\n",
      "I largely agree with this:  [https://www.nytimes.com/2023/06/29/opinion/affirmative-action-supreme-court-harvard.html](https://www.nytimes.com/2023/06/29/opinion/affirmative-action-supreme-court-harvard.html)\n",
      "\n",
      "In practice, affirmative action at elite universities is done at the expense of Asian Americans, and so it had to be done away with.   \n",
      "\n",
      "\n",
      "Many people have been marginalized, including Asian Americans, over time.  So the supreme court is requiring them to find a system that finds diverse candidates without disadvantaging Asian Americans.\n",
      "There's a lot to discuss here. Echoing what some others have said, I'm conflicted. For one, today's ruling takes away opportunities from underserved groups, yet underserved Asian Americans may no longer be passed over due to the race factor. For another, Asian Americans do broadly stand to gain from all this, because AA's implementation had shown to lower our acceptance rates. Yet its removal may be moot if college admissions councils decide to continue discriminating against us under the table, particularly on account of our names or declarations of race - if applications still ask for these. Honestly, applications should omit names and race entirely, including from essays (unlike what the majority opinion wrote). \n",
      "\n",
      "At the same time, removing AA seems like a \"duh\" moment. After all, forcing diversity and equity in higher education simply isn't the answer. It's part of the problem, just like admitting legacy or donors' kids. Instead, it should all be organic. Let me idealize a bit here. The brightest go to the best schools and get access to the best networks, regardless of race or family wealth. And the way to do that is to reform the secondary education system at both the state and federal levels. Better funding, better teachers' pay, better facilities, more accreditation. Less gerrymandering, less redistricting, less redlining, less preferential loans. This is where more tax dollars should be allocated. In the meantime, admissions should adjust to prefer lower household income as others have said. I imagine this must be a better catchall than race for the purpose of lifting up the disadvantaged.\n",
      "\n",
      "When the playing field for American secondary schooling is equalized across all communities, that's when colleges can freely sieve the candidate pool. Once the country organizes this way, students can be confident that pursuing the best grades, extracurricular involvement, and leadership qualities will indeed result in the best outcomes for themselves. I say this with a vested interest, as both me and my sister were top of our respective classes and had glowing recommendations, yet lower-ranked non-Asian classmates with fewer extracurriculars and less volunteering, work or leadership experience had made it to better colleges. It felt defeating that hard work and community involvement did not, in fact, pay off as an Asian American in this country.\n",
      "\n",
      "Will today's ruling relieve that anguish I felt? Nope. It doesn't do away with racism, but it does do one thing well. I'll use an example to show what I mean. If I need surgery, I want the best doctor available. I don't care if they did extracurriculars or volunteered at a food bank, much less what race they are; I just want to know that they know their stuff so I can maximize my chance of a post-op recovery. It's wild to think that a med student at Harvard could have gotten in with [just a 3.2 GPA](https://www.aei.org/carpe-diem/new-chart-illustrates-graphically-racial-preferences-for-blacks-and-hispanics-being-admitted-to-us-medical-schools/) because Harvard needed one more minority to fill some quota for PR purposes. And that student could graduate, hang their diploma on the wall of their own practice, and many patients would be none the wiser because they'd think that \"Harvard graduates must be the best.\" Which would be true if, say, a 3.9 student were admitted in the 3.2 student's place, and today's ruling hopefully makes this more likely. Will colleges still find some way to increase diversity? Of course, as they should. But soon it won't be by something as seemingly facile as race. \n",
      "\n",
      "Nothing suggests that today's ruling will change anything under the table for Asian Americans either, and the optics of the AsAm community being used as a political football are totally problematic. But oh well, it's still a step in the right direction.\n",
      "Have they also gotten rid of donation based legacy admissions too? Because otherwise this is just a smokescreen. Meritocracy has always been a joke and heavily discussed in the book the myth of meritocracy as a satire of aristocracy. It's been adopted because it falls in line with American exceptionalism and the American dream. So trying to fix this, means they put the target onto other people. Don't pay attention to all the rich kids getting to pay their way in. Look at all the ethnic groups getting in because how unfair that is to the Asian students that try so hard. \n",
      "\n",
      "It sucks because you realize that you can do everything right, literally play by all the rules they set up, excel at them and still not succeed due to no fault of your own. And that's just life. I know most of us grew up thinking and believing in a just world. But we are proven time and time again, that it's not the case. \n",
      "\n",
      "Just world fallacy\n",
      "\" the cognitive bias that assumes that \"people get what they deserve\" – that actions will have morally fair and fitting consequences for the actor. \"\n",
      "\n",
      "Affirmative action is an attempt to try and change an old boys club. Is it perfect? No, hardly. But to make the enemy other minorities without looking at who holds the institutional power is short sighted and exactly the narrative the people who control policy want you to keep looking at.\n",
      "Some thoughts from a different place on earth if you abide my outside perspective:\n",
      "\n",
      "\n",
      "Choosing someone for a position/university spot based on \"race\" or gender is inherently racist/discriminatory, I have a hard time seeing how this complies with the fundaments of law and equality. (Is it a continuation/progress of the civil right movement to use discrimination to combat \"perceived\" injustices? Wouldn't it be better to tear down the walls of inequality and unite those who stand for common rights and freedoms against those who seek power to use systems for their advantage/ideologies/etc?\n",
      "\n",
      "Is it fair to implement an unjust system to combat a different unjust system? (eg. rich white Americans buying their way into colleges) Is this fair for eg. white Americans coming from very low income households from the Midwest or somewhere, in communities where maybe all forms of drug addictions are common and \"schools don't offer the best education\"? Would it be fair to give a black American from a wealthy background the advantage solely based on skin colour?\n",
      "\n",
      "How do Asian Americans feel about (American) notions of \"race\" in contrast to ethnicity? How much in common have eg. People of Chinese, Japanese, Thai and Phillipino origin to each other to feel and speak as a group? How much do you feel (culturally) united or share struggles? (not against it, but it can be misused to nourish \"group think\" \" You are part of my group, help me in my cause/crusade!\")\n",
      "\n",
      "Are the tests as basis for entrance to colleges enough? Or can they be played and engineered? eg. learning exactly for tests while maybe lacking skills in critical thought or empathy? How much does performance in tests correlate to good academic performances?\n",
      "\n",
      "How come black American struggle so much while coming partially from similar low income households as Asian Americans? What issues do they face? What skills/systems/schools/social structures need to be created/supported/etc.? How far should groups be supported in the name of diversity and why? Shouldn't we work on giving poor people independent of skin colour fair chances of success? Wouldn't it be better to work on better schools, fair laws/institutions/police/healthcare/working chances, affordable good funded universities, fair wages and worker rights? (not being forced to work overtime etc.) (trying to create a system where the will of voters mean sth and not just the one of mainly businesses/rich)\n",
      "\n",
      "Why does it seem impossible to implement fair, open and affordable colleges? How come they are in such positions of power to play the game how they like? Wouldn't it be better to fight massive wealth inequality and media powerplays? Fair courts and law making (representing the will of the people)?\n",
      "\n",
      "How would any injustice be combated in the US through \"more diversity\" if we favour another? Why not fight any injustice/discrimination where it is? Why not seek answers for societal issues? Why not found independent of politics/parties big social movements for all Americans (or enough) (eg. fair wages movement, fair courts, peoples rights to sue companies for causing environmental disasters)? Trying to get your rights, your freedoms, living wages back?\n",
      "\n",
      "Note: some terms/labels may mean different things, language is tricky\n",
      "[deleted]\n",
      "I'm broadly generic liberal, but I'm against affirmative action as it was implemented.\n",
      "\n",
      "California universities have had a ban on AA for a while and UCLA+UCB (the \"elite\" UC's) are all still great schools to go to.  I worked hard to get into UCLA, it provided opportunities for me, and I'm happy I wasn't judged by my Chinese ethnicity.\n",
      "\n",
      "Lots of non-Asian liberals fail to understand race outside of the black/white dichotomy.  Asian Americans can have their own unique set of politics just like every other group.\n",
      "Asians are being used as a wedge group between white supremacists and proponents of racial equity and it needs to end. Media conglomerates and the elites behind them are too happy to use us as a buffer for their own ends. I'm ashamed so many of us are happy to throw the concept of diversity under the bus for some marginal chance at improving their own standing, to the delight of white supremacists. White supremacists raise us up as an example to other racial groups to shame them into submitting to the hierarchy. In doing so, they completely disregarding AAPI specific issues. Asian american women are experiencing marginally high rates of suicide and are 3 times more likely to suffer from gendered abuse or domestic violence. A third of our elderly experience extreme levels of poverty, especially if they are located far from Asian sub-urban and urban areas. If your lineage is not from a developed Asian country, you are 5 times more likely to live a life of extreme to moderate poverty. We are the furthest thing from a Monolith. I'll go out on a limb and say we may be the most diverse group of people in Western countries when you account for dimensions such as religion, language, nationality, non-traditional family structures, and sexual orientation. But non-Asians would never know it while consuming mainstream media.\n",
      "Idk how to feel about this. Prior to undergrad and grad school I was told by several white classmates that my achievements were only because of affirmative action and I didn’t really achieve them. I feel like it’s a much more complex issue than just being black and white. I’m a first gen college student. I have never had the resources white legacy’s have had. Supposedly high school counselors tell white students that they’ll never be good enough because of affirmative action but it’s funny because every minority parent has sat their kid down and told them that they need to work 3 times as hard to be seen as good compared to an average white person. \n",
      "\n",
      "I think people are going to find out that they’re far more mediocre than they thought.\n",
      "All of this was pointless from the start and remains pointless. Everything squabbling over this is just being distracted from actual issues affecting students.\n",
      "\n",
      "Trying rectify inequality through college admission is too late. These disparities should to be addressed far before high school juniors start filling out applications.\n",
      "\n",
      "But addressing those disparities is expensive and requires multiple systemic changes, so it's easier to convince you that the fault all lies with how college admissions decide who to  accept over others.\n",
      "\n",
      "Work everyone up into a righteous frothing rage over perceived discrimination and they won't think to wonder why there are so many unprepared and uncompetitive students graduating high school in the first place. None of us would care if black and brown students tested and scored at the same level as whites and Asians.\n",
      "[deleted]\n",
      "I support affirmative action in theory, especially since many PoC groups (including many SE asians and Pacifica groups) are underrepresented.\n",
      "\n",
      "I also don't like Edward Blum and think that he's using asians as pawns.\n",
      "\n",
      "However, the implementation of affirmation action used by universities was undeniable extremely racist towards Asians, including the underrepresented ones. Lower personality scores and forcing Asians to hide their \"asian-ness\" is such blatant discrimination, it sounds like satire.\n",
      "\n",
      "Even worse was the gaslighting and vilification of Asians by \"liberals\" against anyone in our community who dared speak out against this discrimination, even by our own community.\n",
      "\n",
      "And despite the obvious problem that is legacy admissions, conservatives and liberals (who only every brought up the issue when defending affirmative action) are just going to let it slide bc rich and power people on both sides don't want to lose a crumb of their advantage in the world.\n",
      "\n",
      "I'm still pretty liberal overall, but the way that the affirmative action was handled by \"my side\" has made me so bitter and cynical overall.\n",
      "This is an interesting thread:\n",
      "https://twitter.com/_ShamGod/status/1674478928679825408\n",
      "\n",
      ">It’s because of neoconservative strategist Edward Blum. He sued for gerrymandering in Texas, funded the case that gutted the voting rights act, and was responsible for changing the image of affirmative action “victims” from white ppl to Asian ppl.\n",
      "\n",
      "-\n",
      "\n",
      ">It’s this man. He started with Abigail Fisher in 2013, failed, and regrouped by exploiting real anxieties.\n",
      ">\n",
      ">“Blum set his sights on recruiting Asian American applicants in particular after his cases with white students alleging discrimination did not yield his\n",
      "desired results.”\n",
      "Universities will do whatever they want but just hide the proxies for their biases. Whether it be favoring a certain race or favoring a certain economic class. You can easily tell someone’s race by their name. You can’t legislate morality, you can only de-incentivize it. But I don’t think this does much to accomplish anything.\n",
      "While it’s true that legacy admissions are a grave injustice to merit-based admissions at private colleges and universities, and the fact is that they are a major issue at one of the schools (Harvard) named in the lawsuit, the fact is that UNC was also practicing anti-Asian academic racism (and let’s be honest, that’s literally what race-based affirmative action in schools amounts to, and if something has a racist effect like that then it’s no less racist than K-12 inequities are to Blacks and Hispanics) without any legacy admissions involved. Thus, while UNC has been getting less publicity than Harvard, the fact is that legacy admissions are being used as a red herring to obscure the fact that racial justice was achieved for the most forgotten Americans (Asian-Americans). This is especially true because the court was not asked to consider legacy admissions.\n",
      "\n",
      "If the court was indeed asked to consider legacy admissions as well, then I would agree with all the people decrying the ruling. Unfortunately, I cannot side with any of them because however small, we have to realize this is a victory for racial justice. We cannot punish those who still manage to succeed in the face of institutionalized white privilege in the name of advancing those who suffer long-term effects of slavery and Jim Crow (Blacks) and/or those who were absorbed by military conquest and faced less well-known Jim Crow-type situations (Hispanics). Moreover, the court did rule that colleges and universities are still allowed to consider a student’s observations on how race affected their life, without using race itself as a deciding factor for admissions.\n",
      "\n",
      "Lastly, the strongest arguments that have been made in favor of affirmative action are ultimately those rooted in economics more than race. A person whose family paid for SAT tutoring has an unfair advantage over a poor kid who flips burgers after school to feed a broken family, and this is a bigger obstacle than color or facial structure even when being dark leads to unfair police profiling. Let affirmative action be based wholly or primarily on family socioeconomic status, otherwise the biggest beneficiaries will be wealthy Black and Latino kids more than those affirmative action purports to help-and let legacy admissions be challenged in a separate lawsuit for effectively functioning as affirmative action for the generationally-wealthy.\n",
      "I think they should just free up more space by getting rid of a % of legacy admissions. With no AA, legacy will take on those spots now. Start working on 🚣‍♀️.\n",
      "i wish people wouldn't hide under words like 'fairness' and just have the intellectual honesty to say that they're pro/against this decision because it favors/doesn't favor their constituency\n",
      "This is a great day for Asians.\n",
      "[deleted]\n",
      "Whether or not affirmative action is a net positive or not for Asian Americans, I think the fact that this will be the prevailing conversation, as pushed by conservative and right political groups, and YET have left more distinctly racist topics within school admissions untouched ([legacies, which overwhelming benefit white student in Ivy Leagues](https://www.nbcnews.com/news/us-news/study-harvard-finds-43-percent-white-students-are-legacy-athletes-n1060361)) should tell you everything you need to know about why this is happening.  \n",
      "\n",
      "I'll give you a hint: it's not because rightwing assholes like Edward Blum and his ilk care about Asian Americans.  It's because they know that issues like these will activate some folks to happily take up arms against other people of color.  Asian Americans are a wedge group and the right is playing us like a fiddle.\n",
      "The problem with affirmative action is that it not only violated the Civil Liberties Act.... it was used to systematically ostracize select groups.\n",
      "\n",
      "\n",
      "Remember what Dr. King said, \"To judge by the content of one's character, and not by the color of one's skin.\"\n",
      "\n",
      "\n",
      "Asian, Native Americans, and Pacific Islanders are still ignored as people of color; while other voices are amplified and heard.... especially by politicians during an election year.\n",
      "Fuck Edward Blum and any conservative pushing this as a win for Asian Americans. We all know if we weren’t on the right side of the bell curve, they’d fuck us too.\n",
      "Such a silly judgement.  Yea, you can't ask them directly what their race is via a checkbox but let's be honest here.  You can tell a race just by their last name.  If they choose to, they can still lower admission of Asian easily by tossing out ppl with Pak/Chan/Khan last name applications.   Applicatiants can still disclose their race in their essay.  So no, this doesn't change shit.\n",
      "I’m choosing to search for the silver lining and hope that Affirmative Action can come back even stronger, with factors that account for class and income.\n",
      "Another good thing for us Asian Americans, is that our fellow Asian Americans no longer have a reason to vote Republican any more. It used to be every time you argue with an Asian Republican, the first thing that comes out of his mouth will be Affirmative Action. Now that excuse is gone.\n",
      "Not a fan of this decision. You can look at class and poverty, but it won't paint the whole picture. There's a difference between a stable 2 parent Asian immigrant family surrounded by a community that encourages education vs a single parent black household living in underfunded schools surrounded by violence. The effects of slavery, segregation, incarceration, and redlining are too strong to dismiss.\n",
      "\n",
      "It's also important that the student body of the college to be diverse because it simulates the real world and allows people to experience different perspectives. Also having a Black doctor in a Black community is important as well as the people in the community will feel more comfortable around a doctor that looks like them as well as a lot of medical conditions are exclusive or common to Black people that a non Black person maybe unaware of.\n",
      "I think schools should 100% merit based with no other factors considered. If that makes things lean towards one particular race, well guess they have the most merit.\n",
      "damn so it really happened.\n",
      "\n",
      "it's an incredibly naive and simplistic worldview some of you guys have if you think the system will let asians win on this.\n",
      "\n",
      "this is a loss for minorities all around.\n",
      "\n",
      "at best, i'm hoping for a lottery system like many of the nyc schools are doing but highly unlikely due to the hOliSTic admissions process that continues to let them discriminate while allowing legacy students in, regardless of qualifications.\n",
      "I think Asians having a “harder time” getting into these elite schools was wrong.  However, if you think this is going to suddenly “level the field” for Asians, I don’t think that will be the outcome.\n",
      "To all leftists, this is what leftist district judge said to try to weasel out of her attempts to blockade court transcript with highly relevant information.\n",
      "\n",
      "https://www.newyorker.com/news/our-columnists/the-secret-joke-at-the-heart-of-the-harvard-affirmative-action-case\n",
      "\n",
      "https://archive.fo/R6Ad7#selection-861.850-861.1071\n",
      "\n",
      "> Judge Burroughs cut me off: “Yes, I know. I get it.” And then: “I hear you. I got it. I got it.” I wouldn’t have anticipated or believed what she said next, but there it is in the transcript: “Greedy, though, Ms. Gersen.”\n",
      "\n",
      "I'm glad her smug ass judgement got stomped by SC.\n",
      "Here's a contrarian opinion:\n",
      "\n",
      "I am for affirmative action. Why? \n",
      "\n",
      "Let's take a look at medical school admission. The number of slots is largely limited to the # of residency slots (funded by medicare). So it's a zero-sum game. For every white med school student is one fewer latinx med school student. For every azn med school student is one fewer black med school student. [We know that black med school students aren't as \"competitive\" as their azn counterparts](https://www.shemmassianconsulting.com/blog/medical-school-acceptance-rates-by-race#part-2-medical-school-acceptance-rates-by-race). (i'm too lazy to calculate confidence intervals after 5pm). \n",
      "\n",
      "However, [we also know that in areas with black doctors, black residents have better health outcomes](https://www.statnews.com/2023/04/14/black-doctors-primary-care-life-expectancy-mortality/#:~:text=Other%20studies%20have%20shown%20that,blood%20tests%20and%20flu%20shots.) So in using a utilitarian argument for maximizing public health, we *should* maximize the # of black medical students. Yes, they should be academically capable of finishing med school and passing the boards (as a dropout is a wasted slot). So sorry Sarah Chen with a 4.0 GPA from Stanford, maybe we shouldn't let you in so we can let a black student in.\n",
      "(posted this in another related thread)\n",
      "\n",
      "Also worth listening is a recent Throughline episode: [https://www.npr.org/2023/06/14/1182149332/affirmative-action](https://www.npr.org/2023/06/14/1182149332/affirmative-action)\n",
      "\n",
      "Echoes some of the same concerns that I'm reading in the replies here.\n",
      "Race is not a proxy for experience. It can now merely be a factor.\n",
      "i and every asian i know who got into harvard got in bc we were assimilated relative to our peers (this was in 2000s) \n",
      "\n",
      "i taught there recently and it seems to have changed a lot for the better -- in that the asian students seem more diverse as a group.\n",
      "\n",
      "this is what the personality score was capturing -- i believe it's legit to criticize it. i hope they continue their diversification efforts but the white elite idea of captured in terms of a demographics score was total bullshit. it's good that's going away, doesn't mean race or income or whatever based admissions has to. the media is reporting the most divisive interpretation of this news\n",
      "Asians are fighting so hard for white supremacists was not on my bingo card. They don’t love you. Next years Harvard stats will prove this.\n",
      "[removed]\n",
      "[removed]\n",
      "I’ll never be referred to as an affirmative action hire.  I’m here because I’m here dammit.\n"
     ]
    }
   ],
   "source": [
    "# print top level comments\n",
    "for top_level_comment in submission.comments:\n",
    "    print(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t3_14m8mf4\n",
      "t3_14m8mf4\n",
      "t3_14m8mf4\n",
      "t3_14m8mf4\n",
      "t3_14m8mf4\n",
      "t3_14m8mf4\n",
      "t3_14m8mf4\n",
      "t3_14m8mf4\n",
      "t3_14m8mf4\n",
      "t3_14m8mf4\n"
     ]
    }
   ],
   "source": [
    "submission.comments.replace_more(limit=None)\n",
    "for comment in submission.comments.list()[:10]:\n",
    "    #print(comment.body)\n",
    "    #print(datetime.datetime.fromtimestamp(comment.created))\n",
    "    print(comment.parent_id)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find posts/submissions that mention Chinese Americans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrected request url\n",
    "#https://www.reddit.com/r/asianamerican/search/?q=affirmative%20action&restrict_sr=1&sort=top\n",
    "\n",
    "def get_search(subreddit,query,limit,timeframe,sort,restrict_sr):\n",
    "    try:\n",
    "        base_url = f'https://www.reddit.com/r/{subreddit}/search.json?q={query}&restrict_sr={restrict_sr}&limit={limit}&t={timeframe}&sort={sort}'\n",
    "        request = requests.get(base_url, headers = {'User-agent': 'yourbot'})\n",
    "    except:\n",
    "        print('An Error Occured')\n",
    "    return request.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_submission_info(r, n):\n",
    "    '''\n",
    "    Get a List of post titles\n",
    "    r: .json object\n",
    "    n: num of top posts\n",
    "    # important keys: (1) id (Reddit thing ID), (2) title, (3) ups (upvotes), \n",
    "    # (4) downs (downvotes), (5) score (not sure), (6) likes (not sure), (7) num_comments)\n",
    "\n",
    "    Returns: pandas Dataframe object\n",
    "    '''\n",
    "    # create 3 empty arrays of length n\n",
    "    id_arr = np.empty(n, dtype=\"S10\")\n",
    "    #up_arr = np.empty(n)\n",
    "    #down_arr = np.empty(n)\n",
    "    score_arr = np.empty(n)\n",
    "    #likes_arr = np.empty(n)\n",
    "    num_com_arr = np.empty(n)\n",
    "    title_arr = np.empty(n, dtype='object')\n",
    "    self_text_arr = np.empty(n, dtype='object')\n",
    "\n",
    "    \n",
    "    for i, post in enumerate(r['data']['children']):\n",
    "        id_arr[i] = post['data']['id']\n",
    "        #up_arr[i] = post['data']['ups']\n",
    "        #down_arr[i] = post['data']['downs']\n",
    "        score_arr[i] = post['data']['score']\n",
    "        #likes_arr[i] = post['data']['likes']\n",
    "        title_arr[i] = post['data']['title']\n",
    "        num_com_arr[i] = post['data']['num_comments']\n",
    "        self_text_arr[i] = post['data']['selftext']\n",
    "\n",
    "    # create Dataframe\n",
    "    df = pd.DataFrame({'id': id_arr, 'score': score_arr, 'title': title_arr, 'self_text': self_text_arr, 'num_comments': num_com_arr})\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_search() params\n",
    "sr = 'asianamerican'\n",
    "q = 'affirmative action'\n",
    "limit = 100\n",
    "timeframe = 'all'\n",
    "sort = 'top'\n",
    "restrict_sr = 1\n",
    "\n",
    "# variable for array of titles\n",
    "titles = None\n",
    "\n",
    "search = get_search(subreddit=sr,query=q, limit=limit, timeframe=timeframe, sort=sort, restrict_sr=restrict_sr)\n",
    "df = get_top_n_submission_info(search, limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_str = ['chinese', 'chinese-americans', 'chinese-american','chineseamerican', 'chineseamericans']\n",
    "\n",
    "chinese_rows = df[df.title.str.lower().str.contains('chinese') | df.self_text.str.lower().str.contains('chinese')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "      <th>self_text</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'hqic5b'</td>\n",
       "      <td>225.0</td>\n",
       "      <td>A big gap in support for affirmative action ha...</td>\n",
       "      <td></td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>b'ti11r7'</td>\n",
       "      <td>32.0</td>\n",
       "      <td>Asian Countries which represent the ancestral ...</td>\n",
       "      <td>I just googled search for the first time affir...</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>b'5cudq1'</td>\n",
       "      <td>25.0</td>\n",
       "      <td>We Will Not Be Used: Are Asian-Americans the R...</td>\n",
       "      <td>By Mari J. Matsuda\\n\\nAmerican lawyer, activis...</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>b'dezvi6'</td>\n",
       "      <td>21.0</td>\n",
       "      <td>Opinion | Why have Chinese immigrants become t...</td>\n",
       "      <td></td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>b'9kgf7i'</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2018 r/AsianAmerican Survey Results</td>\n",
       "      <td>## BASIC DEMOGRAPHICS:\\n\\n* [Age](https://i.im...</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>b'1yujng'</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Should AAs (Asian-Americans) support AA (Affir...</td>\n",
       "      <td>I work at a mostly Chinese-American company in...</td>\n",
       "      <td>177.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>b'hlnwtr'</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Lawmakers Ting, Chiu hearing from Chinese Amer...</td>\n",
       "      <td></td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>b'ldzz0f'</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Affirmative action and rise of the Chinese-Ame...</td>\n",
       "      <td></td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>b'105fx43'</td>\n",
       "      <td>14.0</td>\n",
       "      <td>need advice and insights on traditional mother...</td>\n",
       "      <td>hi all! I have been in this r/ for a while and...</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>b'1juzmv'</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Affirmative Action – Harming Asians Most</td>\n",
       "      <td>I hate how Asians are stereotyped as being “su...</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>b'aulovs'</td>\n",
       "      <td>8.0</td>\n",
       "      <td>The views of Chinese Americans on affirmative ...</td>\n",
       "      <td></td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  score                                              title  \\\n",
       "1    b'hqic5b'  225.0  A big gap in support for affirmative action ha...   \n",
       "26   b'ti11r7'   32.0  Asian Countries which represent the ancestral ...   \n",
       "35   b'5cudq1'   25.0  We Will Not Be Used: Are Asian-Americans the R...   \n",
       "45   b'dezvi6'   21.0  Opinion | Why have Chinese immigrants become t...   \n",
       "46   b'9kgf7i'   20.0                2018 r/AsianAmerican Survey Results   \n",
       "55   b'1yujng'   15.0  Should AAs (Asian-Americans) support AA (Affir...   \n",
       "58   b'hlnwtr'   17.0  Lawmakers Ting, Chiu hearing from Chinese Amer...   \n",
       "60   b'ldzz0f'   12.0  Affirmative action and rise of the Chinese-Ame...   \n",
       "63  b'105fx43'   14.0  need advice and insights on traditional mother...   \n",
       "69   b'1juzmv'   11.0           Affirmative Action – Harming Asians Most   \n",
       "83   b'aulovs'    8.0  The views of Chinese Americans on affirmative ...   \n",
       "\n",
       "                                            self_text  num_comments  \n",
       "1                                                             114.0  \n",
       "26  I just googled search for the first time affir...          66.0  \n",
       "35  By Mari J. Matsuda\\n\\nAmerican lawyer, activis...           6.0  \n",
       "45                                                             25.0  \n",
       "46  ## BASIC DEMOGRAPHICS:\\n\\n* [Age](https://i.im...          40.0  \n",
       "55  I work at a mostly Chinese-American company in...         177.0  \n",
       "58                                                              2.0  \n",
       "60                                                             15.0  \n",
       "63  hi all! I have been in this r/ for a while and...          20.0  \n",
       "69  I hate how Asians are stereotyped as being “su...          26.0  \n",
       "83                                                              5.0  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chinese_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 5)\n"
     ]
    }
   ],
   "source": [
    "print(chinese_rows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of these 11 rows that mention Chinese Americans in the title or selftext, how many of them mention other Asian ethnicities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_other_asian = r'((koreans?)|(japanese)|(indians?)|(filipinos?)|(vietnamese))'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\pandas\\core\\strings.py:1952: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  return func(self, *args, **kwargs)\n",
      "c:\\Users\\danie\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "other_asian = chinese_rows[chinese_rows.title.str.lower().str.contains(regex_other_asian) | df.self_text.str.lower().str.contains(regex_other_asian)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "      <th>self_text</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>b'5cudq1'</td>\n",
       "      <td>25.0</td>\n",
       "      <td>We Will Not Be Used: Are Asian-Americans the R...</td>\n",
       "      <td>By Mari J. Matsuda\\n\\nAmerican lawyer, activis...</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>b'9kgf7i'</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2018 r/AsianAmerican Survey Results</td>\n",
       "      <td>## BASIC DEMOGRAPHICS:\\n\\n* [Age](https://i.im...</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>b'105fx43'</td>\n",
       "      <td>14.0</td>\n",
       "      <td>need advice and insights on traditional mother...</td>\n",
       "      <td>hi all! I have been in this r/ for a while and...</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>b'1juzmv'</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Affirmative Action – Harming Asians Most</td>\n",
       "      <td>I hate how Asians are stereotyped as being “su...</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  score                                              title  \\\n",
       "35   b'5cudq1'   25.0  We Will Not Be Used: Are Asian-Americans the R...   \n",
       "46   b'9kgf7i'   20.0                2018 r/AsianAmerican Survey Results   \n",
       "63  b'105fx43'   14.0  need advice and insights on traditional mother...   \n",
       "69   b'1juzmv'   11.0           Affirmative Action – Harming Asians Most   \n",
       "\n",
       "                                            self_text  num_comments  \n",
       "35  By Mari J. Matsuda\\n\\nAmerican lawyer, activis...           6.0  \n",
       "46  ## BASIC DEMOGRAPHICS:\\n\\n* [Age](https://i.im...          40.0  \n",
       "63  hi all! I have been in this r/ for a while and...          20.0  \n",
       "69  I hate how Asians are stereotyped as being “su...          26.0  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_asian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_asian_ids = other_asian['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "chinese_rows_final = chinese_rows[~chinese_rows['id'].isin(other_asian_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_chinese = df[~df['id'].isin(chinese_rows_final['id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "      <th>self_text</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'14m8mf4'</td>\n",
       "      <td>244.0</td>\n",
       "      <td>[Megathread] Supreme Court Ruling on Affirmati...</td>\n",
       "      <td>This is a consolidated thread for users to dis...</td>\n",
       "      <td>357.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'9nk0do'</td>\n",
       "      <td>210.0</td>\n",
       "      <td>Anti-Asian Bias, Not Affirmative Action, Is on...</td>\n",
       "      <td></td>\n",
       "      <td>44.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'sejv5m'</td>\n",
       "      <td>148.0</td>\n",
       "      <td>Opinion | It’s Time for an Honest Conversation...</td>\n",
       "      <td></td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'h9tuc2'</td>\n",
       "      <td>139.0</td>\n",
       "      <td>University of California regents unanimously e...</td>\n",
       "      <td></td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b'sbrca3'</td>\n",
       "      <td>134.0</td>\n",
       "      <td>Supreme Court to hear Harvard affirmative acti...</td>\n",
       "      <td>[https://www.cnn.com/2022/01/24/politics/supre...</td>\n",
       "      <td>224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>b'5cw7uu'</td>\n",
       "      <td>5.0</td>\n",
       "      <td>LGBT Culture has poor knowledge on Indians and...</td>\n",
       "      <td>I need to state the following points about the...</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>b'789gct'</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Assault on Affirmative Action Unwarranted [By ...</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>b'1belws6'</td>\n",
       "      <td>4.0</td>\n",
       "      <td>How White Supremacy uses Taiwan</td>\n",
       "      <td>This is a long read but I seriously feel the n...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>b'bgj9bw'</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Does this sub really have a disdain for Affirm...</td>\n",
       "      <td>After looking up some information about Affirm...</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>b'4ziaf0'</td>\n",
       "      <td>4.0</td>\n",
       "      <td>From Self-Interest to Collective Morality: How...</td>\n",
       "      <td></td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  score                                              title  \\\n",
       "0   b'14m8mf4'  244.0  [Megathread] Supreme Court Ruling on Affirmati...   \n",
       "2    b'9nk0do'  210.0  Anti-Asian Bias, Not Affirmative Action, Is on...   \n",
       "3    b'sejv5m'  148.0  Opinion | It’s Time for an Honest Conversation...   \n",
       "4    b'h9tuc2'  139.0  University of California regents unanimously e...   \n",
       "5    b'sbrca3'  134.0  Supreme Court to hear Harvard affirmative acti...   \n",
       "..         ...    ...                                                ...   \n",
       "95   b'5cw7uu'    5.0  LGBT Culture has poor knowledge on Indians and...   \n",
       "96   b'789gct'    4.0  Assault on Affirmative Action Unwarranted [By ...   \n",
       "97  b'1belws6'    4.0                    How White Supremacy uses Taiwan   \n",
       "98   b'bgj9bw'    5.0  Does this sub really have a disdain for Affirm...   \n",
       "99   b'4ziaf0'    4.0  From Self-Interest to Collective Morality: How...   \n",
       "\n",
       "                                            self_text  num_comments  \n",
       "0   This is a consolidated thread for users to dis...         357.0  \n",
       "2                                                              44.0  \n",
       "3                                                              85.0  \n",
       "4                                                             108.0  \n",
       "5   [https://www.cnn.com/2022/01/24/politics/supre...         224.0  \n",
       "..                                                ...           ...  \n",
       "95  I need to state the following points about the...          11.0  \n",
       "96                                                              1.0  \n",
       "97  This is a long read but I seriously feel the n...           1.0  \n",
       "98  After looking up some information about Affirm...          19.0  \n",
       "99                                                              4.0  \n",
       "\n",
       "[93 rows x 5 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_chinese"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d9defa72c2715dab9f7f172572cd30a1ab1a2083462d32ef96aadb7c6e0c73b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
